
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Perspectives on Neuroscience, Mathematics, and Machine Learning">
      
      
        <meta name="author" content="Kaiwen Bian">
      
      
        <link rel="canonical" href="https://kbian.org/Kaiwen-Wiki/articles/mathamatics/optimization/">
      
      
        <link rel="prev" href="../confidence/">
      
      
        <link rel="next" href="../constraint/">
      
      
      <link rel="icon" href="../../../assets/index/logo.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.47">
    
    
      
        <title>Twitch on Theory - Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../css/custom.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-V7MWLE7LXW"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-V7MWLE7LXW",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-V7MWLE7LXW",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#twitch-on-theory-in-convex-optimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Wiki" class="md-header__button md-logo" aria-label="Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Twitch on Theory
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="grey" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/KevinBian107/Kaiwen-Wiki" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  My Wiki

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../literature/literatures/" class="md-tabs__link">
          
  
  Literatures

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../neuroscience/cognitive_brain/" class="md-tabs__link">
          
  
  Neuroscience Related

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../stochastic/" class="md-tabs__link">
          
  
  Mathamatics Related

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Wiki" class="md-nav__button md-logo" aria-label="Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Wiki
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/KevinBian107/Kaiwen-Wiki" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    My Wiki
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../literature/literatures/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Literatures
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../neuroscience/cognitive_brain/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Neuroscience Related
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Mathamatics Related
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Mathamatics Related
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stochastic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Unfolding Stochasticity
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../confidence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lend It Some Confidence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Twitch on Theory
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Twitch on Theory
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-in-different-lenses" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent In Different Lenses
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimality-guaranteed" class="md-nav__link">
    <span class="md-ellipsis">
      Optimality Guaranteed
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#all-families-comes-from-twitch" class="md-nav__link">
    <span class="md-ellipsis">
      All Families Comes From Twitch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beyond-convexity-optimality-may-be-guaranteed" class="md-nav__link">
    <span class="md-ellipsis">
      Beyond Convexity, Optimality May Be Guaranteed
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../constraint/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    All Constraint Solving
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-in-different-lenses" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent In Different Lenses
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimality-guaranteed" class="md-nav__link">
    <span class="md-ellipsis">
      Optimality Guaranteed
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#all-families-comes-from-twitch" class="md-nav__link">
    <span class="md-ellipsis">
      All Families Comes From Twitch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beyond-convexity-optimality-may-be-guaranteed" class="md-nav__link">
    <span class="md-ellipsis">
      Beyond Convexity, Optimality May Be Guaranteed
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h1 id="twitch-on-theory-in-convex-optimization">Twitch on Theory In Convex Optimization<a class="headerlink" href="#twitch-on-theory-in-convex-optimization" title="Permanent link">&para;</a></h1>
<div class="wrapper">
  <div class="profile">
    <img src="../../../assets/index/profile_pic.jpeg" alt="Profile Picture">
    <div class="profile-details">
      <span class="name">Kaiwen Bian</span>
      <span class="metadata">10 min read · Dec 11, 2024</span>
    </div>
  </div>
</div>

<p>The beauty with convex optimization is that we can use a theoritical perspective to bound the convergence of the algorithm: <strong><em>"It will stop and it will be the optimal"</em></strong>. More importantly, when we make "a little twitch" on teh formulation of the problem, we see a completely different algorithm with a different perspective.</p>
<h2 id="gradient-descent-in-different-lenses">Gradient Descent In Different Lenses<a class="headerlink" href="#gradient-descent-in-different-lenses" title="Permanent link">&para;</a></h2>
<p>Gradient descent is an extremely popular algorithm that is highly used in modern machine learning (particularly variants of it like the ADAM optimizor). This is not only because it has good practical results but also because it comes with strong theoritical guarantees. In this section, we will use differnt perspective to look at GD.</p>
<h3 id="taylor-expansion">Taylor Expansion<a class="headerlink" href="#taylor-expansion" title="Permanent link">&para;</a></h3>
<p>Taylor's theory is an extremely core concept in convex optimization as many of convex optimization is about <strong><em>"how to satisfy taylor theory such that we have certain part less than some other part"</em></strong>.</p>
<div class="arithmatex">\[
f(\vec y) = f(\vec x) + \nabla f(\vec z)^T (\vec y - \vec x)
\]</div>
<p>This is the recursion definition of Taylor's theoy, with the goal of estimating a point <span class="arithmatex">\(\vec y\)</span> from using a different point <span class="arithmatex">\(\vec x\)</span>, plus its curvature, and recursively unfolding th whole  expression again using <span class="arithmatex">\(\vec z \in (\vec x, \vec y)\)</span> (getting to curvature of curvature and so on). The magic of gradient decent comes when we assume this <span class="arithmatex">\(\vec y\)</span> is our next point <span class="arithmatex">\(\vec x + \mu \vec v\)</span> where we are moving along the direction of <span class="arithmatex">\(\vec v\)</span>.</p>
<div class="arithmatex">\[
f(\vec x + \mu \vec v) = f(\vec x) + \nabla f(\vec z)^T (\mu \vec v)
\]</div>
<p>Ideally, if we  are doing gradient descent, we want to have the next point taking a lower function value than the previous one, meaning that we want:</p>
<div class="arithmatex">\[
f(\vec x + \mu \vec v) \leq f(\vec x)
\]</div>
<p>or teh equivalence <span class="arithmatex">\(f(\vec x + \mu \vec v) - f(\vec x) \leq 0\)</span>. For a small enough <span class="arithmatex">\(\tilde{\mu}\)</span> and a continuous function <span class="arithmatex">\(f\)</span>s, the descent direction of <span class="arithmatex">\(\vec v\)</span> when <span class="arithmatex">\(\vec v \cdot \nabla f(\vec x) \leq 0\)</span> is also the descent direction for <span class="arithmatex">\(f(\vec x + \mu \vec v)\)</span>. To create <span class="arithmatex">\(\vec v \cdot \nabla f(\vec x) \leq 0\)</span>, we need <span class="arithmatex">\(\vec  v = \nabla f(\vec x)\)</span>, which is why the gd equation is in the form:</p>
<div class="arithmatex">\[
x^{(t+1)} = x^{(t)} - \mu^{(t)} \nabla f(x^{(t)})
\]</div>
<p>To recap, we derived our descent direction <span class="arithmatex">\(\vec v\)</span> based on what we wnat to satisfy taylor theory such that <span class="arithmatex">\(f(\vec x + \mu \vec v) - f(\vec x) \leq 0\)</span>. We have shown that there is a intuitive, but theoritical reason behind each step of why we are doing gradient descent.</p>
<h3 id="local-convexity-rightarrow-calculus-optimization">Local Convexity <span class="arithmatex">\(\rightarrow\)</span> Calculus Optimization<a class="headerlink" href="#local-convexity-rightarrow-calculus-optimization" title="Permanent link">&para;</a></h3>
<p>From a different perspective, we can look at GD as doing a local descent. To be more specific, we assume that at each step, the <strong>Armijo condition</strong> is hold, then we have a local convex shape. In full Taylor expansion to the second degree, it cna be expressed as:</p>
<div class="arithmatex">\[
f(z) = f(x^{(t)}) + \nabla f(x^{(t)})^T (z - x^{(t)}) + \frac{1}{2} (z - x^{(t)}) \nabla^2 f(z)^T (z - x^{(t)})
\]</div>
<p>This second <span class="arithmatex">\(\frac{1}{2} (z - x^{(t)}) \nabla^2 f(z)^T (z - x^{(t)})\)</span> is super annoying as we have a recursive hessian term <span class="arithmatex">\(\nabla^2 f(z)\)</span> in it. However, what if we don't care about the curvature of the curvature? We make a simplified assumption that:</p>
<div class="arithmatex">\[
\nabla^2 f(z) \leftarrow \frac{1}{\mu}I
\]</div>
<p>This makes our expression much more simple, giving just</p>
<div class="arithmatex">\[
g(z) = f(x^{(t)}) + \nabla f(x^{(t)})^T (z - x^{(t)}) + \frac{1}{\mu} || (z - x^{(t)}) ||^2
\]</div>
<p>Notice that this is a quadratic-ish function in multi-dimension and then the shape we have is <strong><em>locally convex</em></strong>. We say that this <span class="arithmatex">\(g(z)\)</span> looks very much like <span class="arithmatex">\(f(z)\)</span> from a local perspective and more importnatly, this <span class="arithmatex">\(g(z)\)</span> function is convex and we can use the traditional calculus method of:</p>
<div class="arithmatex">\[
\nabla_z g(z) = 0
\]</div>
<p>And we can retrieve the same result that</p>
<div class="arithmatex">\[
z^* = x^{(t)} - \mu \nabla f(x^{(t)})
\]</div>
<p>Again, by assuming our function is locally convex (locally L-smooth to be specific, hessian bounded), we can <strong><em>hide away much complexity</em></strong> into approximations.</p>
<h2 id="optimality-guaranteed">Optimality Guaranteed<a class="headerlink" href="#optimality-guaranteed" title="Permanent link">&para;</a></h2>
<p>We never really formally define what it means to be concvex here, but for now let's just say that convexity means that we have a <strong><em>Postive Semi-Deminite (PSD)</em></strong> hessian (this is not a definition but a result of teh definition). With gradient descnet + convexity + some tricks (teloscoping theory, series of convex functions, ...), we can have many powerful optimality guaranteed, namely: <strong><em>"It will stop, it will converge, and we will be at the optimal position"</em></strong>. We will name a few here:</p>
<h3 id="l-lipschitz">L-Lipschitz<a class="headerlink" href="#l-lipschitz" title="Permanent link">&para;</a></h3>
<p>L-Lipschitz means that the gradient is bounded where <span class="arithmatex">\(|| \nabla f(x) || \leq L\)</span> and equivalently we have:</p>
<div class="arithmatex">\[
||f(x) - f(y)|| \leq L ||x - y||
\]</div>
<p>With a convex function <span class="arithmatex">\(f\)</span>, initial guess in range <span class="arithmatex">\(||x^{(0)} - x^*|| \leq R\)</span>, total <span class="arithmatex">\(T\)</span> iterations, and the learning rate <span class="arithmatex">\(\mu = \frac{R}{L\sqrt{T}}\)</span>, we can guarantee that the average distance/error to the optimal coordinate <span class="arithmatex">\(x^*\)</span> under function being bounded by:</p>
<div class="arithmatex">\[
f(\frac{1}{T} \sum^{T-1}_{s=0}x^{(s)}) - f(x^*) \leq \frac{RL}{\sqrt{T}}
\]</div>
<p>This is the first proof we introduced such that we can say confidently: gradient descent will stop.</p>
<h3 id="l-smooth">L-Smooth<a class="headerlink" href="#l-smooth" title="Permanent link">&para;</a></h3>
<p>L-smooth means that the hessian is bounded where <span class="arithmatex">\(0 \leq v^T \nabla^2 f(x) v \leq L\)</span> (since we are looking at the hessian, we need to bound by matrix norm).</p>
<div class="arithmatex">\[
||\nabla f(x) - \nabla f(y)|| \leq L ||x - y||
\]</div>
<p>We can guarantee that at each step, tehfunction value decreases:</p>
<div class="arithmatex">\[
f(x^{(t+1)}) \leq f(x^{(t)}) - \frac{\mu}{2} ||\nabla f(x^{(t)})||^2
\]</div>
<p>and more importantly, we should have at least one <span class="arithmatex">\(x^{(t)}\)</span> satisfying the following cndition (this root boost convergence speed hugely):</p>
<div class="arithmatex">\[
\|\nabla f(x^{(t)})\| \leq \sqrt{\frac{2(f(x^{(0)}) - f(x^*))}{\mu T}}
\]</div>
<p>This is an incredably strong condition since non of the L-smooth proof used the fact that teh function need to be convex, only that they are second degree differentiable and L-smooth, signifying that with just gradient descent: <strong><em>"we will stop at some point and this would be optimal, no matter convexity or not"</em></strong>. For more information, reference to this note:</p>
<p><a href="../../../assets/math/optimization_notes.pdf" target="_blank">
    <p><span class="link-icon">&#9881;</span> Notes on convex optimization</p>
    </a></p>
<h2 id="all-families-comes-from-twitch">All Families Comes From Twitch<a class="headerlink" href="#all-families-comes-from-twitch" title="Permanent link">&para;</a></h2>
<p>Now after convexity and basic formulation of gradient descent, this is where we get to the interesting part, turns out that all the variants and instances of gradient descent (Coordinate descent, Uniform descent, Newton's method (i.e. ADAM), GD with Momentum, Nestrov Acceleration, Conjugate GD, ...) is all somewhat like GD but a little twitch on the theoritical formulation.</p>
<h3 id="norm-in-gradient-descent">"Norm" in Gradient Descent<a class="headerlink" href="#norm-in-gradient-descent" title="Permanent link">&para;</a></h3>
<p>Turns out that there is actually a norm hidden in the gradient descent algorithm. When we use GD, we are saying that:</p>
<div class="arithmatex">\[
x^{(t+1)} - x^{(t)} = -\mu \nabla f(x)
\]</div>
<p>When swapping into Taylor's theorem</p>
<div class="arithmatex">\[
f(x^{(t+1)}) + \nabla f(x^{(t)})^T (x^{(t+1)} - x^{(t)}) \approx f(x^{(t)}) - \mu \nabla f(x^{(t)})^T \nabla f(x^{(t)})
\]</div>
<div class="arithmatex">\[
\approx f(x^{(t)}) - \mu ||\nabla f(x^{(t)})||^2
\]</div>
<p>Normally speaking, this norm is a Eucledian norm or norm 2. However, in the same fashion, we can switch to norm-1 or norm-infinity$. This is essentially framing gradient descent as a <strong><em>constraint optimization</em></strong> problem. How do we optimize in the set of this sphere, or this dimond, or this pyramid? With different constraints, GD comes with different property, namely <strong><em>coordinate descent</em></strong> or <strong><em>uniform descent</em></strong>.</p>
<div class="arithmatex">\[
L_1 \rightarrow \text{Sparse Coordinate Descent}: \quad \tilde{p}(x) = \text{sgn}(\nabla f(x)) \cdot \frac{|\nabla f(x)|_i}{|\nabla f(x)|_{\max}}
\]</div>
<div class="arithmatex">\[
L_{\infty} \rightarrow \text{Uniform Descent}: \quad \tilde{p}(x) = \frac{\text{sgn}(\nabla f(x))}{||\text{sgn}(\nabla f(x))||_{\infty}}
\]</div>
<h3 id="newtons-method">Newton's Method<a class="headerlink" href="#newtons-method" title="Permanent link">&para;</a></h3>
<p>Now forget everything we just discussed about gradient descent and we will look at it from  brand new perspective. We used <span class="arithmatex">\(f(x) \approx f(x^{(t)}) + \nabla f(x^{(t)})^T (x - x^{(t)})\)</span> to derive gradient descent before, now let's use the second degree expansion like we did in local convex perspective. However, instead of making a simplified assumption, let's set the gradient directly to <span class="arithmatex">\(0\)</span>.</p>
<div class="arithmatex">\[
f(x) \approx f(x^{(t)}) + \nabla f(x^{(t)})^T (x - x^{(t)}) + \frac{1}{2} (x - x^{(t)}) \nabla^2 f(x)^T (x - x^{(t)})
\]</div>
<p>Setting gradient <span class="arithmatex">\(\nabla_x f(x)\)</span> to zero:</p>
<div class="arithmatex">\[
\nabla_x f(x) = \nabla f(x^{(t)}) + \nabla^2 f(x) (x - x^{(t)}) = 0 
\]</div>
<div class="arithmatex">\[
\nabla f(x^{(t)}) + \nabla^2 f(x) (x - x^{(t)}) = 0 
\]</div>
<div class="arithmatex">\[
\nabla^2 f(x^{(t)}) (x - x^{(t)}) = -\nabla f(x^{(t)})
\]</div>
<p>Notice that <span class="arithmatex">\(\nabla^2 f(x^{(t)})\)</span> is a matrix (A), <span class="arithmatex">\((x - x^{(t)})\)</span> is a vector (x), and <span class="arithmatex">\(-\nabla f(x^{(t)})\)</span> is a vector (b), so essentially we are actually solving the famous problem in linear algebra: how to find <span class="arithmatex">\(Ax + b = 0\)</span>. Alternatively, by changing <span class="arithmatex">\(Ax = -b\)</span> into <span class="arithmatex">\(x = - A^{-1}b\)</span>:</p>
<div class="arithmatex">\[
x^{(t+1)} - x^{(t)} = - [\nabla^2 f(x^{(t)})]^{-1} \nabla f(x^{(t)})
\]</div>
<p>Or we can write in the update form of Newton's method (fun fact, this is called Newton's mthod because the first algorithm os setting to zero is invented by Newton in 1D, is just taht we are doing in <span class="arithmatex">\(R^n\)</span>) as:</p>
<div class="arithmatex">\[
x^{(t+1)} = x^{(t)} - [\nabla^2 f(x^{(t)})]^{-1} \nabla f(x^{(t)})
\]</div>
<p>However, inverting this "A" matrix comes with insanly high computation cost and numerical instability. This is where our familier friend ADAM optimizor comes in, which is essetntially a <strong><em>Quasi-Newton</em></strong> method, or a family of algorithm that estimates/creates condition for this inverse to be easily calculated.</p>
<p>Using Newton's method come with very interesting convergence property such that <strong><em>if we converge, we converge exponentially fast</em></strong>.</p>
<div class="arithmatex">\[
(1) \quad ||x^{(t)} - x^*|| \leq \frac{2h}{3L}
\]</div>
<div class="arithmatex">\[
(2) \quad ||x^{(t)} - x^*||^2 \leq \frac{3L}{2h} ||x^{(t-1)} - x^*||^2
\]</div>
<p>However, we don't have the guarentee  that <span class="arithmatex">\(\quad ||x^{(t)} - x^*|| \leq 1\)</span>, so convergence may not occur, but if it does, it converges expoennetially fast.</p>
<h3 id="gradient-descnet-with-momentum">Gradient Descnet With Momentum<a class="headerlink" href="#gradient-descnet-with-momentum" title="Permanent link">&para;</a></h3>
<p>All discussion in the following few sections will be around a particular case of a convex function, a nice one (we wil generalize later):</p>
<div class="arithmatex">\[
\phi(x) = \frac{1}{2} x^T A x
\]</div>
<p>Now recall gradient descent and let's add a <strong><em>first order Markovian memory</em></strong> to it. If the current gradient direction aligns with the previous gradient direction, we move a little bit further (constructive interference) and do the opposite if we are exact opposite with previous gradient. This is called <strong><em>momentum</em></strong> and using this method we can avoid sudden updates in teh gradient (zig-zag shape) and create the decent in  much smoother way. Formally, we can write it as:</p>
<div class="arithmatex">\[
x^{(t+1)} = x^{(t)} - \mu \nabla f(x^{(t)}) + \beta (x^{(t)} - x^{(t-1)})
\]</div>
<p>To study the convergence property, we can stack up the current state and next state, just like in <strong><em>control theory</em></strong> calculating state transition.</p>
<div class="arithmatex">\[
\begin{bmatrix}
x^{(t+1)} \\
x^{(t)}
\end{bmatrix} = 
\underbrace{
\begin{bmatrix}
1 - \mu \lambda + \beta &amp; -\beta \\
1 &amp; 0
\end{bmatrix}}_{M}
\begin{bmatrix}
x^{(t)} \\
x^{(t-1)}
\end{bmatrix}
\]</div>
<p>Under a quadratic equation condition of <span class="arithmatex">\(\frac{1}{2} x^T A x\)</span> (we can generalize this quadratic equation later with strongly convex property), gradient descnet with momentum will converge under:</p>
<div class="arithmatex">\[
\left(\frac{k-1}{k+1}\right)^t \quad \text{where} \quad k = \frac{\lambda_{max}}{\lambda_{min}}
\]</div>
<p>A similar approach (Nestrov Acceleration) works similar as momentum, but it goes  little bit further first before taking the gradient. It iis less intuitive than gradient descent with momentum, but in practice is has nice property that out performs momentum, namely under continuou  environment and differential euqation environment. In general, we can say the convergence rate from GD to GD + M to GD + N.A. as:</p>
<div class="arithmatex">\[
(\frac{k+1}{k+1})^t \rightarrow \left(\frac{\sqrt{k}-1}{\sqrt{k}+1}\right)^t \rightarrow \left(\sqrt{\frac{\sqrt{k}-1}{\sqrt{k}}}\right)^t
\]</div>
<h3 id="conjugate-gradient-descent">Conjugate Gradient Descent<a class="headerlink" href="#conjugate-gradient-descent" title="Permanent link">&para;</a></h3>
<p>Let's go back to thinking about solving this problem mentioned in Newton's method, we want to solve the problem of <span class="arithmatex">\(Ax^* - b = 0\)</span> where <span class="arithmatex">\(\nabla \phi(x) = Ax - b\)</span>. We know that doing the inverse is very costly and numerically instable. So can we solve this problem <strong><em>without inversing A</em></strong> and maybe we can <strong><em>incrementally</em></strong> solve this issue. Let's first define the mathamaticla notion of conjugate, let's say that <span class="arithmatex">\({p_1, ..., p_n}\)</span> is the conjugate of a Positive Definite (PD) matrix A if:</p>
<div class="arithmatex">\[
p_i^T A p_j = 0 \quad \text{if} \quad i \neq j
\]</div>
<p>This conjugate is a general notion of <strong><em>orthogonality</em></strong> and the idea is to maybe update in one orthogonal direction at each time and just not care of it (independent of future update). Intuitively, we can frame it like the following:</p>
<div class="arithmatex">\[
x^{(t+1)} = x^{(t)} + \alpha_t \vec{p}_t
\]</div>
<p>Where</p>
<div class="arithmatex">\[
\alpha_t = \underset{\alpha \in \mathbb{R}}{\arg \min} \phi(x^{(t)} + \alpha \vec{p}_t)
\]</div>
<p>The first ensures the orthogonal step amd teh second ensure the descent step. Notice that this second optimization problem is a easier problem to solve and close form solution does exist as we are only looking at a minimization problem that involve two vectors, way easier than the problem we begin with. Close form solution of optimal <span class="arithmatex">\(\alpha^*\)</span> would be:</p>
<div class="arithmatex">\[
d_t = \alpha^* = \frac{(b - Ax^{(k)})^T p_t}{p_t^T A p_t} = \frac{- \nabla \phi(x^{(t)})^T p_t}{p_t^T A p_t}
\]</div>
<p>This almost looks like a gradient descent with just a bunch of other fancy multiplication in it. Turns out that we are actually doing a gradient descent that is projected onto the <span class="arithmatex">\(p_t\)</span> conjugate axis. Taking the gradient in the component direction of <span class="arithmatex">\(p_t\)</span>. All of theses sounds very nice, but we still have one problem. How do we find the conjugate? Doing <span class="arithmatex">\(p_i^T A p_j = 0 \quad \text{if} \quad i \neq j\)</span> is not a cheap operation. Let's do it like <strong><em>Bellman Update</em></strong>, let's use a dynamic way of updating the conjugate!</p>
<p>Assume that each <span class="arithmatex">\(p_t\)</span> only need the previous one <span class="arithmatex">\(p_{t-1}\)</span> to be computed, we just need the current vector to be conjugate to the previous one, and if the chain forms for being conjugate, all of them will be conjugate vectors and we can throw away <span class="arithmatex">\(p_o\)</span> to <span class="arithmatex">\(p_{t-2}\)</span>. Let's start with:</p>
<div class="arithmatex">\[
p_t = -\nabla \phi(x^{(t)}) + \beta_t p_{t-1}
\]</div>
<p>Then we multiply <span class="arithmatex">\(p_{t-1} A\)</span> to both side and try to make RHS zero.</p>
<div class="arithmatex">\[
p_{t-1}^T A p_t = -p_{t-1}^T A \nabla \phi(x^{(t)}) + \beta_t p_{t-1}^T A p_{t-1}
\]</div>
<p>This has a close form solution again where we can calculate the optimal <span class="arithmatex">\(\beta_t\)</span> to satisfy this condition. The CGD algorithm becomes clear as well, we just need to randomly initiate <span class="arithmatex">\(\beta\)</span> and \p_0$, then update base on this given constraint.</p>
<div class="arithmatex">\[
\beta_t = \frac{p_{t-1}^T A \nabla \phi(x^{(t)})}{p_{t-1}^T A p_{t-1}}
\]</div>
<p>Conjugate gradient descent is very very powerful (similar to gradient descent with momentum):</p>
<div class="arithmatex">\[
||x^{(t)} - x^*||_A \leq 2 \left(\frac{\sqrt{k}-1}{\sqrt{k}+1}\right)^t ||x^{(0)} - x^*||_A
\]</div>
<h2 id="beyond-convexity-optimality-may-be-guaranteed">Beyond Convexity, Optimality May Be Guaranteed<a class="headerlink" href="#beyond-convexity-optimality-may-be-guaranteed" title="Permanent link">&para;</a></h2>
<h3 id="strongly-convex">Strongly Convex<a class="headerlink" href="#strongly-convex" title="Permanent link">&para;</a></h3>
<p>What happens beyond convexity? We can still talk about them in theory (though not as convineint as in convex situations). First we will introduce the concept of <strong><em>stronly convex</em></strong>, or essentially swapping out the <span class="arithmatex">\(\nabla^2 f(z)\)</span> with the smallest eigen value of such matrix and that <span class="arithmatex">\(\nabla^2 f(z) \geq CI\)</span>. This creats an upper bound condition called strong convex, or essentially constructing sort of a tangent curve instead of a tangent line:</p>
<div class="arithmatex">\[
f(y) \geq f(x) + \nabla f(x)^T (y-x) + \frac{C}{2} ||y-x||^2
\]</div>
<p>and that</p>
<div class="arithmatex">\[
f(x) - f(x^*) \leq \frac{||\nabla f(x)||^2}{2C}
\]</div>
<p>When  strongly convex is achieved, L-smooth condition is matched, and when choosing $\mu =\frac{1}{\mu}, we have a incrediablly strong convergence rate, <strong><em>an exponential convergence rate that is independent of the condition number <span class="arithmatex">\(k\)</span></em></strong>.</p>
<div class="arithmatex">\[
f(x^{(t+1)}) - f(x^*) \leq (\frac{C}{L})^t (f(x^{(0)}) - f(x^*))
\]</div>
<p>More critically, when strongly convex condition is matched, all the previous method's convergence property remains for functions that is strongly convex but not neccessarily just quadratic in the form of <span class="arithmatex">\(\frac{1}{2} x^T A x\)</span>. This is really how strongly convex is used in practice. Practically speaking, <strong><em>when $f(w) is convex and <span class="arithmatex">\(R(w)\)</span> is c-strongly convex, then <span class="arithmatex">\(f(w) + R(w)\)</span> is also c-strongly convex</em></strong>. This gives the power for regulaorizor to show its power. For instance, in ridge regression, other than just making a constraint on keeping the weights small, ridge regression gurantees a better convergence rate than just normal gradient descent.</p>
<h3 id="pl-condition">PL-Condition<a class="headerlink" href="#pl-condition" title="Permanent link">&para;</a></h3>
<p>When discussion in non-convex situations, we talk about the <strong><em>PL-Condition</em></strong>, which is eessentially a condition that looks very similar with strongly convex where strongly convex implies PL-condition but not vice versa.</p>
<div class="arithmatex">\[
f: \mathbb{R}^n \rightarrow \mathbb{R} \text{ satisfies } \mu \text{-PL-Condition if } \forall x, y \in \mathbb{R}^n
\]</div>
<div class="arithmatex">\[
\frac{1}{2}||\nabla f(x)||_2^2 \geq \mu (f(x) - f(x^*))
\]</div>
<p>Essentially, we are putting a upperbound on the gradient saying that when we are far from <span class="arithmatex">\(x^*\)</span>, we have  big gradient to move faster. Importantly, PL-Condition has 2 important factors:</p>
<ol>
<li>PL-Condition can hold for non-convex functions and it acts in as sort of a strongly convex guarantees.</li>
<li>If <span class="arithmatex">\(f(x)\)</span> s L-smooth + <span class="arithmatex">\(\mu\)</span>-PL-Condition, then gradient descent with a step-size of <span class="arithmatex">\(\frac{1}{L}\)</span> will converge at a rate of</li>
</ol>
<div class="arithmatex">\[
f(x^{(t)}) - \delta(x^*) \leq \left(1 - \frac{\mu}{L}\right)^t (f(x^{(0)}) - \delta(x^*))
\]</div>
<p>This is similar to strongly convex, very strong exponential convergence. In practice, we actually observes this phenomenon! Turns out that an <strong><em>over-parametrized</em></strong> neural network (highly non-convex function) can be written in a <strong><em>Neural Tangent Kernel</em></strong> form of:</p>
<div class="arithmatex">\[
||\nabla L(w)||^2 \geq 2\mu L(w)
\]</div>
<p>Which satisfy the PL-Condition and neural network converges as the rate of <span class="arithmatex">\((1 - \frac{\mu}{\beta})^t\)</span>, it converges exponentially fast. This is exactly why we see exponential convergence in teh begining when we randomly initialize an neural network.</p>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 Kaiwen Bian – <a href="#__consent">Change cookie settings</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/KevinBian107" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
      <div class="md-consent" data-md-component="consent" id="__consent" hidden>
        <div class="md-consent__overlay"></div>
        <aside class="md-consent__inner">
          <form class="md-consent__form md-grid md-typeset" name="consent">
            

  
    
  


  
    
  




<h4>Cookie consent</h4>
<p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p>
<input class="md-toggle" type="checkbox" id="__settings" >
<div class="md-consent__settings">
  <ul class="task-list">
    
      
      
        
        
      
      <li class="task-list-item">
        <label class="task-list-control">
          <input type="checkbox" name="analytics" checked>
          <span class="task-list-indicator"></span>
          Google Analytics
        </label>
      </li>
    
      
      
        
        
      
      <li class="task-list-item">
        <label class="task-list-control">
          <input type="checkbox" name="github" checked>
          <span class="task-list-indicator"></span>
          GitHub
        </label>
      </li>
    
  </ul>
</div>
<div class="md-consent__controls">
  
    
      <button class="md-button md-button--primary">Accept</button>
    
    
    
  
    
    
      <button type="reset" class="md-button md-button--primary">Reject</button>
    
    
  
    
    
    
      <label class="md-button" for="__settings">Manage settings</label>
    
  
</div>
          </form>
        </aside>
      </div>
      <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.sections", "toc.follow", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.expand", "navigation.prune", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate", "content.tooltips", "content.code.select", "content.footnote.tooltips"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/js/all.min.js"></script>
      
        <script src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>