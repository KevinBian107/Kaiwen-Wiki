{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Wiki","text":""},{"location":"#welcome-to-my-wiki-page","title":"Welcome to My Wiki Page","text":"<p>Some perspectives I have on Neuroscience, Mathematics, Machine Learning, and other areas of interest. I like to write some small articles becase (1) to share some ideas I have and (2) it gets me to think about the connections between what I learned.</p> <p>Back to my website</p> <p>Here are some of my notes that I will later on write them into articles: notes</p>"},{"location":"#neuroscience-related","title":"Neuroscience Related","text":"<p>I think that the process of thoughts and cognition may be coming from something or a mechanism that we don't understand yet. But the faciliattion of how they go from thoughts to action is what we can seek to understood and such biological mechanistic insgights may help us to accomplish our goals in everyday life. Moreover, I also found neuroscience and biology to be quite amazing in how they may inspire the design of intelligent algorithms/systems and how \"close\" they may be to the true \"structure\" in nature that makes intelligence.</p> <ul> <li> <p>Cognitive Neuroscience Perspectives Building a perspective on the brain.</p> </li> <li> <p>Sensory, Processing, Affective Neuroscience From sensory to processing to perception.</p> </li> <li> <p>Reinforcing &amp; Parallel Processing Reinforcing &amp; searching: some magnificent connections between the brain and algorithms.</p> </li> <li> <p>Neural Adaptation With Cost: Systematic Balance Distortion Addiction is a systematic adaptation to deviation\u2014a well-rounded circular circuit that feeds into itself. Once balance is distorted, problems may occur.</p> </li> <li> <p>What We Think Determines What We Can Think Once the circuit forms, the rest becomes much easier.</p> </li> <li> <p>What You Think May Not Be What You Think What we feel in the moment may not be true and what we think now may not be real.</p> </li> </ul>"},{"location":"#mathematics-related","title":"Mathematics Related","text":"<p>I find theoretical math to be pretty fun. I think that good practical techniques that work well are derived from a theoretical root.</p> <ul> <li> <p>Twitch on Theory In Convex Optimization Deriving everything we want in optimization from Taylor Theory and with small modification on some assumptions or the way we design things, we get completely different families of algorithms.</p> </li> <li> <p>All You Need Is Constraint Solving All hard problems that we want to solve can be framed as a constraint solving process if we look at them from a particular perspective. Both in math and in life.</p> </li> <li> <p>From Set We Create All An very simplified and naive attempt to discuss about ho mathematics are built up from sets using point set topology, that the usual math in \\(R^n\\) is just an small example of the realm of mathematics</p> </li> <li> <p>Unfolding Stochasticity Sequentially Modeling interactions between stochasticity across time sequentially through the key representational example of Random Walk.</p> </li> <li> <p>\\(N(\\mu, \\sigma)\\) Lend It Some Confidence There are deep connections between statistics and probability, even on very basic statistics levels.</p> </li> </ul>"},{"location":"articles/mathamatics/confidence/","title":"Lend It Some Confidence","text":"Kaiwen Bian 5 min read \u00b7 Jun 14, 2024 <p>       Statistics is a very practical domain, but often tools in statistics have very deep mathamatical roots in probability theory. I found this to be quite facinating because I think that only when understand the theoritical aspects of these tools will one be getting        an intuitive understanding of these tools and use/adapt them under appropriate circumstances. Things works for a mathamatical reason, they work because the math have happened to discover and support some things that happen to work.   </p> <p>       I want to use an concept that is very oftenly used through out many branches of statistics and probability to illustarte such point: Confidence Interval. It is so simple that probably a high school statistic class would discuss it but it is also so complex to        the point that one might not fully understand it until learning meausre theory in graduate school (for the record, I don't think I underatand it fully yet, but I can already see some of the intrinsic connections that makes it so amazing).   </p>"},{"location":"articles/mathamatics/confidence/#approach-to-some-distribution","title":"Approach To Some Distribution","text":"<p>       The way I think about confidence interval is to reason why it is needed. I think the idea of needing it comes from statsistics with the question: can we do better than just giving a point estimate of what we believe about the true distribution? Can we give an interval estimate?       Point estimate itself is a huge domain in statistics and probability (i.e. MOM, MLE,...), which we would not go into, but feel free to look into it a bit more as it is also very much rooted in probabilistic theory (i.e. MLE is maximizing the likelihood of observing all observatiosn in the same time, which are treated as independent and        identitically distributed random variable). Essentially an estimator is an random variable that tries to estimate the correct parameter for the given distribution to fit all the data (i.e. \\(\\mu\\) for gaussian distribution or \\(\\lambda\\) for poisson distribution). Okay, so we have an point estimator, but we want a range for it, maybe extracting an interval from the distribution        of this estimator since an estimator is techniqually an random variable? Let's look at the simple example when the estimator is \\(\\bar X\\) or the mean of the sample. We know that        $$       \\bar X = \\frac{x_1+...+x_n}{n}       $$        and that         $$       x_i \\sim N(\\mu, \\sigma) \\text{ with } \\bar X \\sim N(\\mu, \\frac{\\sigma}{\\sqrt n})       $$        Then we can make an new random variable that approaches a standard normal distribution \\(N(0,1)\\) by simply sifting values around:        $$       \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)       $$        This is doesn't show exactly why confidence interval is in it's form shown below, but this at least lend some ideas about how we can \"extend\" the estimator by giving it a interval. You may now have some intuition that these two seems to be weird values attached to the estimator does have a very probabilistic meaning since they look pretty similar.        $$       \\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)       $$        Techniqually, such confidence interval would be descrbed with an confidence level of \\(100(1-\\alpha)%\\), assuming \\(\\alpha=0.05\\), this is a 95% confidence interval, which is saying that we are 95% confident that the true parameter \\(\\theta\\) is in the interval or that when we plug in the data, 95% of the times the real parameter would be in such random range        (random because it is a random varaible + random margin that depends on data)        $$       P(\\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right) \\text{ contains } \\mu) = 1 - \\alpha       $$        Notice that this is not talking about the probability that teh true parameter is in the random interval, it is in it or not, nothing is random here. However, we will see later that in a connection such value may have some probabilistic interpretation.   </p> <p>     In fact, there is a deep conection between decision making hypothesis testing (HT) and confidence interval known as the confidence interval duality where not rejecting the null hypothesis \\(H_0\\)is the equivalence with such random interval around \\(\\hat \\theta\\) contains the real parameter \\(\\theta\\). Intuitively speaking, it is like talking about one with thinking the center as        \\(\\hat \\theta\\) and building confidence around that and the other is thinking center around \\(\\theta\\).        $$       \\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right) \\rightarrow \\left| \\frac{\\bar{X} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} \\right| &lt; z_{\\alpha/2}       $$   </p> <p> Some small detials on HT: The hypothesis testing representation here is illustrating fail to reject the null hypothesis condition, or the region that is not the critical region (the mid region of the normal curve where area is \\(1-\\alpha\\)). Notice that here we are talking about a probabilistic statement where \\(\\alpha\\) is the probability of seeing the observed test statistic (r.v.) as extreme or being more extreme than the        alternative hypothesis \\(H_1\\) direction. Usually \\(\\alpha\\) is manually set (i.e. 0.05) and treat that region to be trhe critical region for rejecting the null hypothesis. Inherently, the rest of the region would be saying that it is not that rare to see the observed test statistics under the null hypothesis, thus not rejecting it.     </p> <p>     These theoritical connection between statistical tools and probability theory goes much longer, extending to some other pretty well known distributions (you may have heard about \\(T\\) or \\(X_n^2\\) distribution, which are used when \\(\\sigma^2\\) is not present) adn we can use standard deviation as a replacement in the new random variable, which approach to a different distribution.      If you are interested, here are some notes that goes deeper into these connections.   </p> <p>Some notes that I concluded on basic statistics and probability connections across three areas: point estimate, interval estimate, adn decision making (HT)          (only personal understandings, may not be 100% correct).</p> <p>\\(N(\\mu, \\sigma)\\) Some Notes on Basic Statistic Connections</p> <p>     The connections for confidence interval itself actually serves much more than just in statistics, also to other realms (i.e. in algorithm and reinforcment: UCB).   </p>"},{"location":"articles/mathamatics/constraint/","title":"All You Need Is Constraint Solving","text":"Kaiwen Bian 15 min read \u00b7 Dec 16, 2024 <p>When I first learned about the idea  of constraint solving (I belive it was in a class taught by professor Sichun Gao in UCSD CSE called \"Search and optimization\"), I wasn't as exiciting as I am. I thought of constraint solving just as a small sub-branch of optimization, but I was wrong. It is actually the \"class\" that can generates any other type optimization, reasoning, or machine learning techniques when you look at if from the right perspective. Quoting from what professor Gao said himself: \"All you need is constraint solving\". If I wnat to summarize the ideas in this article, it would be:</p> <ol> <li>Theporitically, constraints greatly reduced our search problem because among all the answers we can search for, we only care about the ones  that matches with our constraint.</li> <li>Practically, \"doability\" is always part of the puzzle, in theory we can do everything but in practice we can't, so how do we optimize with these constraint not as a hinderance but as a helper?</li> </ol> <p>This article tries to go over some quite complicated topic, which I focuesed more on the intuition of it and less on the athamatical deriviation.</p>"},{"location":"articles/mathamatics/constraint/#standard-models-sm-constraint","title":"Standard Models (SM) Constraint","text":"<p>(Though I should probbaly put the section describing Variational Autoencoder as the first section since it will be a easier introduction to Expectation Maximization, but I think that starting with the Standard Model illusrate mypoints much clearer.)</p> <p>From a theoritical perspective, looking at constraint is very interesting. I come from a background of doing reinforcement learning and in there we like to frame our problem as a search &amp; optimization problem. How do we find the best path in a tree (the tree here is not actually a search tree but an abstract concept of so)? Or how do we find the optimal trajectory for getting to the maximum of the objective surface that changes over time since we get more and more knowledge of such surface? From a classical machine learning perspective, learning is about finding patterns in the dataset, no matter if you are doing supervised or unsupervised, it is all about looking for some useful pattern and characteristic from the dataset. Things got particularly interesting when I took a class named \"Machine learning with few labels\" taught by professor Zhiting Hu in UCSD HDSI. One of the major work that he has done was establishing the standard model for machine learning, similar to how physicist established the standard model of all partcles. As he always says in this class:</p> <p>We want to study machine learning like chemistry, to understand and constructs upon each other, not arcamy.</p> <p>The main reason of mentioning the SM is because when we look at the Standard Equation (SE) proposed in the same paper that unifies all type of learning and can be derived into any type of learning algorithm with specific configuration, which can be written as the following, we results in a constraint optimization problem.</p> \\[ \\min_{q,\\theta,\\xi} \\quad -\\alpha \\mathcal{H}(q) \\;+\\; \\beta \\mathcal{D}(q, p_\\theta) \\;+\\; U(\\xi) \\] <p>subject to</p> \\[ -\\mathbb{E}_q\\left[f_k^{(\\theta)}\\right] \\;\\leq\\; \\xi_k, \\quad k = 1, \\ldots, K. \\] <p>Where the first term is teh Uncertainty function \\(\\mathcal{H}(\\cdot)\\) that controls the compactness of the output model, for example, by regulating the amount of allowed randomness while trying to fit experience. The second term is the Divergence function \\(\\mathcal{D}(\\cdot,\\cdot)\\) that measures the distance between the target model to be trained and the auxiliary model, facilitating a teacher\u2013student mechanism. At last, the thrid term is teh Experience function, introduced by a Penalty term \\(U(\\xi)\\), which incorporates the set of \"experience functions\" \\(f_k^{(\\theta)}\\) that represent external experience of various kinds for training the target model. With the follwoing assumptions, we can frame this problem as a Expecttaion Maximization (EM) procedure (EM is also a very very interesting topic that I want to go into a little bit, for now I have attached an example of EM for binomial mixture model below).</p> <p> <p>\u2699 Example of Expectation Maximization</p> </p> <p>Uncertainty: Maximizing \\(\\mathcal{H}(q)\\) encourages the model to maintain a certain level of 'spread' or variability, implicitly allowing for more uncertainty in the distribution \\(q\\).</p> \\[ \\mathcal{H}(q) = -\\mathbb{E}_q[\\log q] \\quad\\text{(Shannon Entropy)} \\] <p>Divergence: Minimizing \\(\\mathcal{D}(q,p_\\theta)\\) pushes \\(p_\\theta\\) to better match the distribution \\(q\\), effectively acting as a divergence measure.</p> \\[ \\mathcal{D}(q,p_\\theta) = -\\mathbb{E}_q[\\log p_\\theta] \\quad\\text{(Cross-Entropy)} \\] <p>Experience: \\(f(t)\\) encodes knowledge or constraints from external sources or data, influencing the target model\u2019s learning process.</p> <p>With these asumption, the derived update would take in the following form. In EM, parameters doesn\u2019t matter, the hidden distribution is what impact all important information, then we just optimize parameter with regard to this distribution is fine. The below illustrate an analytical ideal \\(q\\) distribution that comes from SE (incorporating both uncertainty, divergence, and experiences), with such \\(q\\) distribution, we just directly MLE the parameter (same with minimizing the KL).</p> \\[ \\text{(E-Step):}\\quad q^{(n+1)}(t) = \\frac{\\exp\\left\\{\\frac{\\beta \\log p_{\\theta^{(n)}}(t) + f(t)}{\\alpha}\\right\\}}{Z} \\] \\[ \\text{(M-Step):}\\quad \\theta^{(n+1)} = \\arg\\max_{\\theta} \\mathbb{E}_{q^{(n+1)}(t)}\\left[\\log p_{\\theta}(t)\\right] \\] <p>We wouldn't dive into the specific formulation and mathamatical deriviation of such model (I have attached my note that trys to conduct some of the deriviation below), but notice that this is a constraint solvcing problem! We will go over one instance of SM (MLE) to illustrate this idea.</p>"},{"location":"articles/mathamatics/constraint/#from-sm-rightarrow-mle","title":"From SM \\(\\rightarrow\\) MLE:","text":"<p>For an arbitrary configuration \\((x_0,y_0)\\), its probability \\(p_d(x_0,y_0)\\) under the data distribution can be seen as measuring the expected similarity between \\((x_0,y_0)\\) and true data samples \\((x^*,y^*)\\), and can be written as:</p> \\[ p_d(x_0,y_0) = \\mathbb{E}_{p_d(x^*,y^*)} \\left[ \\mathbb{I}^{(x^*,y^*)}(x_0,y_0) \\right] \\] <p>Here the similarity measure is \\(\\mathbb{I}^{(x^*,y^*)}(x,y)\\), an indicator function that takes the value 1 if \\((x,y)=(x^*,y^*)\\) and 0 otherwise. The experience function would thus be defined as:</p> \\[ f := f_{\\text{data}}(x,y;D) = \\log \\mathbb{E}_{(x^*,y^*)\\sim D}\\left[\\mathbb{I}^{(x^*,y^*)}(x,y)\\right] \\] <p>Plugging this into the teacher model for the expected \\(q\\)-distribution, we have:</p> \\[ q(x,y) = \\frac{\\exp\\left\\{\\frac{\\beta \\log p_{\\theta}(x,y) + f_{\\text{data}}(x,y;D)}{\\alpha}\\right\\}}{Z} \\] <p>Notice that under this condition we don't care about the \\(\\beta \\log p_{\\theta}(x,y)\\) term (derived from divergence). Since we don't care about the true distribution \\(q\\)'s distance to \\((p_{\\theta}(x,y))\\) and instead we just want to fit the data (which is what MLE is doing):</p> \\[ q(x,y) = \\frac{\\exp\\{f_{\\text{data}}(x,y;D)\\}}{Z} \\approx \\tilde{p}_d(x,y) \\] <p>Then we maximize where the derived \\(q\\) distribution is the direct data distribution since \\(q\\) is retrieved from an indicator function on the data distribution. This is exactly the definition of MLE.</p> \\[ \\max_{\\theta} \\mathbb{E}_{t \\sim \\tilde{p}_d(x,y)}[\\log p_{\\theta}(t)] \\] <p>Essentially, the SE provides a new perspective of looking at all types of learning as a instance of a constraint solving problem. If we say RL is a search and optimization process under the reward constraint, then we can also think of supervised learning not as finding patterns in data, but as a optimization or a search in the landscape of weights with the constraint of data. Think how we are doing projected gradient descent (Hard constraint) or regularized gradient descent (Lagrangian constraint) where we are projecting the steps onto a subspace following the constraint requirement (refer to this article). Then our constraint in supervised setting is just constraining (a strict constraint) all posible weight world steps to a data space. For more information:</p> <p> <p>\u2699 Notes on Standard Model of Machine Learning</p> </p>"},{"location":"articles/mathamatics/constraint/#variational-autoencoder-vae-constraint","title":"Variational Autoencoder (VAE) Constraint","text":"<p>We have briefly mentioned the name of EM in the previous section. Turns out that, in practice, how we optimize EM is also a constraint optimization process. To quickly recap, the key of vanilla EM is to gradually find such \\(q\\) (posterrior) distribution that captures all the hidden variable and we maximize based on this \\(q\\) distribution. Usually, we optimize the Evidence of Lower Bound (ELBO) objective (the equation showing here is for the KL divergence definition).</p> \\[ \\mathcal{L}(\\theta, x) = \\mathbb{E}_{q(z|x)} \\left[ \\log \\frac{p(x, z | \\theta)}{p(z | \\theta)} \\right] - KL(q(z|x) || p(z | x, \\theta)) \\] <p>I have also attached the proof of deriving both ELBO + KL divergence and ELBO + entropy for reference in the links below:</p> <p> <p>\u2699 Proof of ELBO + Entropy</p> </p> <p> <p>\u2699 Proof of ELBO + KL Divergence</p> </p> <p>Deriving such posterrior distribution in simpler examples such as Binomial Mixture Model would be tractable. lHowever, derving it in much more complicated situations may get very tedious (i.e. Baysian Mixture of Gaussian's posterriror distribution):</p> \\[ \\begin{align*}         p(\\mu_{1:K}, z_{1:n} | x_{1:n}) = \\frac{\\prod_{k=1}^{K} p(\\mu_k) \\prod_{i=1}^{n} p(x_i | z_i) p(z_i | \\mu_k)}{\\int_{\\mu_{1:K}} \\sum_{z_{1:n}} \\prod_{k=1}^{K} p(\\mu_k) \\prod_{i=1}^{n} p(x_i | z_i) p(z_i | \\mu_k)} \\end{align*} \\] <p>Naturally, an question that we would be asking is whether we can do a approximation of such posterrior distribution. This is known as Variational Inference (as compared to what we do in traditional EM as inference). Traditional method would involve factorizing this posterrior to many independent Gaussian distrbution (Mean Field) or using re-parametrization tricks to approximate any family of distribution (Black Box Inference). As neural networks becomes more popular, more modern approaches such as Variational Autoencoder (VAE) becomes more popular where it reframes the original ELBO objective in EM into a generative model format of \\(p_{\\theta}(x|z)\\) and adding the same constraint of the KL divergence.</p> \\[ \\log p_{\\theta}(x|z) = \\log \\left[ \\frac{p_{\\theta}(x, z)}{p_{\\theta}(z)} \\right] \\] <p>Thus, the whole VAE expression becomes the following.</p> \\[ \\mathcal{L}(\\theta, \\phi, x) = \\mathbb{E}_{q_{\\phi}(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] - KL(q_{\\phi}(z|x) || p(z)) \\] <p>When taking the gradient, we can borrow the same re-parametrize trick from Black Box Inference where we treat this complicated distribution of \\(z\\) as a transformation on a simpler distribution that we know of. We can then take gradient of the encoder \\(q_{\\phi}\\) distribution with respect to \\(\\phi\\) and also gradient of the decoder \\(p_{\\theta}\\) distribution with respect to \\(\\theta\\).</p> \\[ \\begin{align*} \\nabla_{\\boldsymbol{\\phi}} \\mathcal{L} &amp;= \\mathbb{E}_{\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0,1)} \\left[ \\nabla_{\\boldsymbol{\\phi}} \\left[ \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}, \\mathbf{z}) - \\log q_{\\boldsymbol{\\phi}}(\\mathbf{z} | \\mathbf{x}) \\right] \\nabla_{\\boldsymbol{\\epsilon}} \\mathbf{z}(\\boldsymbol{\\epsilon}, \\boldsymbol{\\phi}) \\right] \\\\ \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L} &amp;= \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z} | \\mathbf{x})} \\left[ \\nabla_{\\boldsymbol{\\theta}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}, \\mathbf{z}) \\right] \\end{align*} \\] <p>Notice that this is a constraint solving problem again. We are maximizing the first term (decoder) to get improved in acccuracy while minimizing the second term (encoder) which is measuring divergence. With more trials, we get better encoder,hence, better decoder, and hence, better encoder. In the same trend of thoughts, further extensions of VAE such as Variational Information Bottleneck (IB) can alsobe framed into an constraint optimization as well.</p>"},{"location":"articles/mathamatics/constraint/#practical-doability-constraint","title":"Practical \"Doability\" Constraint","text":"<p>This constraint solving philosophy does not live in theory only, it is also widely used in practical domains, but just from an different perspective. I will be describiing two eaxmples that I think illustrate this idea quite well, one from the domain of  recommender system and one from the domain of database management.</p>"},{"location":"articles/mathamatics/constraint/#recommender-system-and-netflix-price","title":"Recommender System and Netflix Price","text":"<p>Without going too much into recommender and the Netxflix price, this price is for a contest of seeing who can build the best recommender system that best predict the rating changes in actual Netflix user data. Turns out the best model was the one that captured temporal trends (designed temporal user bias as a parametric function to fit the shape of the data) in ratings by over-engineered towards the particular dataset that the contest was using (not over-fit training data, but over-engineer on the construct of the entire dataset). It teaches us a few really nice engineeing lessons that aligns with what we said before, that true intelligence comes from the data, you should only use a temporal model if your data tells you so, not just by imaginations. There is no \"penacea\" model for recommendation, it's not like the introduction of transformer would suddenly solve all problem in recommender domains. You want to over-engineer and squeeze all the values from the data of a particular problem and in the particular domain.</p> <p>The data (projecting onto data space similar to what we discuessed in gradient descent) and domain (telling you what you  can and can't do, what cost you can afford, and what you can extract from the data.) is essentially a constraint that we put on the model (The goals in the domain may change, the constraint always changes, and the model that is designed to the particular constraint also changes).</p>"},{"location":"articles/mathamatics/constraint/#database-management","title":"Database Management","text":"<p>When thinking about database management, something that people wouldn't really think to have anything to do with optimization (just talking about the higher level management here, not optimizations in data access and retrieving), is actually filled with need of constraint solving.</p> <p>Imagine that you want to store population data with precise geographical locations and you have data for the overall region but not to each clusters of where people are living at. In theory you can use settlelite images and establish a computer vision task with some clustering algorithm to figure out where are the major cluster that people live at. In theory you can, but what if this population data storage is only one small part of your project, your actual problem is trying to design optimal transport? This is a constraint, a constraint on the cost and efficiency of what you can do: \"sometimes taking the mean is not worst than doing computer vision\".</p>"},{"location":"articles/mathamatics/constraint/#life-tree-constraint-solving","title":"Life? Tree Constraint Solving? \ud83c\udf32","text":"<p>Maybe life itself can be framed as a constraint solving or constraint search problem. I think that life itself is not even a supervised problem, you can't just copy someone else's \"success models\" ad hope it works, let along having an analytical solution. The enviornment would take you to the next stage and you can only take the actions. It is a search and optimization problem with a constraint that may not be a hinderance but rather a guidance: the tree yoiu have grown.</p> <p>You should have grown your tree independently from other trees, doing your search and generating more backtracking statistics along the way (similar to the idea mentioned on my ideology page, referencing to Monte Carlo Tree Search). With any new tasks you try to solve or any new paradigms that you step in, you don't try to just adapt to someone elses's tree but rather branch out a leaf from your tree to see what the returnning statistics tells you. You have traveled a long way, expanded the tree to so far, and everything works just fine, so you should trust your \\(q\\) function, trust the constraint that your history and your tree have given upon you.</p> <p>Don't be afraid to branch out to unknown places since the key of going to the unknown is to make mistakes. No matter where you go, your tree leaves a mark in the space and as the further you travel to, the more marks you leave and when they are abundant enough, they form a shape, a more intricate machine, that tell you who you are. At last, I want to put down a quote that inspired me greatly from professor Gao: \"with every try, you have explored the space a little bit more, grown the subtree a little bit deeper, and pushed more values into the table. Success never comes from one good state but rather the path you have explored and the large subtree you have built: The tree has been explored and nothing is lost\".</p>"},{"location":"articles/mathamatics/optimization/","title":"Twitch on Convex Optimization Theory","text":"Kaiwen Bian 20 min read \u00b7 Dec 11, 2024 <p>The beauty with convex optimization is that we can use a theoritical perspective to discuss the properties of the algorithm: \"It will stop and it will be the optimal\". More importantly, when we make \"a little twitch\" on the formulation of the problem or how we view the problem, we see a completely different algorithm with a different perspective. There is a consistent stream of thought in convex optiization that derives everything from Taylor's theory. If I want t sum up what I know about convex optimization, it would be:</p> <ol> <li>Taylor's theorem</li> <li>Try to do incremental/dynamic update</li> <li>We just need to think and make twitches from a different perspective.<ul> <li>We will see that GD local convex perspective \\(\\rightarrow\\) Newton's method \\(\\rightarrow\\) CGD.</li> <li>We will also seee that GD \\(\\rightarrow\\) GD + M (with twitch from GD) \\(\\rightarrow\\) GD + N.A.</li> </ul> </li> <li>Anything cna be framed into a optimization scheme question, just with different constraint that needs to be satisfied.</li> </ol> \\[ \\min_{w \\in \\mathbb{R}^p} \\frac{1}{N} \\sum_{i=1}^N \\ell(\\sigma(x_i; w), y_i) + \\lambda R(w) \\] <p>It may all sound very vague, but this article will make it more clear. This article is not intended for a detailed proof (proof are attached as pdf files in each section), but more of a intuition of how convex optimization leads from one to another.</p>"},{"location":"articles/mathamatics/optimization/#gradient-descent-in-different-lenses","title":"Gradient Descent In Different Lenses","text":"<p>Gradient descent is an extremely popular algorithm that is highly used in modern machine learning (particularly variants of it like the ADAM optimizor used commonly in deep learning context). This is not only because it has good practical performances but also because it comes with strong theoritical guarantees. In this section, we will use differnt perspective to look at gradient descent first.</p>"},{"location":"articles/mathamatics/optimization/#taylors-theorem-represents-fundamental-theorem-in-calculus","title":"Taylor's Theorem Represents Fundamental Theorem In Calculus","text":"<p>If you believe in calculus, then you should believe in taylor expansion as taylor theory is essentially an deriviation from teh fundamental theorem of calculus. If we have two points and their appropriate function values, then calculus tells us that:</p> \\[ F(b) - F(a) = \\int_a^b F'(x) \\, dx \\] <p>Then for the same expression we can say that:</p> \\[ f(x + p) - f(x) = \\int_0^1 f'(x + \\alpha p) \\, d\\alpha = \\int_0^1 \\nabla f(x + \\alpha p)^\\top p \\, d\\alpha \\] <p>We can make an simple assumption that if the next point is close enough, the gradient will be the same, that is:</p> \\[ \\int_0^1 \\nabla f(x + \\alpha p)^\\top p \\, d\\alpha \\approx \\nabla f(x)^\\top p \\] <p>This simplifies to the following and this is exactly what taylor theorem is giving us:</p> \\[ f(x + p) = f(x) + \\nabla f(x)^\\top p \\]"},{"location":"articles/mathamatics/optimization/#taylors-theorem","title":"Taylor's Theorem","text":"<p>Taylor's theory is an extremely core concept in convex optimization as many of convex optimization is about \"how to satisfy taylor theory such that we have certain part less than some other part\". Essentially, Taylor's theorem talks about how we can estimate an point in the function \\(f(y)\\) from using another point on the function $f(x) with the curvature at \\(x\\) scaled by the distance between \\(x\\) and \\(y\\). The most known form wpuld be written like:</p> \\[ f(y) \\approx f(x) + f'(x)(y-x) + \\frac{1}{2}f''(x)(y-x)^2 + \\cdots \\] <p>However, we can make things a little bit more fancy by using a recursive definition in \\(R^n\\):</p> \\[ f(\\vec y) = f(\\vec x) + \\nabla f(\\vec z)^T (y - \\vec x) \\] <p>This is the recursion definition of Taylor's theoy, it recursively unfold th whole expression again using \\(\\vec z \\in (\\vec x, \\vec y)\\) (imagine choosing another point between \\(x\\) and \\(y\\) to help estimate \\(y\\), just like picking \\(y\\) in the first place). One tric here is that the previous expansion is an approximation, but this recursion definition can have equality in it. The magic of gradient decent comes when we assume this \\(\\vec y\\) is our next point \\(\\vec y = \\vec x + \\mu \\vec v\\) where we are moving along the direction of \\(\\vec v\\).</p> \\[ f(\\vec x + \\mu \\vec v) = f(\\vec x) + \\nabla f(\\vec z)^T (\\mu \\vec v) \\] <p>Ideally, if we  are doing gradient descent, we want to have the next point taking a lower function value than the previous one, meaning that we want:</p> \\[ f(\\vec x + \\mu \\vec v) \\leq f(\\vec x) \\] <p>or teh equivalence \\(f(\\vec x + \\mu \\vec v) - f(\\vec x) \\leq 0\\). For a small enough \\(\\tilde{\\mu}\\) and a continuous function \\(f\\)s, the descent direction of \\(\\vec v\\) when \\(\\vec v \\cdot \\nabla f(\\vec x) \\leq 0\\) is also the descent direction for \\(f(\\vec x + \\mu \\vec v)\\). To create \\(\\vec v \\cdot \\nabla f(\\vec x) \\leq 0\\), we need \\(\\vec  v = \\nabla f(\\vec x)\\), which is why the gd equation is in the form:</p> \\[ x^{(t+1)} = x^{(t)} - \\mu^{(t)} \\nabla f(x^{(t)}) \\] <p>To recap, we derived our descent direction \\(\\vec v\\) based on what we wnat to satisfy taylor theory such that \\(f(\\vec x + \\mu \\vec v) - f(\\vec x) \\leq 0\\). We have shown that there is a intuitive, but theoritical reason behind each step of why we are doing gradient descent.</p>"},{"location":"articles/mathamatics/optimization/#local-convexity-rightarrow-calculus-optimization","title":"Local Convexity \\(\\rightarrow\\) Calculus Optimization","text":"<p>From a different perspective, we can look at GD as doing a local descent. To be more specific, we assume that at each step, the Armijo condition (did not cover in this article, but the idea is essentially creating sort of a checker to gurantee local convexity before making the moves) is hold, then we have a local convex shape. In full Taylor expansion to the second degree, it cna be expressed as:</p> \\[ f(z) = f(x^{(t)}) + \\nabla f(x^{(t)})^T (z - x^{(t)}) + \\frac{1}{2} (z - x^{(t)}) \\nabla^2 f(z)^T (z - x^{(t)}) \\] <p>This second \\(\\frac{1}{2} (z - x^{(t)}) \\nabla^2 f(z)^T (z - x^{(t)})\\) is super annoying as we have a recursive hessian term \\(\\nabla^2 f(z)\\) in it. However, what if we don't care about the curvature of the curvature? We make a simplified assumption that:</p> \\[ \\nabla^2 f(z) \\leftarrow \\frac{1}{\\mu}I \\] <p>This makes our expression much more simple, giving just</p> \\[ g(z) = f(x^{(t)}) + \\nabla f(x^{(t)})^T (z - x^{(t)}) + \\frac{1}{\\mu} || (z - x^{(t)}) ||^2 \\] <p>Notice that this is a quadratic-ish function in multi-dimension and then the shape we have is locally convex. We say that this \\(g(z)\\) looks very much like \\(f(z)\\) from a local perspective and more importnatly, this \\(g(z)\\) function is convex and we can use the traditional calculus method of:</p> \\[ \\nabla_z g(z) = 0 \\] <p>And we can retrieve the same result that</p> \\[ z^* = x^{(t)} - \\mu \\nabla f(x^{(t)}) \\] <p>Again, by assuming our function is locally convex (locally L-smooth to be specific, hessian bounded), we can hide away much complexity into approximations.</p>"},{"location":"articles/mathamatics/optimization/#optimality-guaranteed","title":"Optimality Guaranteed","text":"<p>We never really formally define what it means to be concvex here, but for now let's just say that convexity means that we have a Positive Semi-Deminite (PSD) hessian (this is not a definition but a result of teh definition). With gradient descnet + convexity + some tricks (teloscoping theory, series of convex functions, ...), we can have many powerful optimality guaranteed, namely: \"It will stop, it will converge, and we will be at the optimal position\". We will name a few in the following section, but first we can define some notion of convexity.</p>"},{"location":"articles/mathamatics/optimization/#convex-functions","title":"Convex Functions","text":"<p>The real definition of convexity is given by the following. Intuitively it is when we draw a line from any two points on this curve and this line will always stay above our function curve.</p> \\[ f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha) f(y) \\] <p>From this notion of convexity, we can proof that it leads to many others, we will name the important ones here:</p> <ol> <li>\\(f(y) \\geq f(x) + \\nabla f(x)^T (y-x)\\)</li> <li>\\(\\nabla^2 f(x) \\succeq 0, \\quad \\text{PSD}\\) or all eigenvalue above zero</li> <li>\\(\\nabla f(x) \\text{ is monotone, } \\langle \\nabla f(x) - \\nabla f(y), x-y \\rangle \\geq 0\\)</li> </ol> <p>Both 1 and 2 can be derived from convexity definition + Taylor theory.</p> <p>Notice that these 4 notions of convexity all come with different assumptions where the definition makes no assumption, 2 and 3 need twice differentiable functions and 3 need once differentiable function. Importantly, for convex function, local minimum \\(\\rightarrow\\) global minimum and when \\(\\nabla f(x) = 0\\), we can find such minimum.</p>"},{"location":"articles/mathamatics/optimization/#l-lipschitz","title":"L-Lipschitz","text":"<p>L-Lipschitz means that the gradient is bounded where \\(|| \\nabla f(x) || \\leq L\\) and equivalently we have:</p> \\[ ||f(x) - f(y)|| \\leq L ||x - y|| \\] <p>With a convex function \\(f\\), initial guess in range \\(||x^{(0)} - x^*|| \\leq R\\), total \\(T\\) iterations, and the learning rate \\(\\mu = \\frac{R}{L\\sqrt{T}}\\), we can guarantee that the average distance/error to the optimal coordinate \\(x^*\\) under function being bounded by:</p> \\[ f(\\frac{1}{T} \\sum^{T-1}_{s=0}x^{(s)}) - f(x^*) \\leq \\frac{RL}{\\sqrt{T}} \\] <p>This is the first proof we introduced such that we can say confidently: gradient descent will stop.</p> <p> <p>\u2699 Proof of L-Lipschitz + gradient descent convergence</p> </p>"},{"location":"articles/mathamatics/optimization/#l-smooth","title":"L-Smooth","text":"<p>L-smooth means that the hessian is bounded where \\(0 \\leq v^T \\nabla^2 f(x) v \\leq L\\) (since we are looking at the hessian, we need to bound by matrix norm (not formally defined here, but we will use this definition for now)).</p> \\[ ||\\nabla f(x) - \\nabla f(y)|| \\leq L ||x - y|| \\] <p>We can guarantee that at each step, the function value decreases:</p> \\[ f(x^{(t+1)}) \\leq f(x^{(t)}) - \\frac{\\mu}{2} ||\\nabla f(x^{(t)})||^2 \\] <p>And more importantly, we should have at least one \\(x^{(t)}\\) satisfying the following cndition (this root boost convergence speed hugely):</p> \\[ \\|\\nabla f(x^{(t)})\\| \\leq \\sqrt{\\frac{2(f(x^{(0)}) - f(x^*))}{\\mu T}} \\] <p>This is an incredably strong condition since non of the L-smooth proof used the fact that the function need to be convex, only that they are second degree differentiable and L-smooth, signifying that with just gradient descent: \"we will stop at some point and this would be optimal, no matter convexity or not\". For more information, reference to this note:</p> <p> <p>\u2699 Proof of L-Smooth + gradient descent convergence</p> </p>"},{"location":"articles/mathamatics/optimization/#all-families-comes-from-twitch","title":"All Families Comes From Twitch","text":"<p>Now after convexity and basic formulation of gradient descent, this is where we get to the interesting part, turns out that all the variants and instances of gradient descent (Coordinate descent, Uniform descent, Newton's method (i.e. ADAM), GD with Momentum, Nestrov Acceleration, Conjugate GD, ...) is all somewhat like GD but a little twitch on the theoritical formulation.</p>"},{"location":"articles/mathamatics/optimization/#norm-in-gradient-descent","title":"\"Norm\" in Gradient Descent","text":"<p>Turns out that there is actually a norm hidden in the gradient descent algorithm. When we use GD, we are saying that:</p> \\[ x^{(t+1)} - x^{(t)} = -\\mu \\nabla f(x) \\] <p>When swapping into Taylor's theorem</p> \\[ f(x^{(t+1)}) + \\nabla f(x^{(t)})^T (x^{(t+1)} - x^{(t)}) \\approx f(x^{(t)}) - \\mu \\nabla f(x^{(t)})^T \\nabla f(x^{(t)}) \\] \\[ \\approx f(x^{(t)}) - \\mu ||\\nabla f(x^{(t)})||^2 \\] <p>Normally speaking, this norm is a Eucledian norm or norm 2. However, in the same fashion, we can switch to norm-1 or norm-infinity$. This is essentially framing gradient descent as a constraint optimization problem. How do we optimize in the set of this sphere, or this dimond, or this pyramid? With different constraints, GD comes with different property, namely coordinate descent or uniform descent. They can be expressed out analytically:</p> \\[ L_1 \\rightarrow \\text{Sparse Coordinate Descent}: \\quad \\tilde{p}(x) = \\text{sgn}(\\nabla f(x)) \\cdot \\frac{|\\nabla f(x)|_i}{|\\nabla f(x)|_{\\max}} \\] \\[ L_{\\infty} \\rightarrow \\text{Uniform Descent}: \\quad \\tilde{p}(x) = \\frac{\\text{sgn}(\\nabla f(x))}{||\\text{sgn}(\\nabla f(x))||_{\\infty}} \\]"},{"location":"articles/mathamatics/optimization/#local-convex-perspective-rightarrow-newtons-method","title":"Local Convex Perspective \\(\\rightarrow\\) Newton's Method","text":"<p>Now forget everything we just discussed about gradient descent and we will look at it from  brand new perspective. We used \\(f(x) \\approx f(x^{(t)}) + \\nabla f(x^{(t)})^T (x - x^{(t)})\\) to derive gradient descent before, now let's use the second degree expansion like we did in GD's perspective from local convexity. However, instead of making a simplified assumption of \\(\\frac{1}{\\mu}\\) as the hessian, let's set the gradient directly to \\(0\\).</p> \\[ f(x) \\approx f(x^{(t)}) + \\nabla f(x^{(t)})^T (x - x^{(t)}) + \\frac{1}{2} (x - x^{(t)}) \\nabla^2 f(x)^T (x - x^{(t)}) \\] <p>Setting gradient \\(\\nabla_x f(x)\\) to zero:</p> \\[ \\nabla_x f(x) = \\nabla f(x^{(t)}) + \\nabla^2 f(x) (x - x^{(t)}) = 0  \\] \\[ \\nabla f(x^{(t)}) + \\nabla^2 f(x) (x - x^{(t)}) = 0  \\] \\[ \\nabla^2 f(x^{(t)}) (x - x^{(t)}) = -\\nabla f(x^{(t)}) \\] <p>Notice that \\(\\nabla^2 f(x^{(t)})\\) is a matrix (A), \\((x - x^{(t)})\\) is a vector (x), and \\(-\\nabla f(x^{(t)})\\) is a vector (b), so essentially we are actually solving the famous problem in linear algebra: how to find \\(Ax + b = 0\\). Alternatively, by changing \\(Ax = -b\\) into \\(x = - A^{-1}b\\):</p> \\[ x^{(t+1)} - x^{(t)} = - [\\nabla^2 f(x^{(t)})]^{-1} \\nabla f(x^{(t)}) \\] <p>Or we can write in the update form of Newton's method (fun fact, this is called Newton's mthod because the first algorithm os setting to zero is invented by Newton in 1D, is just taht we are doing in \\(R^n\\)) as:</p> \\[ x^{(t+1)} = x^{(t)} - [\\nabla^2 f(x^{(t)})]^{-1} \\nabla f(x^{(t)}) \\] <p>However, inverting this \"A\" matrix comes with insanly high computation cost and numerical instability. This is where our familier friend ADAM optimizor comes in, which is essetntially a Quasi-Newton method, or a family of algorithm that estimates/creates condition for this inverse to be easily calculated. So the whole field of Newton's method can be intuitively summarized as how do you do the inverse of A?</p> <p>Using Newton's method come with very interesting convergence property such that if we converge, we converge exponentially fast. Notice that this is not a contrastice mapping. The value of \\(\\frac{2h}{3L}\\) may be greater than \\(1\\), causing unconverged GD.</p> \\[ (1) \\quad ||x^{(t)} - x^*|| \\leq \\frac{2h}{3L} \\] \\[ (2) \\quad ||x^{(t)} - x^*||^2 \\leq \\frac{3L}{2h} ||x^{(t-1)} - x^*||^2 \\] <p>However, if we start at a good point in the right function where \\(\\frac{2h}{3L} \\leq 1\\), it converges expoennetially fast (this is really hard and usually Newton's method has many oscilations).</p>"},{"location":"articles/mathamatics/optimization/#approximation-perpective","title":"Approximation Perpective","text":"<p>Other than deriving Taylor expansion, there is actually another interesting perspective on these algorithms. First order gradient decsent is then doing an approximation of the complex function that we are trying to model with a line. Under this assumption, we can derive that the gradient dirction is the best direction. In the same sense, Netwton's method is then modeling our \"unknown\" function with a quadratic function (second degree taylor expansion), which makes it really nice to optimize as it is convex and global optimum always exist when this quadratic modeling assumption is true. All we need to solve is:</p> \\[ \\underset{p}{\\min} f(x+p) \\] <p>Any analytical optimization methods would need to make assumptions on the class of the functions that you are looking at, which may not be always true, or even close to true, when looking at some crazy functions. However, there are ways to try to maintain this tiny neighborhood of quadratic approximation at each steps.</p>"},{"location":"articles/mathamatics/optimization/#gradient-descnet-momentum-or-na","title":"Gradient Descnet + Momentum or N.A.","text":"<p>All discussion in the following few sections will be around a particular case of a convex function, a nice one (we wil generalize later):</p> \\[ \\phi(x) = \\frac{1}{2} x^T A x \\] <p>Now recall gradient descent and let's add a first order Markovian memory to it. If the current gradient direction aligns with the previous gradient direction, we move a little bit further (constructive interference) and do the opposite if we are exact opposite with previous gradient. This is called momentum and using this method we can avoid sudden updates in teh gradient (zig-zag shape) and create the decent in  much smoother way. Formally, we can write it as:</p> \\[ x^{(t+1)} = x^{(t)} - \\mu \\nabla f(x^{(t)}) + \\beta (x^{(t)} - x^{(t-1)}) \\] <p>To study the convergence property, we can stack up the current state and next state, just like in control theory calculating state transition. This way of analyzing convergence is very common and quite popular in optimization as well (the intuition is that we want the eigenvalues of this \\(M\\) matrix to be less than \\(1\\), then the system would approach towards stability).</p> \\[ \\begin{bmatrix} x^{(t+1)} \\\\ x^{(t)} \\end{bmatrix} =  \\underbrace{ \\begin{bmatrix} 1 - \\mu \\lambda + \\beta &amp; -\\beta \\\\ 1 &amp; 0 \\end{bmatrix}}_{M} \\begin{bmatrix} x^{(t)} \\\\ x^{(t-1)} \\end{bmatrix} \\] <p>Under a quadratic equation condition of \\(\\frac{1}{2} x^T A x\\) (we can generalize this quadratic equation later with strongly convex property), gradient descnet with momentum will converge under:</p> \\[ \\left(\\frac{k-1}{k+1}\\right)^t \\quad \\text{where} \\quad k = \\frac{\\lambda_{max}}{\\lambda_{min}} \\] <p>A similar approach (Nestrov Acceleration) works similar as momentum, but it goes  little bit further first before taking the gradient. It is less intuitive than gradient descent with momentum, but in practice is has nice property that out performs momentum, namely under continuou  environment and differential euqation environment.</p> \\[ (1) \\quad y^{(t+1)} = x^{(t)} + \\beta (x^{(t)} - x^{(t-1)}) \\] \\[ (2) \\quad x^{(t+1)} = y^{(t+1)} - \\mu \\nabla f(y^{(t+1)}) \\] <p>In general, we can say the convergence rate \\(|| x^{(t)} - x^* ||\\) from GD to GD + M to GD + N.A. as:</p> \\[ \\underbrace{(\\frac{k+1}{k+1})^t}_{\\text{GD}} \\rightarrow \\underbrace{\\left(\\frac{\\sqrt{k}-1}{\\sqrt{k}+1}\\right)^t}_{\\text{GD + M}} \\rightarrow \\underbrace{\\left(\\sqrt{\\frac{\\sqrt{k}-1}{\\sqrt{k}}}\\right)^t}_{\\text{GD + N.A.}} \\] <p>Notice that they all converges exponentially fast, but this exponentially fast is conditioned on this conditon number \\(k\\), essetially the ratio of largest eigenvalue to the smallest eigenvalue. With more square root bracket is added to it, it will make this number very small, which makes taking the exponential more powerful for minimizing it.</p>"},{"location":"articles/mathamatics/optimization/#newtons-method-rightarrow-conjugate-gradient-descent","title":"Newton's Method \\(\\rightarrow\\) Conjugate Gradient Descent","text":"<p>Let's go back to thinking about solving this problem mentioned in Newton's method, but let's still look at teh special case of \\(\\phi(x) = \\frac{1}{2} x^T A x\\) and so we want to solve the problem of \\(Ax^* - b = 0\\) where the gradient is \\(\\nabla \\phi(x) = Ax - b\\). We know that doing the inverse is very costly and numerically instable. So can we solve this problem without inversing A and maybe we can incrementally solve this issue. Let's first define the mathamaticla notion of conjugate, let's say that \\({p_1, ..., p_n}\\) is the conjugate of a Positive Definite (PD) matrix A if:</p> \\[ p_i^T A p_j = 0 \\quad \\text{if} \\quad i \\neq j \\] <p>This conjugate is a general notion of orthogonality and the idea is to maybe update in one orthogonal direction at each time and just not care of it (independent of future update). Intuitively, we can frame it like the following:</p> \\[ x^{(t+1)} = x^{(t)} + \\alpha_t \\vec{p}_t \\] <p>Where</p> \\[ \\alpha_t = \\underset{\\alpha \\in \\mathbb{R}}{\\arg \\min} \\phi(x^{(t)} + \\alpha \\vec{p}_t) \\] <p>The first ensures the orthogonal step and the second ensure the descent step. Notice that this second optimization problem is a easier problem to solve and close form solution does exist as we are only looking at a minimization problem that involve two vectors, way easier than the problem we begin with. Close form solution of optimal \\(\\alpha^*\\) would be:</p> \\[ d_t = \\alpha^* = \\frac{(b - Ax^{(k)})^T p_t}{p_t^T A p_t} = \\frac{- \\nabla \\phi(x^{(t)})^T p_t}{p_t^T A p_t} \\] <p>This almost looks like a gradient descent with just a bunch of other fancy multiplication in it. Turns out that we are actually doing a gradient descent that is projected onto the \\(p_t\\) conjugate axis. Taking the gradient in the component direction of \\(p_t\\). All of theses sounds very nice, but we still have one problem. How do we find the conjugate? Doing \\(p_i^T A p_j = 0 \\quad \\text{if} \\quad i \\neq j\\) is not a cheap operation. Let's do it like Bellman Update, let's use a dynamic way of updating the conjugate!</p> <p>Assume that each \\(p_t\\) only need the previous one \\(p_{t-1}\\) to be computed, we just need the current vector to be conjugate to the previous one, and if the chain forms for being conjugate, all of them will be conjugate vectors and we can throw away \\(p_o\\) to \\(p_{t-2}\\). Let's start with:</p> \\[ p_t = -\\nabla \\phi(x^{(t)}) + \\beta_t p_{t-1} \\] <p>Then we multiply \\(p_{t-1} A\\) to both side and try to make RHS zero.</p> \\[ p_{t-1}^T A p_t = -p_{t-1}^T A \\nabla \\phi(x^{(t)}) + \\beta_t p_{t-1}^T A p_{t-1} \\] <p>This has a close form solution again where we can calculate the optimal \\(\\beta_t\\) to satisfy this condition. The CGD algorithm becomes clear as well, we just need to randomly initiate \\(\\beta\\) and \\p_0$, then update base on this given constraint.</p> \\[ \\beta_t = \\frac{p_{t-1}^T A \\nabla \\phi(x^{(t)})}{p_{t-1}^T A p_{t-1}} \\] <p>Conjugate gradient descent is very very powerful (similar to gradient descent with momentum):</p> \\[ ||x^{(t)} - x^*||_A \\leq 2 \\left(\\frac{\\sqrt{k}-1}{\\sqrt{k}+1}\\right)^t ||x^{(0)} - x^*||_A \\] <p>However, the plain CGD algorithm may not work as efficiently in practice, which is why we usually use th redidual algorithm, which is described in more details in the note.</p> <p> <p>\u2699 Two expressions of the CGD algorithm</p> </p> <p> <p>\u2699 Code implementation of CGD</p> </p> <p>As a comparison to Newton's method, CGD is trying to make the original hard optimization problem to smaller problems \"along each direction\". Newton's method is trying to solve everything at once (in fact when the function is linear regression, the update rule for Newton's method is actually directly the analytical solution normal equation for linear regression) while CGD want to solve the problem along every single conjugate direction.</p>"},{"location":"articles/mathamatics/optimization/#beyond-convexity-optimality-may-be-guaranteed","title":"Beyond Convexity, Optimality May Be Guaranteed","text":""},{"location":"articles/mathamatics/optimization/#strongly-convex","title":"Strongly Convex","text":"<p>What happens beyond convexity? We can still talk about them in theory (though not as convineint as in convex situations). First we will introduce the concept of stronly convex, there are 2 ways of understanding it.</p> <ol> <li>Back to Taylor's theory, we are essentially swapping out the \\(\\nabla^2 f(z)\\) with the smallest eigenvalue of such matrix and that \\(\\nabla^2 f(z) \\geq CI\\). Mathamatically, this is \\(0 \\leq C \\leq \\lambda_{\\text{min}} \\nabla^2 f(z)\\).</li> <li>We are essentially constructing sort of a tangent curve instead of a tangent line in the original convex definition.</li> </ol> \\[ f(y) \\geq f(x) + \\nabla f(x)^T (y-x) + \\frac{C}{2} ||y-x||^2 \\] <p>and we can show that the above definition leads to the following:</p> \\[ f(x) - f(x^*) \\leq \\frac{||\\nabla f(x)||^2}{2C} \\] <p>When  strongly convex is achieved, L-smooth condition is matched, and when choosing learning rate as \\(\\frac{1}{\\mu}\\), we have a incrediablly strong convergence rate, an exponential convergence rate that is independent of the condition number \\(k\\).</p> \\[ f(x^{(t+1)}) - f(x^*) \\leq (\\frac{C}{L})^t (f(x^{(0)}) - f(x^*)) \\] <p>More critically, when strongly convex condition is matched, all the previous method's convergence property remains for functions that is strongly convex but not neccessarily just quadratic in the form of \\(\\frac{1}{2} x^T A x\\). This is really how strongly convex is used in practice. Practically speaking, when \\(f(w)\\) is convex and \\(R(w)\\) is c-strongly convex, then \\(f(w) + R(w)\\) is also c-strongly convex. This gives the power for regulaorizor to show its power. For instance, in ridge regression, other than just making a constraint on keeping the weights small, ridge regression gurantees a better convergence rate than just normal gradient descent. For more proof related content, visit the notes.</p> <p> <p>\u2699Strongly convex proof</p> </p>"},{"location":"articles/mathamatics/optimization/#pl-polyak-lojasiewicz-condition","title":"PL (Polyak-Lojasiewicz) Condition","text":"<p>When discussion in non-convex situations, we talk about the PL-Condition, which is eessentially a condition that looks very similar with strongly convex where strongly convex implies PL-condition but not vice versa.</p> \\[ f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\text{ satisfies } \\mu \\text{-PL-Condition if } \\forall x, y \\in \\mathbb{R}^n \\] \\[ \\frac{1}{2}||\\nabla f(x)||_2^2 \\geq \\mu (f(x) - f(x^*)) \\] <p>Essentially, we are putting a upperbound on the gradient saying that when we are far from \\(x^*\\), we have  big gradient to move faster. Importantly, PL-Condition has 2 important factors:</p> <ol> <li>PL-Condition can hold for non-convex functions and it acts in as sort of a strongly convex guarantees.</li> <li>If \\(f(x)\\) s L-smooth + \\(\\mu\\)-PL-Condition, then gradient descent with a step-size of \\(\\frac{1}{L}\\) will converge at a rate of</li> </ol> \\[ f(x^{(t)}) - f(x^*) \\leq \\left(1 - \\frac{\\mu}{L}\\right)^t (f(x^{(0)}) - f(x^*)) \\] <p>This is similar to strongly convex, very strong exponential convergence. In practice, we actually observes this phenomenon! Turns out that an over-parametrized neural network (highly non-convex function) can be written in a Neural Tangent Kernel form (where we use, again, the eigenvectors to hide away the complexity of matrix) and then we can derive that:</p> \\[ ||\\nabla L(w)||^2 \\geq 2\\mu L(w) \\] <p>Which satisfy the PL-Condition since the loss function at \\(x^*\\) should always be zero. This shows that neural network would converge as the rate of \\((1 - \\frac{\\mu}{\\beta})^t\\), it converges exponentially fast. This is exactly why we see exponential convergence in teh begining when we randomly initialize an neural network. This sort of example is what the PL-Condition is truly used for.</p> <p>The moral of the story is, with certain constraints/conditions satisfied, we can use theory to make something practically work!</p>"},{"location":"articles/mathamatics/optimization/#extra-resources-code","title":"Extra Resources &amp; Code","text":"<p>For more detailed information and code implementation of some of teh algorithms:</p> <p> <p>\u2699 Notes on convex optimization</p> </p> <p> <p>\u2699 Code implementation of some variants of gradient descent</p> </p>"},{"location":"articles/mathamatics/point_set_topology/","title":"From Set We Create All","text":"Kaiwen Bian 15 min read \u00b7 Mar 22, 2025 <p>Topology have always seem to be a interesting but puzzling field of study to me. When first introduced to the idea it seems to be highly abstractbut yet rigorous in nature. In this article, I want to goover some of the most fundamental properties of the field of point-set topology and illustarte the point of how it can be served as the most general form of mathematics. All of the ideas in this article is developed from Allen Hatcher's notes. Point-set topology is defining some of the fundamental properties and bases of mathematics from scratch, which is not usually my strength to discuss about something that is so theoritical with pure math, but I still find it to be extremely facinating and this is why I am writing this article. Paticularly, I want to stress on the abstract intuitive and generative nature of point-set topology.</p> <p>Different mathematics spaces from here, illustration of how topology is this bigger wreaparound all spaces.</p>"},{"location":"articles/mathamatics/point_set_topology/#point-set-topology","title":"Point-set Topology?","text":"<p>Point-set topology aims to develop an intuition about abstract spaces in a general, universal setting. It tries to define and derive intuitions for abstract spaces broadly, including real number spaces, probability spaces, and even spaces as general as metric spaces. Unlike traditional geometry, topology defines and explores geometric properties purely from a set theory perspective. The key is that these geometric properties does not only apply to geometric objects, but rather a space in general, topology discussses not only about the functions, but the space themselves. This approach makes point-set topology both fundamental and deeply abstract.</p> <p>The numbers and structures we traditionally use in mathematics, like real numbers, are merely specific instances of how logic can truly be defined mathematically. Similar to programming, topology requires defining a much broader abstract function class to fully encapsulate all possible scenarios.</p> <p>In essence, the mathematical objects studied in topology, called topological spaces, are defined by the rules or operations applied to sets. For instance, the real number space \\( \\mathbb{R}^n \\) is merely a specific example of a more general concept known as a topological space.</p> <p>There are many facinating concepts in point-set topolgy and this article is not served as a exahustive list, but rather an intuition. I will be discussing mainly about homeomorphisms and basis to showcase how topology is describing something so general using an method that is rather intuitive. There are so much more inpoint-set topology discussing about connectedness, compactness, quotient space, ect, that are equivalently amazing.</p>"},{"location":"articles/mathamatics/point_set_topology/#defining-topological-spaces","title":"Defining Topological Spaces","text":"<p>I will try to introduce some of the basic concepts in topology to illustarte how amazing it is. A topological space is defined by a set \\(X\\) together with a collection of subsets called open sets \\(\\mathcal{O}\\), which must satisfy the following conditions:</p> <ol> <li>The union of any collection of sets in \\(\\mathcal{O}\\) must be open.</li> <li>The intersection of any finite collection of sets in \\(\\mathcal{O}\\) must also be open.</li> <li>Both the empty set \\(\\emptyset\\) and the entire set \\(X\\) are open.</li> </ol> <p>Topology can be viewed as qualitative geometry, where two objects are considered topologically equivalent (homeomorphic) if one can be continuously deformed into the other without breaking or tearing, this is the classic idea of how a coffee mug is equivalent to a donut. Turns out some of the fundamental rules in calculus have similar representations in topology.</p>"},{"location":"articles/mathamatics/point_set_topology/#being-homeomorphic","title":"Being Homeomorphic","text":"<p>We can first try to define being homeomorphic. A fundamental question in topology is: when can two topological spaces be considered the same, even if their physical or geometric properties appear very different?</p> <p>A homeomorphism between two topological spaces \\((X, \\mathcal{T})\\) and \\((Y, \\mathcal{S})\\) is a function:</p> \\[ f: X \\rightarrow Y \\] <p>which satisfies three crucial conditions:</p> <ol> <li> <p>Bijective:</p> <ul> <li>The function \\( f \\) must be one-to-one (injective), meaning if \\( f(a_1) = f(a_2) \\), then \\( a_1 = a_2 \\).</li> <li>It must be onto (surjective), meaning every element \\( b \\in Y \\) has at least one element \\( a \\in X \\) such that \\( f(a) = b \\).</li> </ul> </li> <li> <p>Continuous:</p> <ul> <li>For every open set in \\( Y \\), its pre-image under \\( f \\) must be open in \\( X \\).</li> </ul> </li> <li> <p>Inverse is continuous:</p> <ul> <li>The inverse function \\( f^{-1}: Y \\rightarrow X \\) must also be continuous. Continuity must be checked both ways.</li> </ul> </li> </ol> <p>A homeomorphism ensures the two spaces have the same underlying topological structure, even if their representations or physical appearances differ significantly. Now, consider points in the unit circle \\( S^1 \\) in \\( \\mathbb{R}^2 \\) described by angles \\( \\theta \\). Polar coordinates establish a homeomorphism from \\( S^1 \\times (0,\\infty) \\) onto the plane excluding the origin, \\( \\mathbb{R}^2 - \\{0\\} \\). Specifically, we define:</p> \\[ f: S^1 \\times (0,\\infty) \\rightarrow \\mathbb{R}^2 - \\{0\\}, \\quad f(\\theta, r) = (r \\cos \\theta, r \\sin \\theta) \\] <p>This mapping is bijective because every point in the plane (excluding the origin) has unique polar coordinates \\( (\\theta, r) \\). It also takes basis sets of the form \\( U \\times V \\)\u2014where \\( U \\) is an interval of angles \\( (\\theta_0, \\theta_1) \\) and \\( V \\) is an interval of radii \\( (r_0, r_1) \\)\u2014to open polar rectangles, which form a basis for the topology on \\( \\mathbb{R}^2 - \\{0\\} \\).</p> <p>Hence, from a topological perspective, the space of polar coordinates \\( S^1 \\times (0,\\infty) \\) is essentially the same as the plane without the origin. This illustrates that changing coordinates or parameterizations is essentially a form of homeomorphism. In general, a homeomorphism maps one topological space onto another in a way that preserves their structural properties, indicating that any reparameterization of a space (like changing from Cartesian to polar coordinates) can be viewed as a homeomorphism.</p> <p>I find this example to be very interesting exactly because it connects some of the abstract ideas in topology back to what we have learned maybe back in freshman year of college. To me this is the power of topology describing familier things, the most generic and abstract form, while still maintaining its intuition (i.e. continuity doesn't need epsilon-delta definition in analysis, we just need it to be a \"line\" that is connected). I still remanber the first thing that my professor said in our lecture was: \"I think the definition given by analysis is not intuitive enough, let's define things more intuitive.\"</p>"},{"location":"articles/mathamatics/point_set_topology/#generative-nature-of-topology","title":"Generative Nature of Topology","text":"<p>Topology emphasizes finding an origin story or a \"key generative set\" from which all other subsets and structures of the space can arise. For instance, in \\(\\mathbb{R}^n\\), all open sets can be generated by open balls. Thus, the generative definition, trivial for the case of real numbers, becomes profound when generalized to abstract spaces:</p>"},{"location":"articles/mathamatics/point_set_topology/#basis-for-a-topology","title":"Basis for a Topology","text":"<p>A basis is a fundamental concept in topology, providing a convenient way to define a topology by using simpler, smaller sets known as basis elements. A basis \\( \\mathcal{B} \\) for a topology on a set \\( X \\) is a collection of open sets satisfying two conditions:</p> <ol> <li>Every point in \\( X \\) is contained in at least one basis element.</li> <li>If a point \\( x \\) belongs to the intersection of two basis elements \\( B_1 \\) and \\( B_2 \\), there must exist a third basis element \\( B_3 \\) such that:</li> </ol> <p>$$    x \\in B_3 \\subseteq B_1 \\cap B_2    $$</p> <p>The importance of a basis lies in its ability to generate all open sets of a topology through unions of basis elements. For instance, consider the real numbers \\( \\mathbb{R} \\). The collection of all open intervals forms a basis for the standard (Euclidean) topology on \\( \\mathbb{R} \\). Hence, formally, the Euclidean topology basis is defined as:</p> \\[ \\mathcal{B} = \\{(a,b) \\mid a,b \\in \\mathbb{R}, a &lt; b\\} \\] <p>And that every open set in \\( \\mathbb{R} \\) can be expressed as a union of these open intervals.</p>"},{"location":"articles/mathamatics/point_set_topology/#magical-open-ball","title":"Magical Open Ball","text":"<p>In a metric space \\( (X,d) \\), the concept of a basis emerges naturally from the notion of open balls. Given a metric \\( d \\), an open ball centered at a point \\( a \\in X \\) with radius \\( r &gt; 0 \\) is defined as:</p> \\[ B_r(a) = \\{ x \\in X \\mid d(a,x) &lt; r \\} \\] <p>The collection of all such open balls constitutes a basis for the metric space topology:</p> \\[ \\mathcal{B} = \\{B_r(a) \\mid a \\in X, r &gt; 0\\} \\] <p>This shows a powerful link between metric spaces and topology: every metric inherently defines a natural topological structure based on open balls, highlighting an important geometric intuition in topology.</p>"},{"location":"articles/mathamatics/point_set_topology/#subspace-topology","title":"Subspace Topology","text":"<p>Given a topological space \\( (X, \\mathcal{T}) \\) and a subset \\( Y \\subseteq X \\), we can define a topology specifically on the subset \\( Y \\). This induced topology is called the subspace topology.</p> <p>Formally, the subspace topology \\( \\mathcal{T}_Y \\) is defined as:</p> \\[ \\mathcal{T}_Y = \\{ U \\cap Y \\mid U \\in \\mathcal{T} \\} \\] <p>In simpler terms, a set is open in the subspace \\( Y \\) if and only if it is the intersection of \\( Y \\) with an open set in the original space \\( X \\). The idea behind the subspace topology is intuitive: it allows us to \"zoom in\" and locally examine topological properties within a subset without losing consistency with the broader topological structure.</p> <p>Consider the example of a circle \\( S^1 \\) embedded in \\( \\mathbb{R}^2 \\). The subspace topology on \\( S^1 \\) is generated by intersections of open balls (open sets in \\( \\mathbb{R}^2 \\)) with the circle itself. For example, open arcs on the circle come from the intersection of open balls in \\( \\mathbb{R}^2 \\) with \\( S^1 \\). Specifically, if we define:</p> \\[ S^1 = \\{(x,y) \\in \\mathbb{R}^2 \\mid x^2 + y^2 = 1\\} \\] <p>Then, an open arc on \\( S^1 \\) such as:</p> \\[ U = \\{(x,y) \\in S^1 \\mid x &gt; 0, y &gt; 0\\} \\] <p>can be expressed as the intersection of \\( S^1 \\) with an open set (e.g., the first quadrant in \\( \\mathbb{R}^2 \\)).</p>"},{"location":"articles/mathamatics/point_set_topology/#basis-for-subspace-topology","title":"Basis for Subspace Topology","text":"<p>A basis for the subspace topology on \\( Y \\subseteq X \\) is similarly derived from the basis of the larger space \\( X \\):</p> <ul> <li>If \\( \\mathcal{B} \\) is a basis for topology \\( \\mathcal{T} \\) on \\( X \\), then the set:</li> </ul> \\[ \\mathcal{B}_Y = \\{ B \\cap Y \\mid B \\in \\mathcal{B} \\} \\] <p>forms a basis for the subspace topology on \\( Y \\).</p> <p>For example, the Euclidean basis for \\( \\mathbb{R}^2 \\) generates a basis for any line or curve within the plane, simply by intersecting these open sets with the subspace.</p> <p>This interplay between bases and subspaces provides a powerful method to systematically analyze topological properties within subsets, demonstrating topology\u2019s deep structural versatility.</p>"},{"location":"articles/mathamatics/stochastic/","title":"Unfolding Stochasticity Sequentially","text":"Kaiwen Bian 10 min read \u00b7 Jun 12, 2024"},{"location":"articles/mathamatics/stochastic/#setting-bases-of-sequential-process-in-nature","title":"Setting Bases of Sequential Process In Nature","text":"<p>       Stochastic processes is abut understanding from local to gloabl, by having single transition probability matrix (discrete) or transition probability function (continuous) and trying to understand some global behavior of such chain or processes (i.e. absorption time, expected time of termination, or stationary distribution). WHat I found stochastic processing to be really amazing is how it is trying to model        how randomness and stochasticity may play out in nature, and not at one time stamp but rather across different time stamp sequentially. It try to reason and capture some details about the nature, to undertsand a chain of        \"randomness\" seuqntiall (not so random when you cna model it). Compare to random variables that reasons about how \"randomness\" plays out at one time stamp, stochatsic processes may dive into the interactions between \"randomness\" across time, to        dive into their interactions, seeing how they sequentially interact and the prior effect the laters (of course under markov property only one layer of looking backward is needed).   </p> <p>     The interesting perspective is that all the discrete Markov chain technique is actually an instance of the larger class  of Continuous Time Markov Chain. We will gho into this later.    </p> <p>Random Walk: Key Example</p> <p>       In this section I want to discuss (not techniqually) about how amazing this field may be with one classic example that bridges across discrete Markove Chains (MC), Continuous Time Markov Chains (CTMC) and COntinuous Time Continuous State Brownian Motion (BM): Random Walk (RW).   </p> <p>\u2699 Some Notes on Stochastic Processes</p> <p>Some extension with more techniqual details on the three interpretations of Continuous Time Stochastic Processes.</p> <p>\u2699 More Techniqual Details</p>"},{"location":"articles/mathamatics/stochastic/#markov-chain-analysis-into-stochaticity","title":"Markov Chain: Analysis Into Stochaticity","text":"<p>       When considering about a sequential process that have \"randomness' depending on teh previous \"randomness\", onemust consider using ideas from conditional probability, which discusses about a distribution given that a different distribution has occured. No matter MC, CTMC, or BM, they all hold/are established on        a very powerful property known as the Markov Property that makes many of the techniqual details working with conditional probability much easier. Mathamatically:       $$       P(X_n = x | X_{n-1} = x_{n-1}, \\dots, X_0 = x_0) = P(X_n = x | X_{n-1} = x_{n-1})       $$        $$       P(X_{n+1} = x_{n+1} | X_n = x_n) = P(X_{n+1} = x_{n+1} | X_n = x_n, \\dots, X_0 = x_0)       $$       Saying that the \"current\" \\(X_n\\) does not depends on distance past but rather only the nearest past \\(X_{n-1}\\) and that the \"future\" \\(X_{n+1}\\) does not depend on the past but only the \"present\" \\(X_n\\).   </p> <p>       With Markov Property holding, mathamatician can reason with the chain or such sequential process much more easily and some global probability such as expected time of termination, return probability, stationary distribution can be analytically calculated. One key theme in stochastic processes is that one might        want to setup an recureent (recursive) system to reason about how things unfold, then finding  generalized pattern (explicit solution) from such system of equation. Remanber that a recurrence system is equivalently represented in linear algebra as a system of equation when flatening all of it out, then numerical optimization can        also be conducted. Taking an example of the return probability:       $$       u_{ik} = P_{ik} + \\sum_{j=0}^{k-1} P_{ij} u_{jk} = P(X_{T} = k | X_0 = i)       $$       which talks about teh probability of terminating the chain at point \\(k\\) given that it starts on point \\(i\\). Equivalently, it can be written in matrix format for numerical optimization:       $$       u^{(k)} = (I - Q)^{-1} R^{(k)}       $$       Same idea can be used to reaosn with expected time of termination:       $$       E[T | X_0 = i] = 1 + \\sum_{j=0}^{r-1} P_{ij} \\omega_j = \\omega_j       $$       These types of reaosnings are known to be First Step Analysis and it is one of the mos commonly used idea to reason with a sequential event in stochastic processing.   </p> <p>       To the core example that I want to cover in this section: Random Walk (RW). RW in the discrete state + discrete time condition is essentially a Markov Chain with the transition probability matrix:       $$       \\Pr(X_{n+1} = j | X_n = i) =        \\begin{cases}        q_i &amp; \\text{if } j = i-1 \\\\       r_i &amp; \\text{if } j = i \\\\       p_i &amp; \\text{if } j = i+1       \\end{cases}       $$       under the constrained that \\(q_i + r_i + p_i = 1\\).   </p> <p>       Knowing that this is a Markov Chain and its transition probability matrix, much can be conducted and all of the analysis that was mentioned earlier can be used to help finding a global state of the chain. It        is worth notice that though vanilla RW is only in discrete state + discrete time condition, it is actually a core example taht can be carried over to more complicated situations. Even under discrete condition,        it can serev a pretty fine model for stochastic modeling in some practical senerios. For instance, gamble's ruin modeling or modeling stochasticity to see if learning has occured.   </p>"},{"location":"articles/mathamatics/stochastic/#continuous-time-markov-chain-discretize-functional-analysis","title":"Continuous Time Markov Chain: Discretize + Functional Analysis","text":"<p>       Now this is where tings gets a lot more complicated because CTMC is trying to reason under the condition that time is continuous, which introduces many more deficulties mathamaticaly. Remenber that we said one core idea in stochastic processes is to setup a recurence system and        try to solve such system? When state is discrete, we can count them as steps and do recursion in that fashion, but when states are continuous, recursion can still be conducted, but sometimes with differential equation, which is not something that w  want to do, the complexity is very high.        When studying more into CTMC, one may encounter many classic processes that are CTMC, namely most of the classic examples deals with reasoning \"when customer come into and leave a store\":       <ul> <li> <p>                   Poisson Process (PP): constant \\(\\lambda\\) (rate), random time, unidirectional jump               </p> </li> <li> <p>                   Pure Birth Process (PBP): dynamic \\(\\lambda\\) (rate), random time, unidirectional jump               </p> </li> <li> <p>                   Birth and Death Process (BDP): dynamic \\(\\lambda\\) (rate), random time, random jump               </p> </li> </ul>       These process in nature are easier to be described by rate diagram, which is how they are defined (rate diagram description is one of the three ways to interpret an CTMC) and we can use random variables to monitor such process or deducing transition probability function (i.e. increments of an        Poisson Process follows Poison distribution random variable). Maybe in some sense, these random variavbles may be a good abstraction to represent the process just like how weights can be used as a representation of a predictive function? (ideas in Jump &amp; Hold description?) </p> <p>       Problem with description comes where it becomes kind of hard trying to find some global property when reasoning under this paradigm. Thus, we can extend to the second and third interpretation of an CTMC: Jump &amp; Hold Description and Infinite Decimal Generator Decription. This is where things really gets complicated        because the first need to project an contnious chain to an discrete chain by discretize the continuous scale into a discrete numbers of continuous random variable depending on teh process in interest and the second is invlolved in using Functional Analysis techniques and theorems to find an Q Matrix that captures time dependent information        while being itself a time independent matrix (hence the name genertaor). Here is a few names that might be worth looking into:       <ul> <li> <p>Semigroup Property &amp; Hille-Yoshida Theorem: Functional Analysis aspects of stochastic processes that's the bases of the Infinite Decimal Generator Description, creating the Q matrix for generating transition probability functions.</p> </li> <li> <p>Kolmogorov Forward/Backward Equation: Turning transition probability function into a system of differential equation, also connecting to dieas later on in Brownian Motion using  as set of differential equation to monitor a system.</p> </li> </ul> </p> <p>Different Description of Continuous Time Markov Chain</p> <p>       However, though being extremely complex, when dive into the analysis, it has a deep connection back with random walk, but just under a much complicated condition.   </p>"},{"location":"articles/mathamatics/stochastic/#brownian-motion-bridging-analyst-probabilist","title":"Brownian Motion: Bridging Analyst &amp; Probabilist","text":"<p>       Moving to Brownain motion, this is modeling stochasticity under continuous time and continuous state, which has a very deep theoritical root connecting back to the Analysis aspects of mathamatics. This is also where the quote I put on the left comes from where equality equation refers to the Analysis aspects of mathamatics and inequality equation refers to the modeling aspects of mathamatics.    </p> <p>Inspired by Prof.Carfagnini: \"In math you either deal with equality equation or inequality equation, but sometimes they bridge\"</p> <p>       Essentially in Brownian Motion, wes ay that modeling a system of differentiual equation to know everything about how heat dissapate in a region is the eqivalent with deploying a stochaastic agent into the environment, let it play around and see how the probabilistic rollout would be to map out the boundary condition (analyst's approach v.s. a probabilist's approach). And, again, when looking it from        certain angle, the connection to Random Walk als shows up again.   </p>"},{"location":"articles/neuroscience/affective_neuroscience/","title":"Sensory, Processing, and Affective Neuroscience","text":"Kaiwen Bian 5 min read \u00b7 Jun 11, 2024 <p>       We precieve the world, the environment, and things around us through sensory informations, through sensory receptors capturing informations (i.e. wave length of light (visual color), checmical flying in airs (smells),...) and delivering it        to many sophesticated informational pathway that sends information across the brain for processing. From there on, we may talk about perception or deeper processing of these informations in PFC or in Amygdala (some may have direct projection,        there are simply too many complciated circuit in the brain for information procesing that spreads spatially and temporally, sometimes reaching the same target across different time periods). I like to think of it as 3 layers, from sensory to        processing to more higher level and affective processing. </p>"},{"location":"articles/neuroscience/affective_neuroscience/#encode-neural-representation","title":"Encode Neural Representation","text":"<p>       Here is where neuroscience gets a little bit philosophical and wehere I find neuroscience to be facinating, on the sensory processing information, if we account that the idea or representation of the information comes from perception (i.e. you would probably know what bird singing would        probably sounds like if you heard it before), but the nature of the information may not take form in such representation (i.e. birds' singing is techniqually waves and only when encoding such information in the basilar membrane from wave signal to neuronal signals is where these representations are created),        then when these natural form of the information is not been captured, does the representation of the information still exist? Or in another word: \"Does a sound exist when a tree falls in the forest, if no one is near enough to hear it?\".   </p> <p>From Wave to Neural Information (Borrowed from Prof.Mooshgian's Cogs107B Slides)</p> <p>Here is one of my favorite quote on ideass relating to system neuroscience:</p> <p>           \"Our perceptions differ qualitatively from the physical properties of stimuli because the nervous system only extracts certain information from a stimulus and then interprets this information in the context of its earlier experience. We experience            electromagnetic waves of different frequencies not as waves but as actual colors that we see: red, blue, or green. We experience objects vibrating at different frequencies as tones that we hear. We experience chemical compounds dissolved in air or water as            specific smells or tastes. Colors, tones, smells, and tastes are mental constructions created by the brain out of sensory experience. They do not exist, as such, outside of the brain. . . . Does a sound exist when a tree falls in the forest, if no one is near enough to hear it?            We now believe that the fall causes vibration in the air but not sound. Sound only occurs when pressure waves from the falling tree reach and are perceived by a living being.\"       </p> <p> From Principles of Neural Science, 3rd edition., eds. Kandel, Schwartz, &amp; Jessell, p. 330</p>"},{"location":"articles/neuroscience/affective_neuroscience/#representation-of-the-world","title":"Representation of the World","text":"<p>From neural encoding, our brain builds a representation, a mental representation of the environment that we are in, a \"salient map\" (particularly from vision) to help us navigate. Here is an classic example that would probably be covered in an system neuroscience class and also one that illustrates some        quite interesting points: Hemispatial Neglect. Hemispatial Neglect Theory discusses, under damages to the Parital Lobe and Cingulate Gyrus, the damaged brain neglects one side of the vision field completely (vision is teh most severe, but auditory, mechanical senses are also effected). This is not a sesnory problem as all the sensory pathways        are intact but rather a processing issue (an \"attention\" one). In another word, that damaged brain's mental representaion (space's representation in the mind's eye) is missing half of the information. This example really delivers how the brain may be creating a model of the world, a represenattion of it through some fundamental processing first, illustrating that        sensory information is one thing and the processed version is another. Here comes an interesting question: \"would developmental differences cause different neurogenisis and synapsis formation that makes one precieve the world completely different from another?\"   </p> <p>Vision Salient Map (Borrowed from Prof.Mooshgian's Cogs107B Slides)</p>"},{"location":"articles/neuroscience/affective_neuroscience/#affective-processing","title":"Affective Processing","text":"<p>       Now moving to more higher level and affective processing, one primary question that we should ask is: \"what is emotions?\". Personally, one of my favorite interpretation of emotion is from Prof.Chiba's cognitive neuroscientist's perspective, which states that:   </p> <p>Emotion reflects a kind of motion outward, an inferred complex sequence of reactions to a stimulus including cognitive evaluations, subjective changes, autonomic and neuronal arousal, impulses to action, and behavior designed to have an effect (functional) upon the stimulus that initiated the complex.       </p> <p>       That is a pretty techniqual definition, but the key idea is that emotion may be a very complicated circuitery phenomenon that spans across many regions of the brain. In neuroscience, Amygdala have always been tagged as the center of emotions as people really like to classifiy biological system with engineering thinking. However, is it really 100% true that it is built for emotion?        It is the center of many things that associates with emotions, but emotion processing is just a tag that we are giving to amygdala. It just happen to be a center of many information control, receive a lot bottom up sensory input, learns very quickly, monitors introceptivly while also extroreceptively, monitors reward association, and also projects greatly to many other cortical areas. It is key to        realize that Amygdala is not the only player as Analysis from higher level does matter, pripor knowledge-based processing does occur, your Prefrontal Cortex plays a huge role in emotion and it is more than the stimulus of the cue or the response of the Amygdala to these cue, the interpretation of them matters. </p> <p>Extended Amygdala (Borrowed from Prof.Chiba's Cogs107C Slides)</p> <p>       In a different word, we constructs the world we live in and we constructs such labels of emotion from a collection of big circuit and modulatory changes, from retrospective perspective, we made emotions.   </p>"},{"location":"articles/neuroscience/cognitive_brain/","title":"Cognitive Neuroscience's Perspective","text":"Kaiwen Bian 5 min read \u00b7 Jun 10, 2023 <p>       Inspired by Prof.Chiba from Cognitive Science Department at UCSD: In cognitive neuroscience, the brain paints a story about \"all the area, all the functions, all the state, all at once\".   </p>"},{"location":"articles/neuroscience/cognitive_brain/#never-a-control-rather-a-balance","title":"Never a Control, Rather a Balance","text":"<p>       For so long people have trying to pose an \"engineering system\" perspective on the brain and on biology, which simply may not be true.   </p> <p>OPFC (Borrowed from Prof.Chiba's Cogs107C Slides)</p> <p> Appropriate balance between excitation and inhibition is a basic principal of brain functions and stable cognitions. Looking at the anatomy, you would be shocked by the intricate circuit our brain has: \"processing everything in all places to support all functions and all at once in parrallel\".        The brain is so much more than just areas connecting together, it is a compelx, in parralel and recurrent circuit (no area is responsible for an function and no function is limited to an area). For instance, the Prefrontal Cortex (PFC) is deemed to be a key component that supports executive functions, learning, decision making, error testing,        mental sketch pad, reaprasal, representation learning, inhibition of inappropriate action. In another word, it supports so much of the important functions that makes us \"intelligent\". However, no functions would be carried out if the Basal Ganlia doesn't have the ability for reinforcing actions with dopaminergic cells from Ventral Tagmental Area (VTA), or if the Basal Forebrain        doesn't have the cholinergic cells to desynchronize the brain areas for low tonic/high phasic optimal sensory processing (changing the dynamics of the cortex). Note that this is just examining one small circuit on the cortical level, there are so much more to talk about when we expand our scope to more subcortical areas or brain stem areas (i.e. Hypothalamus, HPA Axis, Extended Amygdala, CRH Pathway's Effect on        Development &amp; Functioning, Sensory Information Procssing Pathways, Brain-Body Connections,...).   </p> <p>PFC/Amygdala (Arnsten, A. F. T. (2009))</p> <p>     To drive home the points about the complexity of the brain, here is some \"small\" circuit illustration:   </p> <p>\u2699 \ud83e\udde0 Some \u201cSmall\" Circuit in the Brain </p>"},{"location":"articles/neuroscience/cognitive_brain/#the-bodily-brain","title":"The Bodily Brain \ud83e\udde0","text":"<p>When thinking about the brain, we need to realize that it is not about a \"neck-up\" science. The brain and the body are always tightly connected is capable of powerfully influencing each other's development. The first effect may be well known, but the the reverse had been discovered recently to be true as well.        I want to use the following 4 examples to demonstrate such point:   </p> <ul> <li> Bone-Neurogenisis Connections <p>               The birth and death of neurons can be influenced by oxycalcin (a hormonal substance in the bones) that can inhibit apoptosis and promote neurogenesis.           </p> </li> <li> Gut-Stress Connections <p>               Gut microbiota significantly impact brain development where studies have shown that Germ-free rats exhibit increased HPA activity compared to normal rats, signaling that they are more prone to stress or CRF effects. Think about this, this sounds crazy that our stress level                can be influenced by microbes in our gut. Furthermore, this change in HPA activity leads to worse spatial memory performance since a properly regulated stress response, influenced by the gut microbiota, is important for optimal cognitive function, including memory. Thus, the imbalance or absence of gut                microbiota can negatively affect brain functions related to memory and learning.           </p> </li> <li> Stress-VMPC Connections <p>               During pregnancy, mother's body influences the fetus where increased stress in a pregnant mother leads to an decreased efficacy of the vagus nerve in the infant. If this infant does not experience sufficient opportunities to further develop this system, they may experience difficulties regulating using their parasympathetic nervous system, which                may lead to an overall increase in cortisol and CRF. This can potentially lead to delayed development of the Ventromedial Prefrontal Cortex (VMPFC), which is a part of the cortex that is the key for learning and emotional regulation as it poses projection directly on teh Amygdala for inhibition. With less developed VMPFC comes with less emotion stability and decision making.           </p> </li> <li> Stress-Allostasis Connections <p>               Knowing the effects decribed above, the same can be said for teenager anxiety, the more anxious they are, the more CRH there would be from the HPA axis trying to regulate Homeostasis. However, the brain is not just about homeostasis, it chanegs over time and have both mechanism for feeding backward (via Locus Correculus (LC) projection onto Hypothalamus) and feeding forward (via LC projecting onto                Bed Nucleus of Stria Terminails (BNST) of the Extended Amygdala through Norepinephrine (NE)). It is about Allostasis. Then there will be more CRH released from the feed forward system to \"keep you going\" and causes the VMPFC to be less developed, making emotion stability and learning even harder (not mentioning other down stream effect of PFC being less activated and Amygdala being more activated and support \"habitual mode\").                Thus, making them having more anxiety.           </p> </li> </ul>"},{"location":"articles/neuroscience/dev_neurobio/","title":"What We Think Deternmines What We Can Think","text":"Kaiwen Bian 10 min read \u00b7 Aug 20, 2024 Modified on Feb 21, 2025 <p>I think that one can not go to all the abstract realm of the mind without grounding in a biological system. In the same time, it becomes very hard to connect biology to our perceptual expereiences and what we feel everyday without the support of these abstract concepts**. They have the mutual relationship where thoughts develops the neurological foundations within our brain and later such system we developed effects how we think and feel** (i.e. the formation of habits is a great demostration of Hebian plasticity and such system formation).</p> <p>I have always been facinated about how our ideas and thoughts may be shaped by the biological circuit behind it, discussing abstract ideas in psycology, but grounded in biology. I believe that with understanding of psycology and neuroscience, we can make better decisions and reach closer to where we want to be at. To some degree, just like practicing muscles, we want to develop a neuronal system that is strong and suitable for what we want. First we will go over a perspective in developmental neurobiology, then dive more into how we can guide ourselves better with positive psycology, and at last attempt to connect them.</p> <p>*The ideas discussed here are drawn mainly from professor Kaiping Peng's lectures in hundun-academy and from professor Andrea Chiba's Cogs 164 lectures in UCSD.</p>"},{"location":"articles/neuroscience/dev_neurobio/#psycological-neurological-development","title":"Psycological &lt;-&gt; Neurological Development","text":"<p>Let's first discuss this from a developmental neurobiology perspective, specifically through looking at the period of being a teenager. Teenager period is the most susceptiable period to impulse-control disorder, substance-use disorder, anxiety disorde, mood disorder, or even Schizophrenia. As what professor Andrea Chiba described in her lecture:</p> <p>\"This is a period of extreme vulnerability, but with such vulnerability, there raises plasticity and hopes.\"</p> <p>This is the period when there are vast perturbing to the system, vast plasticity that can both take you to the top of the moutain as well as the bottom of a big hole. With the physical development that we get to our brain, we need sufficient psycological development to maintain the stability of the system while perturbing it for \"thining\" the cortical layers (less connections but stronger connections, prunning on the grey matters, more quicker and efficient connections). The Pre Frontal Cortex (PFC) continue to develop way surpass teenager period, so one is essentially constructing up their own PFC with the experiences they have and the perception they have on these experiences: \"psycology guides neurological development and neurological determines where psycological development can even go towards\".</p> <p>According to the Self-Deternmination theory, psycological growth can only be acheived when the followig are achieved (*note that each of the following section may be its own filed in psycology, this is just a brief description):</p> <ul> <li> <p>Autonomy: One need to feel in control of their own behaviors and goals. This sense of being able to take direct action that will result in real change plays a major parf in helping people feel self-determined and finding their sense of self.</p> </li> <li> <p>Competence: One need to gain mastery of tasks and learn different skills. When people feel that they have the skils needed for success, they are more likely to take actions that will help them achieve their goals. This sense of behaviors driven by intrinsic reward is what makes them feel in control.</p> </li> <li> <p>Connection or relatedness: One need to experience a sense of belonging and attachment to other people.</p> </li> </ul> <p>When any of these fails, it becomes hard for one to feel in control of the situation and may trigger un-favoriable adapattion towards teh wrong circuit.</p>"},{"location":"articles/neuroscience/dev_neurobio/#how-you-feel-towards-distal-stimulus","title":"How You Feel Towards Distal Stimulus","text":"<p>Evolution doesn't pick the strongest or the smartest species, but rather those that adapt to changes and cooperate most effectively. It is crucial for us to have the ability to manage our internal state and adapt to fit what  the environment need. This is the same as \"state flexability\" in the context of Allostasis (discussed in the other article). In positive psycology research, people have found these 3 characteristics to be key of adaptation.</p> <ul> <li>Resilience: The capacity to recover quickly, the ability to constantly rise again.</li> <li>Grit: The ability to endure and persist, the ability to be tenacious.</li> <li>PTGD: The ability to not only survive but thrive after experiencing hardships and challenges, to strive post-traumatic growth and development.</li> </ul> <p>I think that there are three (two emphasizing on the abstract level and one on the biological detailed level) core aspects (Self Efficacy, Growth Mindset, and Building Neuronal Circuits) that can give an good example of how what we think effects what we do and what we can think later. We don't want to not fail negativity, but rather how to think of it and react towards it through strong self-efficacy and growth mindset.</p>"},{"location":"articles/neuroscience/dev_neurobio/#faith-in-yourself-self-efficacy","title":"Faith in yourself: self-efficacy","text":"<p>Self-efficacy is the faith that you give to youyrself, which is crucial to the ability to withstand pressure. It is about believeing that you can and you have the ability to succeed. With high self-efficacy, stress and challenges are opportunities to prove yourself. Though self-efficacy is a higher-level abstraction idea, we can use other abstractions to trigger neruonal adaptations, namely:</p> <ul> <li> <p>Fake it until make it: When thoughts becomes action repeatidly, it becomes a habit, then habits becomes more actions, repeated wise.</p> </li> <li> <p>Vicarious success: When being accepted by people you want to be, it becomes easier to be these people.</p> </li> <li> <p>Unexpected social support: Evolution favors cooperators (the strong effect of weak ties). Opportunities are usually brought by people that you wouldn't expect.</p> </li> <li> <p>Simulated practice: One of the greatest power of the human brain is our predictive ability. Our brain can envision the future or jump around dimensions neglecting the effect of time. When the brain is under the default mode/circuit (a mode when you are not exactly thinking about anything and just being \"board\", i.e., during shower), it constantly makes predictions, processes emotions, conduct introspects, at a level beneath cognition. With such ability, we can escape the realm of \"reality\" to travel in much more abstract space.</p> <ul> <li>We can conduct \"visualization\", to mentally imagine success. The more you envision, the clearer the intention (mirror cell effect as well).</li> <li>Instead of focusing on details, see the bigger picture, the global properties.</li> </ul> </li> </ul>"},{"location":"articles/neuroscience/dev_neurobio/#avoiding-the-burden-of-excellence-growth-mindset","title":"Avoiding the burden of excellence: growth mindset","text":"<p>A growth mindset, as contast to a stationary mindset, is a belief believing that only attitude and effort determine what can be learned, where risks and challenges are enjoyed and self-feedback/suggestions determines outcomes. Th greatest hinders to a grwoth mindset is The burden of excellence where people believe in that they \"should be something\" instead of they \"could be something\". This can can lead to negative feelings (below the standard line), while \"earned through effort\" provides positive feelings. A growth mindset believes that there is no \u201cshould,\u201d no \u201cshould be good at learning\u201d or \u201cshould be smart,\u201d risks and challenges bring new outcomes.</p> <ul> <li> <p>Process-oriented: Self-assessment standards should be process-oriented, not outcome-oriented, R&amp;D is about challenges and risks (Success should be measured by changes, just as dopamine does). Instead of saying that an event is \"impossible to achieve\", frame it as \"not yet\", or \"maybe try more later\".</p> </li> <li> <p>Telling of the story makes the story: The stories we weave causes physical and mental reactions that we felt</p> <ul> <li>Activating event -&gt; Belief -&gt; Consequences: A cannot be changed, but B can, hence C changes. Use the prefrontal cortex to refute B's thoughts.</li> <li>Remanber that the failing to acknowledge negative aspects means missing out on positive experiences; the two are interconnected. One cannot achieve positivity without ubnderstadning negativity.</li> </ul> </li> </ul>"},{"location":"articles/neuroscience/dev_neurobio/#thoughts-biological-circuits","title":"Thoughts &lt;-&gt; Biological Circuits","text":"<p>In cognitive science, a well adopted perspective is the following:</p> <p>\"It is not the distal stimulus itself that matters, but rather the nueronal activation that give us perception\".</p> <p>It is the perspective and the interpretation of the distal stimulus that matters, it is the attitude that determines the actions and the reality. A positive cognition would deliver a physical and mental state that is full of vitality and makes it easier to see the goodness in humanity. However, it is impossible to not feel negative emotions. Hinders of feeling negativity causes unhealthy connections between Amygdala and PFC, causing potential development of being a psycopath. With that being said, we can use our understanding of neuroscience to navigate what we can do when negative feeling strikes, the key is always to adapt to changes that the environment poses and regulate our internal feeings/mental state, what we think effects the biolgical mechanism of the brain and such system effects how we think.</p> <ul> <li> <p>Physical cooling: The amygdala is located behind the nose and its activation represents congestion with temperature rises. When inhaling cool air, we physically cool down the amygdala and lowers its activations.</p> </li> <li> <p>Celebrate success: Prolong the time of happiness, after four minutes, memory conslolidates and the physiological pleasure neural network is formed (remember, later watching is equivalent to doing in the eyes of neuronal mechanisms).</p> </li> <li> <p>Neurologically \"proactive\": Use positive neurotransmitters to form positive neuro-emotional networks. Psychological activity isn't a single point, but an interconnected area. Quoting from professor Andrea Chiba, sometimes it is not the \"regions\" that went wrong but the lack of myolination that caused insufficient information sent.</p> <ul> <li>Dopamine: Released when helping others, discovering self-strengths, experiencing self-worth, doing what others cannot.</li> <li>Serotonin: Released when protecting self-esteem or basking in the sun.</li> <li>Endorphins: Happiness from deeper thinking or delayed gratifications.</li> <li>Oxytocin: Giving feel of love. released by warm hugs, receiving and giving compliments, or engaging in empathetic, understanding, appreciative, and supportive conversations.</li> </ul> </li> <li> <p>Habitual behaviors: Humans easily fall into habitual behavior, to act automatically without thinking. When challenges arise, habits become instincts, and instincts serve as an important protective mechanism.</p> </li> <li> <p>Build default immersive circuits: Find your own Flow and immerse in it to experience deep engagement. With such an system build and reinforced by Basal Ganglia, doing more of it is much easier.</p> </li> </ul>"},{"location":"articles/neuroscience/dev_neurobio/#evolution-of-a-desirable-circuits","title":"Evolution of a Desirable Circuits","text":"<p>Building a circuit against stress is a process caused by evolution, we are keep trying to construct better and better neuronal systems. We need to use all possible methods at hand to build a desirable neuronal cuirtuit during moment of stress to react to it at the moment and also build foundations for later reactions. With more of such response, habitual circuits would be formed and reaction to stress or challenges later would be as simple as instinct.</p> <p>Matching on the thoughts levels than reality will follow. Think about the person that you want to be, imagine what this person would do and do it, act like him/her and align your thinking on an abstract level first. Every single little small thing that you do like this person would prove a bit mroe that you are this person. As the bricks lays and as time goes on, you become this person.</p>"},{"location":"articles/neuroscience/out_of_helplessness/","title":"What You Think May Not Be What You Think","text":"Kaiwen Bian 5 min read \u00b7 Dec 17, 2024 <p>What we feel in the moment may not be true and what we think now may not be real.</p> <p>We have seen in previous articles describing how our brain is so good to adapt but affording a big cost and how we think determines what we can think. Our brain can be super quickly be trapped in an Learned Helplessness mood where we simply don't see a way to go for solving the issues we face. With more and more reinforcing with more learned helplessness experiences, this anchors our brain state at such bad region, making every cues an excuse for Amygdala to take over. We can't control the environment or the source of stress, but we can build up our system to be biologically strong (mentally strong if you want to call it from a psycological perspective) where we have good enough state flexability that we can operate under any conditions. State Flexibility refers to the ability where you can switch between different internal brain stages very quickly to match the needs of the environment. It is down by jumping out of first person's perspective and change an view point. When it is needed, such state flexability would use all PFC power to jump out of any personal \"precived\" neagtive emotions to see from a third person's perspective on your thoughts, actions, and feelings, hence, helping you to restore your senses and see what is the true conditions and what is the conditions that your stressor is letting you see.</p> <p>PFC/Amygdala (Arnsten, A. F. T. (2009))</p> <p>When you are outside of the environment and context that you are in, you will realize that all the problem you think matters a lot earlier actually is no longer a problem. Making decision in your emotion is never wise, but sometiems it is really hard to escape such habitual way of thinkig unless if you explictly practice to \"jump out of the scene\". This helps to prevent allostatic load making you seeing misleading informations, feeling inacurate emotions, or consolidating the wrong thinking process.</p> <p>We tend to think to the tails of the distribution when under high allostatic load, pushing thoughts to discretized bins thta is either black or white, but this isn't usually the truth, the truth tends to be normally distributed. When you try to jump out of your havitual thinking process more, more neuroplastic effect\uff0cneuropeptide effect, and HPA effects would take place and make your mental power stronger on a physical and biological state, \"helping you see the full spectrum again\".</p> <p>What you think now may not be what you actually think and what you feel now may not be what you actually feel. Give your body the time it need to drop all the allostatic load, it will restore to a balance state, don't make this waiting process a consolidation of wrong thinkings. When you can, please  stop and wait a bit, step out of the picture, or just simply don't care about it for tonight. No matter ho  you frame it, we will fake it until make it and  we will not be blind folded by the emotions and ideas that all aour stressors give us.</p>"},{"location":"articles/neuroscience/parralel_reinforcing/","title":"Searching & Parralel Processing","text":"Kaiwen Bian 5 min read \u00b7 Jun 14, 2024 <p>     Biological system has always been used as inspirations for artificial networks that boosted learning. Upon all of them, I find Basal Ganglia to be the most facinating as it seems to resemble some \"nature\" of      information processing and finding the true structure that stems \"intelligence\" in nature.   </p>"},{"location":"articles/neuroscience/parralel_reinforcing/#basal-ganglia-structure","title":"Basal Ganglia Structure","text":"<p>     Basal Ganglia can be deemed as a \u201creinforcing unit\" that is highly connected to many of the other functional circuit in the brain and it is strongly modulated by the expectation of reward.     As a theme in neuroscience, \"generally speaking, we learn changes in the world\", the errors \\(\\delta\\) from expectation is what governs the neuromodulation system (dopamine ) in the Ventral Tagmental Area (VTA), which      have dense projections on the Striatum that encodes sensory motor information (via direct pathway D1 and indirect pathway D2).   </p> <p>     When reward is expected, the D1 direct pathway would be mroe active with D2 indirect pathway less active (reward association is captured: one facilitate reward prediction and one facilitate non-reward prediction). To be more      specific, the direct pathway would have inhibition from the Striatum to the GPi that inhibits the Thalamus. Thus, such inhibition of the inhibition would cause the Thalamus to be more active, creating positive reinforcing. On the      other hand, the indirect pathwat excite the GPe that inhibits the STN that excites the Thalamus. Thus, such excitation of the inhibition on the inhibition would cause the Thalamus to be less active, creating negative reinforcing.    </p> <p>Basal Ganglia (Kim, Hyoung F., and Okihide Hikosaka (2015))</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#search-manipulate-valuable-objects","title":"Search &amp; Manipulate Valuable Objects","text":"<p>The goal is always to maximize the reward, which can be separate into two problems, one being finding valueable objects in the environments with sensory input (exploration) and the other being manipulating such valuabel object to retain reward (exploitation). To reach      the reward, it is always a brain-wise effort because all attention, motivation, context, uncertainty measurement, risk assessment, ... need to be performed, in here we just particularly examine Basal Ganglia.   </p> <p>     Dopamine signals is what biases sensorimotor inputs, but how should it balance between the exploration adn exploitation? In algorithm research, that is always a question that puzzles researchers, but seems like the brain have its own answer. The blue (CDh-rvmSNr) circuit is inherently recieving projection that makes      quick learning + short memory retention while the red (CDt-cdlSNr) circuit is slow learning + long memory retention. In another words, the blue circuit is very sensitive to immediate reward (boost learning) while the red circuit is more proned to using stable-value-objects.   </p> <p>     Not talking about algorithm but just as a discussion about life, this is a neuroscience argument of why you should persue the things you are interested in the moment you find them interested   </p> <p>Exploration/Exploitation (Kim, Hyoung F., and Okihide Hikosaka (2015))</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#parallel-processing","title":"Parallel Processing","text":"<p>     From a more macro perspective, Basal Ganglia recieves multiple input from multiple areas at once. Both limbic/higher control (association cortical areas) and sensory information (somatosensory cortex) reaches the Striatum in the same time. The      true amazing aspect of Basal Ganglia is that no convergence is really needed, the topographic input is preserved in the same format when outputted. It can work independently on several streams of information from different orders of levels.   </p> <p>Projection to Basal Ganglia (Kim, Hyoung F., and Okihide Hikosaka (2015))</p> <p>     In a familier environment with familier objects (stable-value-objects), the objects should have well-predicetd valeus as they have been seen many times (mental representation is well defined): red (CDt-cdlSNr) circuit is active with blue (CDh-rvmSNr) circuit providing less bias. This setup can be seen      as an exploitation stage if putting under the framework of algorithm, it is maximizing the reward from previously known values. On teh other hand, if the envioronment is unfamilier (flexiable-objects), we can \"pick some object adn check their values\", refering to      the exploration stage. Overtime as more are explored, these known \"well-known value objects\" becomes stable-value-objects (I kind of interpret it as frozen neurons).   </p> <p>     That was describing one object, but in the real environment it is always multiple objects happening at thes ame time, some flexiable-object and some stable-value-objects, parralel processing comes in and the circuit is all processing in the same time  ith very different input, recieving projection and      projecting to many other functional areas + circuits. Techiqually speaking, it is many autonomic mechnisms and one voluntary mechanism operating in parralel to aim to achieve higher level common goals that requires multiple motor and mental processes, between mental processing and taking actions      (maybe this solves search and optimization problem?).   </p> <p>Learning is very expensive, long term learning is needed to learn a stable representation of coordination of such mental and motor functions (i.e. learning a sequential movement, from thinking of the movement to doing the movement). Initially, the motor action may be concious with many    prediction errors, but later performance should increase and the motor action to mental representation association is learned and motor actions can be facilitated by learned cognitive processes. Reward's association of flexiable-object is transfered to a stable-value-object.   </p> <p>Parallel Processing (Kim, Hyoung F., and Okihide Hikosaka (2015))</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#continual-learning-algorithms","title":"Continual Learning &amp; Algorithms?","text":""},{"location":"articles/neuroscience/parralel_reinforcing/#balancing-search","title":"Balancing Search","text":"<p>     To me the previously descrribed system that balances the search problem seems to be very much like AlphaGo, but I am not sure if either the discovery of them draws inspiration from each other, which is the exact reason of why this is so facinating: maybe some truth, some correct representation/structure of stemming intelligence is being captured.   </p>"},{"location":"articles/neuroscience/parralel_reinforcing/#different-perspective-on-learning","title":"Different Perspective on Learning","text":"<p>     Drawing inspirations on how we learn, When the system is initially learning, the representation of the weight should not be \"locked\" fully, but when learning continually, some key representation should be learned and then these stable-value objects should be locked for later needed task as a cognitive processes. The question of CL should be deem as a sequential learning that is not      epsisode over an episode tasks trying to preserve the previously learned information, but as a whole thing of continuously learning trying to find/search for the actual correct/useful representation over a sequential unit of time and training (learning some representation, then some more).   </p> <p>       From Prof.Gao: \"We may know the principal, but Computers may travel to places that is far beyond us.\"     </p> <p>With in mind, we should let the system itself to search for the right representation to lock, we should let them serach for needles in the haystacks.</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#parralel-processing","title":"Parralel Processing","text":"<p>There is one stream of higher level guidance (voluntary voice): learned stable-value-objects or previously learned weights. Moreover, such stream of guidance processes simultaneously in parralel with the otehr sensory input: flexiable-objects or new data induced weightes.</p>"},{"location":"articles/neuroscience/systematic_deviation/","title":"Neural Adaptation With Cost: Systematic Balance Distortion","text":"Kaiwen Bian 5 min read \u00b7 Jun 17, 2024 <p>       I find addiction's neural circuit as one that is really interesting to look at because it demonstrate a systematic method of adaptation to deviation (sometimes this is quite problematic), an well rounded circular circuit that feeds into each        other to achieve the adaptation that has an cost. More importantly, I think it may serve as a model that can help us to understand how adaptation works on a neuronal level, which may effect how we think, feel, and do on the cognitive level, how        higher level cognition may stem from basic biological units that continuously reevaluates the needs in the environment and readjust the body's parameter to fit such needs. After reading this, you may have a better intuition of how good cognitive functions really rely on        the balance between different circuits that communicates with each others.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#allostatic-processes-nueral-adapttaion-with-cost","title":"Allostatic Processes: Nueral Adapttaion With Cost","text":"<p>       I want to discuss the idea of addiction through the perspective of allostasis, which I will intriduce first. Homeostasis, the perspective of deeming how biological system works as a restoration of balance, of an setpoint, may be one that is too ideal to        impose to our body sometimes. It assumes a perfect senerio, an egineering system that is imposed onto biology. Allostasis, on the other hand, involves a feed-forward mechanism and describes \"stability through chanegs\".        In the Allostasis hypothesis, the feed-forward allows the matching and adjustment to the environment through continuous reevaluation of needs and continuous readjustment of all parameters toward the new set points.        However, when the body is under states that \"we don't exactly want to adapt to\", the ability to quickly mobilize resources and use feed-forward mechanisms to adapt to the environment can lead to some issues, the cost of neural adaptation.   </p> <p>An allostatic state can be defined as a state of chronic deviation of the regulatory system from its normal (homeostatic) operating level</p> <p>Koob and Le Moal (2001)</p> <p>       Sometimes the same circuit that help us adapt to an new environent quickly and adjust ourselves may be the same one that leads to allostatic load, allostatic state, or ultimately pathology under extreme conditions. Allostatic mechanims have long being hypothesized to be        regulating and maintaining motivational systems that have relevance to the pathology of additction. Using an simile to take home the points:   </p> <p>           An acute elevation of blood pressure is \u201cappropriate\u201d in an allostasis model to meet the environmental demand of acute arousal, but chronic blood pressure elevations under conditions of chronic stress may address the chronic environmental            demand but is certainly not healthy.       </p> <p>Sterling and Eyer (1988)</p> <p>The circuit of addiction spans across multiple area and doing multiple things all at once, aligning with the whole theme of \"all the area, all the functions, all the state, all at once\". Using the conceptual framework layed out in Koob and Schulkin's paper,      addiction is mainly separtated to 3 stages: Intoxination, Withdraw Affect, and Preoccupation and each stage have circuits that is specifically for them but also feeding in into other stages. For example:     <ul> <li> <p>Basal Ganglia (blue structure) is relevant to the Intoxination stage with the activation of various neurotransmitters such as dopamine and opiod peptide, engaging asscoiative mechanism with nucleus accumbens shell, and making an stimulus to response            habits engaging the dorsal striatum.</p> </li> <li> <p>In the Withdraw Affect, the extended amygdala (red structures) is highly activated and deviates the balance far from the original set point.</p> </li> <li> <p>Then during the Preoccupation period (green structure) would be relevant to hippocampus and executive function areas like prefrontal cortex + active seacrhing processing areas like orbital and anterior cingulate cortices and temporal lobe.</p> </li> </ul> </p> <p>Conceptual framework of how addiction happens (Koob and Volkow (2016)))</p>"},{"location":"articles/neuroscience/systematic_deviation/#too-much-reward-may-be-chronic-stress","title":"Too Much Reward May Be Chronic Stress","text":"<p>Motivation is a construct that can be defined as \u201ca state that varies with arousal and guides behavior in relationship to changes in the environment. The environment can be external (incentives) or internal (central motive states or drives), and such motivation or motivational states are not constants and vary over time\u201d.</p> <p>Koob et al. (2010)</p> <p>     Motivation talks about a state with arousal that guides behaviors in the changing environmengt, which have been shown to be correlated with how stress we are. Some levels of stress can be rewarding and benificial for learning as we know that Cortical Releasing Factor (CRF) (think it as stress hormone) may help us to spring to actions, lower the sensory threshold, and      have involvement in attentional responses to both external and internal events that increases incentive salience. However, excessive stress would take our body into the vicious cycle and may leads to pathology. The interesting things is that the same circuit that responses to stress have been indicated to have similar effect towards reward, lending the idea that too uch reward may be problematic.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#hpa-axis-glucocorticoid","title":"HPA Axis &amp; Glucocorticoid","text":"<p>     When we are facing stress, the HPA axis woud be activated and the CRF expressing neurons in the paraventricular nucleus of the hypothalamus would release CRF into the portal system, activates the release of ACTH from the pituitary, which in turn activates the release of glucocorticoids from the adrenal cortex.      This is one of the brain's attempt to restore balances as glucocorticoid activation may decrease CRF gene expression, which leads to less CRF. However, this is already a neuroadaptations that initiate the allostatic process, one that deviates from the original point (notice that deviation is perfectly normal as      we need to adapt to changes, but a systematic deviation that causes too great of a magnitude shift may be causing some issues).   </p> <p>     Back to addiction, drug (reward) would initiates similar effect that stimulate the HPA Axis with ACTH release and establishing the whole down stream effect that can be caused with mild stress described above. In some sense, it seems like stress and reward does both trigger similar activations that causes the reduction in CRH, which      makes us feel good. However, the difference being the amount of neurotransmitter that is released. In the drug's case, it has ahuge magnitute (in some sense it is signaling the brain that the person is under chronic stress whne they are actually under euphoria caused by drugs), which links to issues that will be expanded more below.   </p> <p>CRF Circuit (Koob and Schulkin (2019))</p>"},{"location":"articles/neuroscience/systematic_deviation/#withdraw-affect-residual-hysteresis","title":"Withdraw Affect: Residual Hysteresis","text":"<p>     The massive release of reward neurotransmitters (causing the downstream effect mentioned previously) comes with drug-opposite responses as the drug wears off. Because of the magnitute of neurotransmitter release, such opponent processes have been hypothesized to occur even with a single injection of a drug.      If the intake is persistant, the brain attempts to keep adapting to the new state and the bar with feeling rewarding would be elevated and may never returns to baseline levels hypothesized by residual hysteresis. progressively, it creats greater elevation of the baseline reward thresholds and leading to more compulsize drug seeking behavior (addiction).      Remanber that CRH may boost our learning because it have involvement in attentional responses to both external and internal events that increases incentive salience? Well, the same phenomenon would occur for drugs, the attentional salience map for this object (drug) wouuld be elevated with the \"value\" of such object over-estimated, hence, amking it even harder to withdraw from it.   </p> <p>Systematic decitation from setpoint (Koob and Schulkin (2019))</p> <p>     The Withdraw stage's effect may feed back into the Intoxination stage since such massive release of neurotransmitter from drug's effect are associated with adaptations such as decreases in dopaminergic transmission and dopaminergic neuron firing in the ventral striatum, nucleus accumbens, and ventral tagemental area, causing the lack of      dopamine signal when needed. As well known, dopamine is an key neural transmitter in encoding reward signal, so if there exist a adaptation of systematic deviation, issues would occur as this adaptation is somewhat also raising the bar of feeling reward becuase the body thinks that not as much dopamine is really needed because of the new given condition.      Similraly, molecular adaptation with G-protein functioning and protein kinase A activity in the nucleus accumbens also occurs to consolidate these changes that occurs between-systems.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#preoccupation-pfc-being-offline","title":"Preoccupation: PFC Being Offline","text":"<p>     The Preprontal cortex is usually deemed as a key for executive function and coognitive decision making (i.e. smart choices of not doing drugs). The activation of the HPA axis and the CRF system would negatively impacts the prefrontal cortex to impair such top-down connectivity and boost allostatic changes in the extended amygdala.      Now there is a decrease in the inhibitory function of the prefrontal cortex (ventromedial prefrontal cortex, orbitofrontal cortex, and cingulate cortex) and a boosting in the extended amygdala, the habitual mode mentioned in the other article would start to occur, leading to further preoccupation of drug stimuli.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#distortion-of-balance","title":"Distortion of Balance","text":"<p>     Now you may begin to have some intuition of how these 3 stages work independently on its own but also feeding in to each other to create such trap of addiction and also how systematic deviatoon from the balance point may introduce really serious issues. On the extreme side of these distortion of balance would be psychopaths where it have been hypothesized that      one reaosn that may be behind the lack of emotion of psychopath is because of childhood traumatic events that caused them to seggragate their PFC and extended amygdala for too long, leading to the adaptation of distotred balance and the lack of feeling emotions.   </p>"},{"location":"literature/literatures/","title":"Literatures","text":"<p>Stuff I find to be an interesting read... discussing novel ideas in mathematics, neuroscience, machine learning, or their intersections. My notes on literatures</p>"},{"location":"literature/literatures/#research-paradigm","title":"Research Paradigm","text":"<ul> <li> <p>The Standard Model of Machine Learning Standardlizing all machine learning approahces, forming a standard model for ML.</p> </li> <li> <p>The Neuroconnectionist Research Programme Lakatosian research program setting computational understanding of the brain.</p> </li> <li> <p>Building Machines that Learn and Think like People Foundation review discussing potential research directions for human-like AI.</p> </li> </ul>"},{"location":"literature/literatures/#reinforcement-learning","title":"Reinforcement Learning","text":""},{"location":"literature/literatures/#theoretical-reinforcement-learning","title":"Theoretical Reinforcement Learning","text":"<ul> <li> <p>Maximum a Posteriori Policy Optimisation Probabilistic flavor-infused cutting-edge actor-critic.</p> </li> <li> <p>Proximal Policy Optimization Algorithms Actor-critic algorithm on steroids.</p> </li> <li> <p>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures Distributed RL framework for off-policy learning.</p> </li> <li> <p>Addiction as a Computational Process Gone Awry Using RL methods to model the addiction process.</p> </li> </ul>"},{"location":"literature/literatures/#goal-directed-deep-reinforcement-learning","title":"Goal-Directed Deep Reinforcement Learning","text":"<ul> <li> <p>Divergent Representations of Ethological Visual Inputs Emerge from Supervised, Unsupervised, and Reinforcement Learning Proposes using ANNs to model the brain.</p> </li> <li> <p>Deep Neuroethology of a Virtual Rodent Aligning deep RL with biological counterparts.</p> </li> <li> <p>Whole-body Simulation of Realistic Fruit Fly Locomotion with Deep Reinforcement Learning Distributed trained MPO policy for goal-directed RL.</p> </li> </ul>"},{"location":"literature/literatures/#inverse-kinematics-imitation-learning","title":"Inverse Kinematics Imitation Learning","text":"<ul> <li> <p>A Virtual Rodent Predicts the Structure of Neural Activity Across Behaviors Imitation learning mimicking rodent behaviors, showing similar neuronal activations.</p> </li> <li> <p>CoMic: Complementary Task Learning &amp; Mimicry for Reusable Skills Encoder/decoder architecture transferring motor skills across tasks.</p> </li> </ul>"},{"location":"literature/literatures/#representation-building","title":"Representation Building","text":"<ul> <li>Inductive Biases of Neural Network Modularity in Spatial Navigation Building beliefs in artificial agents through MOPDP conditions.</li> </ul>"},{"location":"literature/literatures/#world-model-agent-model","title":"World Model &amp; Agent Model","text":"<ul> <li> <p>Language Models Meet World Models: Embodied Experiences Enhance Language Models L-policy: building embodied world model into language model agent through embodied experiences and finetunning.</p> </li> <li> <p>Building Cooperative Embodied Agents Modularly with Large Language Models Building collaborative agent in an partially observable embodied environment.</p> </li> </ul>"},{"location":"literature/notes/","title":"Notes","text":""},{"location":"literature/notes/#research-literatures-notes","title":"Research Literatures' Notes","text":"<p>To share ideas and hopefully inspire some new development... Constantly updating with new ideas.</p> <ul> <li> <p>Standard Model of Machine Learning Written by Dec 3, 2024</p> </li> <li> <p>Building Cooperative Modular Embodied Agents With LLM Written by Oct 18, 2024</p> </li> <li> <p>Language Models Meet World Models Written by Oct 16, 2024</p> </li> <li> <p>The Neuroconnectionist Research Programme Written by Jun 30, 2024</p> </li> <li> <p>Addiction as a Computational Process Gone Awry Written by Jul 18, 2024</p> </li> <li> <p>Building Machines that Learn and Think like People Written by Jul 16, 2024</p> </li> </ul>"},{"location":"literature/notes/#formulation-proofs-notes","title":"Formulation &amp; Proof's Notes","text":"<p>This is the section is for putting the hand written script of what I might put it into an article later.</p> <ul> <li> <p>Bellman Update: Convergence To Single Reality Written by Apr 22 2024: all \"realities\" converge to a single reality in MDP</p> </li> <li> <p>Q's Search: Incremental Optimal Leads To Global Optimality Written by May 22 2024: How Q constructs the world</p> </li> <li> <p>Expectation Maximization: Creating Something From Nothing written by Dec 13 2024: creating \"something\" arbitrary, then gradually make it real</p> </li> </ul>"},{"location":"literature/notes/#computation-demos","title":"Computation Demos:","text":"<p>Here are some computational demos that I created/adapted from different courses and think that would be useful for providing intuitions into different topics.</p> <ul> <li>Statistical Inference Pipeline written by Mar 16 2025: some intuition towards SIP and what traditional inference care about</li> </ul>"},{"location":"literature/notes/#neuroscience-related-ideas","title":"Neuroscience Related Ideas","text":"<p>Here are some interesting neuroscience ideas that I have notes on but didn't put into an article yet</p> <ul> <li> <p>Little Blue Spot: Locus Coeruleus Written by Jan 25 2025: Solving flexability in complexity &amp; exploration vs exploitation</p> </li> <li> <p>LC Activates All: Security Motivational System Written by Feb 14 2025: How the LC-NE system serve as a \"gadget\" to activate the SMS system</p> </li> </ul>"}]}