{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Wiki","text":""},{"location":"#welcome-to-my-wiki-page","title":"Welcome to My Wiki Page","text":"<p>Some perspectives I have on Neuroscience, Mathematics, Machine Learning, and other areas of interest.</p> <p>Back to my website</p>"},{"location":"#neuroscience-related","title":"Neuroscience Related","text":"<p>Essentially, this is how I found neuroscience and biology to be quite amazing\u2014how they may inspire the design of intelligent algorithms/systems and how \"close\" they may be to the true \"structure\" in nature that makes intelligence. I have written some articles referenced below:</p> <ul> <li> <p>Cognitive Neuroscience Perspectives Building a perspective on the brain.</p> </li> <li> <p>Sensory, Processing, Affective Neuroscience From sensory to processing to perception.</p> </li> <li> <p>Reinforcing &amp; Parallel Processing Reinforcing &amp; searching: some magnificent connections between the brain and algorithms.</p> </li> <li> <p>Neural Adaptation With Cost: Systematic Balance Distortion Addiction is a systematic adaptation to deviation\u2014a well-rounded circular circuit that feeds into itself. Once balance is distorted, problems may occur.</p> </li> <li> <p>What We Think Determines What We Can Think Once the circuit forms, the rest becomes much easier.</p> </li> </ul>"},{"location":"#mathematics-related","title":"Mathematics Related","text":"<p>I find theoretical math to be pretty fun. I think that good practical techniques that work well are derived from a theoretical root.</p> <ul> <li> <p>Unfolding Stochasticity Sequentially Modeling interactions between stochasticity across time sequentially through the key representational example of Random Walk.</p> </li> <li> <p>\\(N(\\mu, \\sigma)\\) Lend It Some Confidence There are deep connections between statistics and probability, even on very basic statistics levels.</p> </li> <li> <p>Twitch on Theory In Convex Optimization Deriving everything we want in optimization from Taylor Theory and with small modification on some assumptions or the way we design things, we get completely different families of algorithms.</p> </li> <li> <p>All About Constraint Solving All problems that we want to solve can be framed as a constraint solving process. Both in math and in life.</p> </li> </ul>"},{"location":"articles/mathamatics/confidence/","title":"Lend It Some Confidence","text":"Kaiwen Bian 5 min read \u00b7 Jun 14, 2024 <p>       Statistics is a very practical domain, but often tools in statistics have very deep mathamatical roots in probability theory. I found this to be quite facinating because I think that only when understand the theoritical aspects of these tools will one be getting        an intuitive understanding of these tools and use/adapt them under appropriate circumstances. Things works for a mathamatical reason, they work because the math have happened to discover and support some things that happen to work.   </p> <p>       I want to use an concept that is very oftenly used through out many branches of statistics and probability to illustarte such point: Confidence Interval. It is so simple that probably a high school statistic class would discuss it but it is also so complex to        the point that one might not fully understand it until learning meausre theory in graduate school (for the record, I don't think I underatand it fully yet, but I can already see some of the intrinsic connections that makes it so amazing).   </p>"},{"location":"articles/mathamatics/confidence/#approach-to-some-distribution","title":"Approach To Some Distribution","text":"<p>       The way I think about confidence interval is to reason why it is needed. I think the idea of needing it comes from statsistics with the question: can we do better than just giving a point estimate of what we believe about the true distribution? Can we give an interval estimate?       Point estimate itself is a huge domain in statistics and probability (i.e. MOM, MLE,...), which we would not go into, but feel free to look into it a bit more as it is also very much rooted in probabilistic theory (i.e. MLE is maximizing the likelihood of observing all observatiosn in the same time, which are treated as independent and        identitically distributed random variable). Essentially an estimator is an random variable that tries to estimate the correct parameter for the given distribution to fit all the data (i.e. \\(\\mu\\) for gaussian distribution or \\(\\lambda\\) for poisson distribution). Okay, so we have an point estimator, but we want a range for it, maybe extracting an interval from the distribution        of this estimator since an estimator is techniqually an random variable? Let's look at the simple example when the estimator is \\(\\bar X\\) or the mean of the sample. We know that        $$       \\bar X = \\frac{x_1+...+x_n}{n}       $$        and that         $$       x_i \\sim N(\\mu, \\sigma) \\text{ with } \\bar X \\sim N(\\mu, \\frac{\\sigma}{\\sqrt n})       $$        Then we can make an new random variable that approaches a standard normal distribution \\(N(0,1)\\) by simply sifting values around:        $$       \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)       $$        This is doesn't show exactly why confidence interval is in it's form shown below, but this at least lend some ideas about how we can \"extend\" the estimator by giving it a interval. You may now have some intuition that these two seems to be weird values attached to the estimator does have a very probabilistic meaning since they look pretty similar.        $$       \\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)       $$        Techniqually, such confidence interval would be descrbed with an confidence level of \\(100(1-\\alpha)%\\), assuming \\(\\alpha=0.05\\), this is a 95% confidence interval, which is saying that we are 95% confident that the true parameter \\(\\theta\\) is in the interval or that when we plug in the data, 95% of the times the real parameter would be in such random range        (random because it is a random varaible + random margin that depends on data)        $$       P(\\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right) \\text{ contains } \\mu) = 1 - \\alpha       $$        Notice that this is not talking about the probability that teh true parameter is in the random interval, it is in it or not, nothing is random here. However, we will see later that in a connection such value may have some probabilistic interpretation.   </p> <p>     In fact, there is a deep conection between decision making hypothesis testing (HT) and confidence interval known as the confidence interval duality where not rejecting the null hypothesis \\(H_0\\)is the equivalence with such random interval around \\(\\hat \\theta\\) contains the real parameter \\(\\theta\\). Intuitively speaking, it is like talking about one with thinking the center as        \\(\\hat \\theta\\) and building confidence around that and the other is thinking center around \\(\\theta\\).        $$       \\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right) \\rightarrow \\left| \\frac{\\bar{X} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} \\right| &lt; z_{\\alpha/2}       $$   </p> <p> Some small detials on HT: The hypothesis testing representation here is illustrating fail to reject the null hypothesis condition, or the region that is not the critical region (the mid region of the normal curve where area is \\(1-\\alpha\\)). Notice that here we are talking about a probabilistic statement where \\(\\alpha\\) is the probability of seeing the observed test statistic (r.v.) as extreme or being more extreme than the        alternative hypothesis \\(H_1\\) direction. Usually \\(\\alpha\\) is manually set (i.e. 0.05) and treat that region to be trhe critical region for rejecting the null hypothesis. Inherently, the rest of the region would be saying that it is not that rare to see the observed test statistics under the null hypothesis, thus not rejecting it.     </p> <p>     These theoritical connection between statistical tools and probability theory goes much longer, extending to some other pretty well known distributions (you may have heard about \\(T\\) or \\(X_n^2\\) distribution, which are used when \\(\\sigma^2\\) is not present) adn we can use standard deviation as a replacement in the new random variable, which approach to a different distribution.      If you are interested, here are some notes that goes deeper into these connections.   </p> <p>Some notes that I concluded on basic statistics and probability connections across three areas: point estimate, interval estimate, adn decision making (HT)          (only personal understandings, may not be 100% correct).</p> <p>\\(N(\\mu, \\sigma)\\) Some Notes on Basic Statistic Connections</p> <p>     The connections for confidence interval itself actually serves much more than just in statistics, also to other realms (i.e. in algorithm and reinforcment: UCB).   </p>"},{"location":"articles/mathamatics/constraint/","title":"All Constraint Solving","text":"Kaiwen Bian 10 min read \u00b7 Dec 16, 2024 <p>Still working on this section, ideally to cover the following content.</p> <ul> <li>Theoritical perspective:<ul> <li>Standard model</li> <li>EM is constraint</li> </ul> </li> <li>Practical perspective:<ul> <li>Database is about constraint</li> <li>NetFlix engineering lessons</li> </ul> </li> </ul>"},{"location":"articles/mathamatics/optimization/","title":"Twitch on Convex Optimization Theory","text":"Kaiwen Bian 20 min read \u00b7 Dec 11, 2024 <p>The beauty with convex optimization is that we can use a theoritical perspective to bound the convergence of the algorithm: \"It will stop and it will be the optimal\". More importantly, when we make \"a little twitch\" on the formulation of the problem, we see a completely different algorithm with a different perspective. There is actually a consistent stream of thought in convex optiization that derives everything</p>"},{"location":"articles/mathamatics/optimization/#gradient-descent-in-different-lenses","title":"Gradient Descent In Different Lenses","text":"<p>Gradient descent is an extremely popular algorithm that is highly used in modern machine learning (particularly variants of it like the ADAM optimizor). This is not only because it has good practical results but also because it comes with strong theoritical guarantees. In this section, we will use differnt perspective to look at GD.</p>"},{"location":"articles/mathamatics/optimization/#taylor-expansion","title":"Taylor Expansion","text":"<p>Taylor's theory is an extremely core concept in convex optimization as many of convex optimization is about \"how to satisfy taylor theory such that we have certain part less than some other part\".</p> \\[ f(\\vec y) = f(\\vec x) + \\nabla f(\\vec z)^T (\\vec y - \\vec x) \\] <p>This is the recursion definition of Taylor's theoy, with the goal of estimating a point \\(\\vec y\\) from using a different point \\(\\vec x\\), plus its curvature, and recursively unfolding th whole  expression again using \\(\\vec z \\in (\\vec x, \\vec y)\\) (getting to curvature of curvature and so on). The magic of gradient decent comes when we assume this \\(\\vec y\\) is our next point \\(\\vec x + \\mu \\vec v\\) where we are moving along the direction of \\(\\vec v\\).</p> \\[ f(\\vec x + \\mu \\vec v) = f(\\vec x) + \\nabla f(\\vec z)^T (\\mu \\vec v) \\] <p>Ideally, if we  are doing gradient descent, we want to have the next point taking a lower function value than the previous one, meaning that we want:</p> \\[ f(\\vec x + \\mu \\vec v) \\leq f(\\vec x) \\] <p>or teh equivalence \\(f(\\vec x + \\mu \\vec v) - f(\\vec x) \\leq 0\\). For a small enough \\(\\tilde{\\mu}\\) and a continuous function \\(f\\)s, the descent direction of \\(\\vec v\\) when \\(\\vec v \\cdot \\nabla f(\\vec x) \\leq 0\\) is also the descent direction for \\(f(\\vec x + \\mu \\vec v)\\). To create \\(\\vec v \\cdot \\nabla f(\\vec x) \\leq 0\\), we need \\(\\vec  v = \\nabla f(\\vec x)\\), which is why the gd equation is in the form:</p> \\[ x^{(t+1)} = x^{(t)} - \\mu^{(t)} \\nabla f(x^{(t)}) \\] <p>To recap, we derived our descent direction \\(\\vec v\\) based on what we wnat to satisfy taylor theory such that \\(f(\\vec x + \\mu \\vec v) - f(\\vec x) \\leq 0\\). We have shown that there is a intuitive, but theoritical reason behind each step of why we are doing gradient descent.</p>"},{"location":"articles/mathamatics/optimization/#local-convexity-rightarrow-calculus-optimization","title":"Local Convexity \\(\\rightarrow\\) Calculus Optimization","text":"<p>From a different perspective, we can look at GD as doing a local descent. To be more specific, we assume that at each step, the Armijo condition is hold, then we have a local convex shape. In full Taylor expansion to the second degree, it cna be expressed as:</p> \\[ f(z) = f(x^{(t)}) + \\nabla f(x^{(t)})^T (z - x^{(t)}) + \\frac{1}{2} (z - x^{(t)}) \\nabla^2 f(z)^T (z - x^{(t)}) \\] <p>This second \\(\\frac{1}{2} (z - x^{(t)}) \\nabla^2 f(z)^T (z - x^{(t)})\\) is super annoying as we have a recursive hessian term \\(\\nabla^2 f(z)\\) in it. However, what if we don't care about the curvature of the curvature? We make a simplified assumption that:</p> \\[ \\nabla^2 f(z) \\leftarrow \\frac{1}{\\mu}I \\] <p>This makes our expression much more simple, giving just</p> \\[ g(z) = f(x^{(t)}) + \\nabla f(x^{(t)})^T (z - x^{(t)}) + \\frac{1}{\\mu} || (z - x^{(t)}) ||^2 \\] <p>Notice that this is a quadratic-ish function in multi-dimension and then the shape we have is locally convex. We say that this \\(g(z)\\) looks very much like \\(f(z)\\) from a local perspective and more importnatly, this \\(g(z)\\) function is convex and we can use the traditional calculus method of:</p> \\[ \\nabla_z g(z) = 0 \\] <p>And we can retrieve the same result that</p> \\[ z^* = x^{(t)} - \\mu \\nabla f(x^{(t)}) \\] <p>Again, by assuming our function is locally convex (locally L-smooth to be specific, hessian bounded), we can hide away much complexity into approximations.</p>"},{"location":"articles/mathamatics/optimization/#optimality-guaranteed","title":"Optimality Guaranteed","text":"<p>We never really formally define what it means to be concvex here, but for now let's just say that convexity means that we have a Postive Semi-Deminite (PSD) hessian (this is not a definition but a result of teh definition). With gradient descnet + convexity + some tricks (teloscoping theory, series of convex functions, ...), we can have many powerful optimality guaranteed, namely: \"It will stop, it will converge, and we will be at the optimal position\". We will name a few here:</p>"},{"location":"articles/mathamatics/optimization/#l-lipschitz","title":"L-Lipschitz","text":"<p>L-Lipschitz means that the gradient is bounded where \\(|| \\nabla f(x) || \\leq L\\) and equivalently we have:</p> \\[ ||f(x) - f(y)|| \\leq L ||x - y|| \\] <p>With a convex function \\(f\\), initial guess in range \\(||x^{(0)} - x^*|| \\leq R\\), total \\(T\\) iterations, and the learning rate \\(\\mu = \\frac{R}{L\\sqrt{T}}\\), we can guarantee that the average distance/error to the optimal coordinate \\(x^*\\) under function being bounded by:</p> \\[ f(\\frac{1}{T} \\sum^{T-1}_{s=0}x^{(s)}) - f(x^*) \\leq \\frac{RL}{\\sqrt{T}} \\] <p>This is the first proof we introduced such that we can say confidently: gradient descent will stop.</p> <p> <p>\u2699 Proof of L-Lipschitz + gradient descent convergence</p> </p>"},{"location":"articles/mathamatics/optimization/#l-smooth","title":"L-Smooth","text":"<p>L-smooth means that the hessian is bounded where \\(0 \\leq v^T \\nabla^2 f(x) v \\leq L\\) (since we are looking at the hessian, we need to bound by matrix norm).</p> \\[ ||\\nabla f(x) - \\nabla f(y)|| \\leq L ||x - y|| \\] <p>We can guarantee that at each step, tehfunction value decreases:</p> \\[ f(x^{(t+1)}) \\leq f(x^{(t)}) - \\frac{\\mu}{2} ||\\nabla f(x^{(t)})||^2 \\] <p>and more importantly, we should have at least one \\(x^{(t)}\\) satisfying the following cndition (this root boost convergence speed hugely):</p> \\[ \\|\\nabla f(x^{(t)})\\| \\leq \\sqrt{\\frac{2(f(x^{(0)}) - f(x^*))}{\\mu T}} \\] <p>This is an incredably strong condition since non of the L-smooth proof used the fact that teh function need to be convex, only that they are second degree differentiable and L-smooth, signifying that with just gradient descent: \"we will stop at some point and this would be optimal, no matter convexity or not\". For more information, reference to this note:</p> <p> <p>\u2699 Notes on convex optimization</p> </p>"},{"location":"articles/mathamatics/optimization/#all-families-comes-from-twitch","title":"All Families Comes From Twitch","text":"<p>Now after convexity and basic formulation of gradient descent, this is where we get to the interesting part, turns out that all the variants and instances of gradient descent (Coordinate descent, Uniform descent, Newton's method (i.e. ADAM), GD with Momentum, Nestrov Acceleration, Conjugate GD, ...) is all somewhat like GD but a little twitch on the theoritical formulation.</p>"},{"location":"articles/mathamatics/optimization/#norm-in-gradient-descent","title":"\"Norm\" in Gradient Descent","text":"<p>Turns out that there is actually a norm hidden in the gradient descent algorithm. When we use GD, we are saying that:</p> \\[ x^{(t+1)} - x^{(t)} = -\\mu \\nabla f(x) \\] <p>When swapping into Taylor's theorem</p> \\[ f(x^{(t+1)}) + \\nabla f(x^{(t)})^T (x^{(t+1)} - x^{(t)}) \\approx f(x^{(t)}) - \\mu \\nabla f(x^{(t)})^T \\nabla f(x^{(t)}) \\] \\[ \\approx f(x^{(t)}) - \\mu ||\\nabla f(x^{(t)})||^2 \\] <p>Normally speaking, this norm is a Eucledian norm or norm 2. However, in the same fashion, we can switch to norm-1 or norm-infinity$. This is essentially framing gradient descent as a constraint optimization problem. How do we optimize in the set of this sphere, or this dimond, or this pyramid? With different constraints, GD comes with different property, namely coordinate descent or uniform descent.</p> \\[ L_1 \\rightarrow \\text{Sparse Coordinate Descent}: \\quad \\tilde{p}(x) = \\text{sgn}(\\nabla f(x)) \\cdot \\frac{|\\nabla f(x)|_i}{|\\nabla f(x)|_{\\max}} \\] \\[ L_{\\infty} \\rightarrow \\text{Uniform Descent}: \\quad \\tilde{p}(x) = \\frac{\\text{sgn}(\\nabla f(x))}{||\\text{sgn}(\\nabla f(x))||_{\\infty}} \\]"},{"location":"articles/mathamatics/optimization/#newtons-method","title":"Newton's Method","text":"<p>Now forget everything we just discussed about gradient descent and we will look at it from  brand new perspective. We used \\(f(x) \\approx f(x^{(t)}) + \\nabla f(x^{(t)})^T (x - x^{(t)})\\) to derive gradient descent before, now let's use the second degree expansion like we did in local convex perspective. However, instead of making a simplified assumption, let's set the gradient directly to \\(0\\).</p> \\[ f(x) \\approx f(x^{(t)}) + \\nabla f(x^{(t)})^T (x - x^{(t)}) + \\frac{1}{2} (x - x^{(t)}) \\nabla^2 f(x)^T (x - x^{(t)}) \\] <p>Setting gradient \\(\\nabla_x f(x)\\) to zero:</p> \\[ \\nabla_x f(x) = \\nabla f(x^{(t)}) + \\nabla^2 f(x) (x - x^{(t)}) = 0  \\] \\[ \\nabla f(x^{(t)}) + \\nabla^2 f(x) (x - x^{(t)}) = 0  \\] \\[ \\nabla^2 f(x^{(t)}) (x - x^{(t)}) = -\\nabla f(x^{(t)}) \\] <p>Notice that \\(\\nabla^2 f(x^{(t)})\\) is a matrix (A), \\((x - x^{(t)})\\) is a vector (x), and \\(-\\nabla f(x^{(t)})\\) is a vector (b), so essentially we are actually solving the famous problem in linear algebra: how to find \\(Ax + b = 0\\). Alternatively, by changing \\(Ax = -b\\) into \\(x = - A^{-1}b\\):</p> \\[ x^{(t+1)} - x^{(t)} = - [\\nabla^2 f(x^{(t)})]^{-1} \\nabla f(x^{(t)}) \\] <p>Or we can write in the update form of Newton's method (fun fact, this is called Newton's mthod because the first algorithm os setting to zero is invented by Newton in 1D, is just taht we are doing in \\(R^n\\)) as:</p> \\[ x^{(t+1)} = x^{(t)} - [\\nabla^2 f(x^{(t)})]^{-1} \\nabla f(x^{(t)}) \\] <p>However, inverting this \"A\" matrix comes with insanly high computation cost and numerical instability. This is where our familier friend ADAM optimizor comes in, which is essetntially a Quasi-Newton method, or a family of algorithm that estimates/creates condition for this inverse to be easily calculated.</p> <p>Using Newton's method come with very interesting convergence property such that if we converge, we converge exponentially fast.</p> \\[ (1) \\quad ||x^{(t)} - x^*|| \\leq \\frac{2h}{3L} \\] \\[ (2) \\quad ||x^{(t)} - x^*||^2 \\leq \\frac{3L}{2h} ||x^{(t-1)} - x^*||^2 \\] <p>However, we don't have the guarentee  that \\(\\quad ||x^{(t)} - x^*|| \\leq 1\\), so convergence may not occur, but if it does, it converges expoennetially fast.</p>"},{"location":"articles/mathamatics/optimization/#gradient-descnet-with-momentum","title":"Gradient Descnet With Momentum","text":"<p>All discussion in the following few sections will be around a particular case of a convex function, a nice one (we wil generalize later):</p> \\[ \\phi(x) = \\frac{1}{2} x^T A x \\] <p>Now recall gradient descent and let's add a first order Markovian memory to it. If the current gradient direction aligns with the previous gradient direction, we move a little bit further (constructive interference) and do the opposite if we are exact opposite with previous gradient. This is called momentum and using this method we can avoid sudden updates in teh gradient (zig-zag shape) and create the decent in  much smoother way. Formally, we can write it as:</p> \\[ x^{(t+1)} = x^{(t)} - \\mu \\nabla f(x^{(t)}) + \\beta (x^{(t)} - x^{(t-1)}) \\] <p>To study the convergence property, we can stack up the current state and next state, just like in control theory calculating state transition.</p> \\[ \\begin{bmatrix} x^{(t+1)} \\\\ x^{(t)} \\end{bmatrix} =  \\underbrace{ \\begin{bmatrix} 1 - \\mu \\lambda + \\beta &amp; -\\beta \\\\ 1 &amp; 0 \\end{bmatrix}}_{M} \\begin{bmatrix} x^{(t)} \\\\ x^{(t-1)} \\end{bmatrix} \\] <p>Under a quadratic equation condition of \\(\\frac{1}{2} x^T A x\\) (we can generalize this quadratic equation later with strongly convex property), gradient descnet with momentum will converge under:</p> \\[ \\left(\\frac{k-1}{k+1}\\right)^t \\quad \\text{where} \\quad k = \\frac{\\lambda_{max}}{\\lambda_{min}} \\] <p>A similar approach (Nestrov Acceleration) works similar as momentum, but it goes  little bit further first before taking the gradient. It iis less intuitive than gradient descent with momentum, but in practice is has nice property that out performs momentum, namely under continuou  environment and differential euqation environment. In general, we can say the convergence rate from GD to GD + M to GD + N.A. as:</p> \\[ (\\frac{k+1}{k+1})^t \\rightarrow \\left(\\frac{\\sqrt{k}-1}{\\sqrt{k}+1}\\right)^t \\rightarrow \\left(\\sqrt{\\frac{\\sqrt{k}-1}{\\sqrt{k}}}\\right)^t \\]"},{"location":"articles/mathamatics/optimization/#conjugate-gradient-descent","title":"Conjugate Gradient Descent","text":"<p>Let's go back to thinking about solving this problem mentioned in Newton's method, we want to solve the problem of \\(Ax^* - b = 0\\) where \\(\\nabla \\phi(x) = Ax - b\\). We know that doing the inverse is very costly and numerically instable. So can we solve this problem without inversing A and maybe we can incrementally solve this issue. Let's first define the mathamaticla notion of conjugate, let's say that \\({p_1, ..., p_n}\\) is the conjugate of a Positive Definite (PD) matrix A if:</p> \\[ p_i^T A p_j = 0 \\quad \\text{if} \\quad i \\neq j \\] <p>This conjugate is a general notion of orthogonality and the idea is to maybe update in one orthogonal direction at each time and just not care of it (independent of future update). Intuitively, we can frame it like the following:</p> \\[ x^{(t+1)} = x^{(t)} + \\alpha_t \\vec{p}_t \\] <p>Where</p> \\[ \\alpha_t = \\underset{\\alpha \\in \\mathbb{R}}{\\arg \\min} \\phi(x^{(t)} + \\alpha \\vec{p}_t) \\] <p>The first ensures the orthogonal step amd teh second ensure the descent step. Notice that this second optimization problem is a easier problem to solve and close form solution does exist as we are only looking at a minimization problem that involve two vectors, way easier than the problem we begin with. Close form solution of optimal \\(\\alpha^*\\) would be:</p> \\[ d_t = \\alpha^* = \\frac{(b - Ax^{(k)})^T p_t}{p_t^T A p_t} = \\frac{- \\nabla \\phi(x^{(t)})^T p_t}{p_t^T A p_t} \\] <p>This almost looks like a gradient descent with just a bunch of other fancy multiplication in it. Turns out that we are actually doing a gradient descent that is projected onto the \\(p_t\\) conjugate axis. Taking the gradient in the component direction of \\(p_t\\). All of theses sounds very nice, but we still have one problem. How do we find the conjugate? Doing \\(p_i^T A p_j = 0 \\quad \\text{if} \\quad i \\neq j\\) is not a cheap operation. Let's do it like Bellman Update, let's use a dynamic way of updating the conjugate!</p> <p>Assume that each \\(p_t\\) only need the previous one \\(p_{t-1}\\) to be computed, we just need the current vector to be conjugate to the previous one, and if the chain forms for being conjugate, all of them will be conjugate vectors and we can throw away \\(p_o\\) to \\(p_{t-2}\\). Let's start with:</p> \\[ p_t = -\\nabla \\phi(x^{(t)}) + \\beta_t p_{t-1} \\] <p>Then we multiply \\(p_{t-1} A\\) to both side and try to make RHS zero.</p> \\[ p_{t-1}^T A p_t = -p_{t-1}^T A \\nabla \\phi(x^{(t)}) + \\beta_t p_{t-1}^T A p_{t-1} \\] <p>This has a close form solution again where we can calculate the optimal \\(\\beta_t\\) to satisfy this condition. The CGD algorithm becomes clear as well, we just need to randomly initiate \\(\\beta\\) and \\p_0$, then update base on this given constraint.</p> \\[ \\beta_t = \\frac{p_{t-1}^T A \\nabla \\phi(x^{(t)})}{p_{t-1}^T A p_{t-1}} \\] <p>Conjugate gradient descent is very very powerful (similar to gradient descent with momentum):</p> \\[ ||x^{(t)} - x^*||_A \\leq 2 \\left(\\frac{\\sqrt{k}-1}{\\sqrt{k}+1}\\right)^t ||x^{(0)} - x^*||_A \\]"},{"location":"articles/mathamatics/optimization/#beyond-convexity-optimality-may-be-guaranteed","title":"Beyond Convexity, Optimality May Be Guaranteed","text":""},{"location":"articles/mathamatics/optimization/#strongly-convex","title":"Strongly Convex","text":"<p>What happens beyond convexity? We can still talk about them in theory (though not as convineint as in convex situations). First we will introduce the concept of stronly convex, or essentially swapping out the \\(\\nabla^2 f(z)\\) with the smallest eigen value of such matrix and that \\(\\nabla^2 f(z) \\geq CI\\). This creats an upper bound condition called strong convex, or essentially constructing sort of a tangent curve instead of a tangent line:</p> \\[ f(y) \\geq f(x) + \\nabla f(x)^T (y-x) + \\frac{C}{2} ||y-x||^2 \\] <p>and that</p> \\[ f(x) - f(x^*) \\leq \\frac{||\\nabla f(x)||^2}{2C} \\] <p>When  strongly convex is achieved, L-smooth condition is matched, and when choosing $\\mu =\\frac{1}{\\mu}, we have a incrediablly strong convergence rate, an exponential convergence rate that is independent of the condition number \\(k\\).</p> \\[ f(x^{(t+1)}) - f(x^*) \\leq (\\frac{C}{L})^t (f(x^{(0)}) - f(x^*)) \\] <p>More critically, when strongly convex condition is matched, all the previous method's convergence property remains for functions that is strongly convex but not neccessarily just quadratic in the form of \\(\\frac{1}{2} x^T A x\\). This is really how strongly convex is used in practice. Practically speaking, when $f(w) is convex and \\(R(w)\\) is c-strongly convex, then \\(f(w) + R(w)\\) is also c-strongly convex. This gives the power for regulaorizor to show its power. For instance, in ridge regression, other than just making a constraint on keeping the weights small, ridge regression gurantees a better convergence rate than just normal gradient descent.</p>"},{"location":"articles/mathamatics/optimization/#pl-condition","title":"PL-Condition","text":"<p>When discussion in non-convex situations, we talk about the PL-Condition, which is eessentially a condition that looks very similar with strongly convex where strongly convex implies PL-condition but not vice versa.</p> \\[ f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\text{ satisfies } \\mu \\text{-PL-Condition if } \\forall x, y \\in \\mathbb{R}^n \\] \\[ \\frac{1}{2}||\\nabla f(x)||_2^2 \\geq \\mu (f(x) - f(x^*)) \\] <p>Essentially, we are putting a upperbound on the gradient saying that when we are far from \\(x^*\\), we have  big gradient to move faster. Importantly, PL-Condition has 2 important factors:</p> <ol> <li>PL-Condition can hold for non-convex functions and it acts in as sort of a strongly convex guarantees.</li> <li>If \\(f(x)\\) s L-smooth + \\(\\mu\\)-PL-Condition, then gradient descent with a step-size of \\(\\frac{1}{L}\\) will converge at a rate of</li> </ol> \\[ f(x^{(t)}) - \\delta(x^*) \\leq \\left(1 - \\frac{\\mu}{L}\\right)^t (f(x^{(0)}) - \\delta(x^*)) \\] <p>This is similar to strongly convex, very strong exponential convergence. In practice, we actually observes this phenomenon! Turns out that an over-parametrized neural network (highly non-convex function) can be written in a Neural Tangent Kernel form of:</p> \\[ ||\\nabla L(w)||^2 \\geq 2\\mu L(w) \\] <p>Which satisfy the PL-Condition and neural network converges as the rate of \\((1 - \\frac{\\mu}{\\beta})^t\\), it converges exponentially fast. This is exactly why we see exponential convergence in teh begining when we randomly initialize an neural network.</p>"},{"location":"articles/mathamatics/stochastic/","title":"Unfolding Stochasticity Sequentially","text":"Kaiwen Bian 10 min read \u00b7 Jun 12, 2024"},{"location":"articles/mathamatics/stochastic/#setting-bases-of-sequential-process-in-nature","title":"Setting Bases of Sequential Process In Nature","text":"<p>       Stochastic processes is abut understanding from local to gloabl, by having single transition probability matrix (discrete) or transition probability function (continuous) and trying to understand        some global behavior of such chain or processes (i.e. absorption time, expected time of termination, or stationary distribution). WHat I found stochastic processing to be really amazing is how it is trying to model        how randomness and stochasticity may play out in nature, and not at one time stamp but rather across different time stamp sequentially. It try to reason and capture some details about the nature, to undertsand a chain of        \"randomness\" seuqntiall (not so random when you cna model it). Compare to random variables that reasons about how \"randomness\" plays out at one time stamp, stochatsic processes may dive into the interactions between \"randomness\" across time, to        dive into their interactions, seeing how they sequentially interact and the prior effect the laters (of course under markov property only one layer of looking backward is needed).   </p> <p>Random Walk: Key Example</p> <p>       In this section I want to discuss (not techniqually) about how amazing this field may be with one classic example that bridges across discrete Markove Chains (MC), Continuous Time Markov Chains (CTMC) and COntinuous Time Continuous State Brownian Motion (BM): Random Walk (RW).   </p> <p>\u2699 Some Notes on Stochastic Processes</p> <p>Some extension with more techniqual details on the three interpretations of Continuous Time Stochastic Processes.</p> <p>\u2699 More Techniqual Details</p>"},{"location":"articles/mathamatics/stochastic/#markov-chain-analysis-into-stochaticity","title":"Markov Chain: Analysis Into Stochaticity","text":"<p>       When considering about a sequential process that have \"randomness' depending on teh previous \"randomness\", onemust consider using ideas from conditional probability, which discusses about a distribution given that a different distribution has occured. No matter MC, CTMC, or BM, they all hold/are established on        a very powerful property known as the Markov Property that makes many of the techniqual details working with conditional probability much easier. Mathamatically:       $$       P(X_n = x | X_{n-1} = x_{n-1}, \\dots, X_0 = x_0) = P(X_n = x | X_{n-1} = x_{n-1})       $$        $$       P(X_{n+1} = x_{n+1} | X_n = x_n) = P(X_{n+1} = x_{n+1} | X_n = x_n, \\dots, X_0 = x_0)       $$       Saying that the \"current\" \\(X_n\\) does not depends on distance past but rather only the nearest past \\(X_{n-1}\\) and that the \"future\" \\(X_{n+1}\\) does not depend on the past but only the \"present\" \\(X_n\\).   </p> <p>       With Markov Property holding, mathamatician can reason with the chain or such sequential process much more easily and some global probability such as expected time of termination, return probability, stationary distribution can be analytically calculated. One key theme in stochastic processes is that one might        want to setup an recureent (recursive) system to reason about how things unfold, then finding  generalized pattern (explicit solution) from such system of equation. Remanber that a recurrence system is equivalently represented in linear algebra as a system of equation when flatening all of it out, then numerical optimization can        also be conducted. Taking an example of the return probability:       $$       u_{ik} = P_{ik} + \\sum_{j=0}^{k-1} P_{ij} u_{jk} = P(X_{T} = k | X_0 = i)       $$       which talks about teh probability of terminating the chain at point \\(k\\) given that it starts on point \\(i\\). Equivalently, it can be written in matrix format for numerical optimization:       $$       u^{(k)} = (I - Q)^{-1} R^{(k)}       $$       Same idea can be used to reaosn with expected time of termination:       $$       E[T | X_0 = i] = 1 + \\sum_{j=0}^{r-1} P_{ij} \\omega_j = \\omega_j       $$       These types of reaosnings are known to be First Step Analysis and it is one of the mos commonly used idea to reason with a sequential event in stochastic processing.   </p> <p>       To the core example that I want to cover in this section: Random Walk (RW). RW in the discrete state + discrete time condition is essentially a Markov Chain with the transition probability matrix:       $$       \\Pr(X_{n+1} = j | X_n = i) =        \\begin{cases}        q_i &amp; \\text{if } j = i-1 \\\\       r_i &amp; \\text{if } j = i \\\\       p_i &amp; \\text{if } j = i+1       \\end{cases}       $$       under the constrained that \\(q_i + r_i + p_i = 1\\).   </p> <p>       Knowing that this is a Markov Chain and its transition probability matrix, much can be conducted and all of the analysis that was mentioned earlier can be used to help finding a global state of the chain. It        is worth notice that though vanilla RW is only in discrete state + discrete time condition, it is actually a core example taht can be carried over to more complicated situations. Even under discrete condition,        it can serev a pretty fine model for stochastic modeling in some practical senerios. For instance, gamble's ruin modeling or modeling stochasticity to see if learning has occured.   </p>"},{"location":"articles/mathamatics/stochastic/#continuous-time-markov-chain-discretize-functional-analysis","title":"Continuous Time Markov Chain: Discretize + Functional Analysis","text":"<p>       Now this is where tings gets a lot more complicated because CTMC is trying to reason under the condition that time is continuous, which introduces many more deficulties mathamaticaly. Remenber that we said one core idea in stochastic processes is to setup a recurence system and        try to solve such system? When state is discrete, we can count them as steps and do recursion in that fashion, but when states are continuous, recursion can still be conducted, but sometimes with differential equation, which is not something that w  want to do, the complexity is very high.        When studying more into CTMC, one may encounter many classic processes that are CTMC, namely most of the classic examples deals with reasoning \"when customer come into and leave a store\":       <ul> <li> <p>                   Poisson Process (PP): constant \\(\\lambda\\) (rate), random time, unidirectional jump               </p> </li> <li> <p>                   Pure Birth Process (PBP): dynamic \\(\\lambda\\) (rate), random time, unidirectional jump               </p> </li> <li> <p>                   Birth and Death Process (BDP): dynamic \\(\\lambda\\) (rate), random time, random jump               </p> </li> </ul>       These process in nature are easier to be described by rate diagram, which is how they are defined (rate diagram description is one of the three ways to interpret an CTMC) and we can use random variables to monitor such process or deducing transition probability function (i.e. increments of an        Poisson Process follows Poison distribution random variable). Maybe in some sense, these random variavbles may be a good abstraction to represent the process just like how weights can be used as a representation of a predictive function? (ideas in Jump &amp; Hold description?) </p> <p>       Problem with description comes where it becomes kind of hard trying to find some global property when reasoning under this paradigm. Thus, we can extend to the second and third interpretation of an CTMC: Jump &amp; Hold Description and Infinite Decimal Generator Decription. This is where things really gets complicated        because the first need to project an contnious chain to an discrete chain by discretize the continuous scale into a discrete numbers of continuous random variable depending on teh process in interest and the second is invlolved in using Functional Analysis techniques and theorems to find an Q Matrix that captures time dependent information        while being itself a time independent matrix (hence the name genertaor). Here is a few names that might be worth looking into:       <ul> <li> <p>Semigroup Property &amp; Hille-Yoshida Theorem: Functional Analysis aspects of stochastic processes that's the bases of the Infinite Decimal Generator Description, creating the Q matrix for generating transition probability functions.</p> </li> <li> <p>Kolmogorov Forward/Backward Equation: Turning transition probability function into a system of differential equation, also connecting to dieas later on in Brownian Motion using  as set of differential equation to monitor a system.</p> </li> </ul> </p> <p>Different Description of Continuous Time Markov Chain</p> <p>       However, though being extremely complex, when dive into the analysis, it has a deep connection back with random walk, but just under a much complicated condition.   </p>"},{"location":"articles/mathamatics/stochastic/#brownian-motion-bridging-analyst-probabilist","title":"Brownian Motion: Bridging Analyst &amp; Probabilist","text":"<p>       Moving to Brownain motion, this is modeling stochasticity under continuous time and continuous state, which has a very deep theoritical root connecting back to the Analysis aspects of mathamatics. This is also where the quote I put on the left comes from where equality equation refers to the Analysis aspects of mathamatics and inequality equation refers to the modeling aspects of mathamatics.    </p> <p>Inspired by Prof.Carfagnini: \"In math you either deal with equality equation or inequality equation, but sometimes they bridge\"</p> <p>       Essentially in Brownian Motion, wes ay that modeling a system of differentiual equation to know everything about how heat dissapate in a region is the eqivalent with deploying a stochaastic agent into the environment, let it play around and see how the probabilistic rollout would be to map out the boundary condition (analyst's approach v.s. a probabilist's approach). And, again, when looking it from        certain angle, the connection to Random Walk als shows up again.   </p>"},{"location":"articles/neuroscience/affective_neuroscience/","title":"Sensory, Processing, and Affective Neuroscience","text":"Kaiwen Bian 5 min read \u00b7 Jun 11, 2024 <p>       We precieve the world, the environment, and things around us through sensory informations, through sensory receptors capturing informations (i.e. wave length of light (visual color), checmical flying in airs (smells),...) and delivering it        to many sophesticated informational pathway that sends information across the brain for processing. From there on, we may talk about perception or deeper processing of these informations in PFC or in Amygdala (some may have direct projection,        there are simply too many complciated circuit in the brain for information procesing that spreads spatially and temporally, sometimes reaching the same target across different time periods). I like to think of it as 3 layers, from sensory to        processing to more higher level and affective processing. </p>"},{"location":"articles/neuroscience/affective_neuroscience/#encode-neural-representation","title":"Encode Neural Representation","text":"<p>       Here is where neuroscience gets a little bit philosophical and wehere I find neuroscience to be facinating, on the sensory processing information, if we account that the idea or representation of the information comes from perception (i.e. you would probably know what bird singing would        probably sounds like if you heard it before), but the nature of the information may not take form in such representation (i.e. birds' singing is techniqually waves and only when encoding such information in the basilar membrane from wave signal to neuronal signals is where these representations are created),        then when these natural form of the information is not been captured, does the representation of the information still exist? Or in another word: \"Does a sound exist when a tree falls in the forest, if no one is near enough to hear it?\".   </p> <p>From Wave to Neural Information (Borrowed from Prof.Mooshgian's Cogs107B Slides)</p> <p>Here is one of my favorite quote on ideass relating to system neuroscience:</p> <p>           \"Our perceptions differ qualitatively from the physical properties of stimuli because the nervous system only extracts certain information from a stimulus and then interprets this information in the context of its earlier experience. We experience            electromagnetic waves of different frequencies not as waves but as actual colors that we see: red, blue, or green. We experience objects vibrating at different frequencies as tones that we hear. We experience chemical compounds dissolved in air or water as            specific smells or tastes. Colors, tones, smells, and tastes are mental constructions created by the brain out of sensory experience. They do not exist, as such, outside of the brain. . . . Does a sound exist when a tree falls in the forest, if no one is near enough to hear it?            We now believe that the fall causes vibration in the air but not sound. Sound only occurs when pressure waves from the falling tree reach and are perceived by a living being.\"       </p> <p> From Principles of Neural Science, 3rd edition., eds. Kandel, Schwartz, &amp; Jessell, p. 330</p>"},{"location":"articles/neuroscience/affective_neuroscience/#representation-of-the-world","title":"Representation of the World","text":"<p>From neural encoding, our brain builds a representation, a mental representation of the environment that we are in, a \"salient map\" (particularly from vision) to help us navigate. Here is an classic example that would probably be covered in an system neuroscience class and also one that illustrates some        quite interesting points: Hemispatial Neglect. Hemispatial Neglect Theory discusses, under damages to the Parital Lobe and Cingulate Gyrus, the damaged brain neglects one side of the vision field completely (vision is teh most severe, but auditory, mechanical senses are also effected). This is not a sesnory problem as all the sensory pathways        are intact but rather a processing issue (an \"attention\" one). In another word, that damaged brain's mental representaion (space's representation in the mind's eye) is missing half of the information. This example really delivers how the brain may be creating a model of the world, a represenattion of it through some fundamental processing first, illustrating that        sensory information is one thing and the processed version is another. Here comes an interesting question: \"would developmental differences cause different neurogenisis and synapsis formation that makes one precieve the world completely different from another?\"   </p> <p>Vision Salient Map (Borrowed from Prof.Mooshgian's Cogs107B Slides)</p>"},{"location":"articles/neuroscience/affective_neuroscience/#affective-processing","title":"Affective Processing","text":"<p>       Now moving to more higher level and affective processing, one primary question that we should ask is: \"what is emotions?\". Personally, one of my favorite interpretation of emotion is from Prof.Chiba's cognitive neuroscientist's perspective, which states that:   </p> <p>Emotion reflects a kind of motion outward, an inferred complex sequence of reactions to a stimulus including cognitive evaluations, subjective changes, autonomic and neuronal arousal, impulses to action, and behavior designed to have an effect (functional) upon the stimulus that initiated the complex.       </p> <p>       That is a pretty techniqual definition, but the key idea is that emotion may be a very complicated circuitery phenomenon that spans across many regions of the brain. In neuroscience, Amygdala have always been tagged as the center of emotions as people really like to classifiy biological system with engineering thinking. However, is it really 100% true that it is built for emotion?        It is the center of many things that associates with emotions, but emotion processing is just a tag that we are giving to amygdala. It just happen to be a center of many information control, receive a lot bottom up sensory input, learns very quickly, monitors introceptivly while also extroreceptively, monitors reward association, and also projects greatly to many other cortical areas. It is key to        realize that Amygdala is not the only player as Analysis from higher level does matter, pripor knowledge-based processing does occur, your Prefrontal Cortex plays a huge role in emotion and it is more than the stimulus of the cue or the response of the Amygdala to these cue, the interpretation of them matters. </p> <p>Extended Amygdala (Borrowed from Prof.Chiba's Cogs107C Slides)</p> <p>       In a different word, we constructs the world we live in and we constructs such labels of emotion from a collection of big circuit and modulatory changes, from retrospective perspective, we made emotions.   </p>"},{"location":"articles/neuroscience/cognitive_brain/","title":"Cognitive Neuroscience's Perspective","text":"Kaiwen Bian 5 min read \u00b7 Jun 10, 2023 <p>       Inspired by Prof.Chiba from Cognitive Science Department at UCSD: In cognitive neuroscience, the brain paints a story about \"all the area, all the functions, all the state, all at once\".   </p>"},{"location":"articles/neuroscience/cognitive_brain/#never-a-control-rather-a-balance","title":"Never a Control, Rather a Balance","text":"<p>       For so long people have trying to pose an \"engineering system\" perspective on the brain and on biology, which simply may not be true.   </p> <p>OPFC (Borrowed from Prof.Chiba's Cogs107C Slides)</p> <p> Appropriate balance between excitation and inhibition is a basic principal of brain functions and stable cognitions. Looking at the anatomy, you would be shocked by the intricate circuit our brain has: \"processing everything in all places to support all functions and all at once in parrallel\".        The brain is so much more than just areas connecting together, it is a compelx, in parralel and recurrent circuit (no area is responsible for an function and no function is limited to an area). For instance, the Prefrontal Cortex (PFC) is deemed to be a key component that supports executive functions, learning, decision making, error testing,        mental sketch pad, reaprasal, representation learning, inhibition of inappropriate action. In another word, it supports so much of the important functions that makes us \"intelligent\". However, no functions would be carried out if the Basal Ganlia doesn't have the ability for reinforcing actions with dopaminergic cells from Ventral Tagmental Area (VTA), or if the Basal Forebrain        doesn't have the cholinergic cells to desynchronize the brain areas for low tonic/high phasic optimal sensory processing (changing the dynamics of the cortex). Note that this is just examining one small circuit on the cortical level, there are so much more to talk about when we expand our scope to more subcortical areas or brain stem areas (i.e. Hypothalamus, HPA Axis, Extended Amygdala, CRH Pathway's Effect on        Development &amp; Functioning, Sensory Information Procssing Pathways, Brain-Body Connections,...).   </p> <p>PFC/Amygdala (Arnsten, A. F. T. (2009))</p> <p>     To drive home the points about the complexity of the brain, here is some \"small\" circuit illustration:   </p> <p>\u2699 \ud83e\udde0 Some \u201cSmall\" Circuit in the Brain </p>"},{"location":"articles/neuroscience/cognitive_brain/#the-bodily-brain","title":"The Bodily Brain \ud83e\udde0","text":"<p>When thinking about the brain, we need to realize that it is not about a \"neck-up\" science. The brain and the body are always tightly connected is capable of powerfully influencing each other's development. The first effect may be well known, but the the reverse had been discovered recently to be true as well.        I want to use the following 4 examples to demonstrate such point:   </p> <ul> <li> Bone-Neurogenisis Connections <p>               The birth and death of neurons can be influenced by oxycalcin (a hormonal substance in the bones) that can inhibit apoptosis and promote neurogenesis.           </p> </li> <li> Gut-Stress Connections <p>               Gut microbiota significantly impact brain development where studies have shown that Germ-free rats exhibit increased HPA activity compared to normal rats, signaling that they are more prone to stress or CRF effects. Think about this, this sounds crazy that our stress level                can be influenced by microbes in our gut. Furthermore, this change in HPA activity leads to worse spatial memory performance since a properly regulated stress response, influenced by the gut microbiota, is important for optimal cognitive function, including memory. Thus, the imbalance or absence of gut                microbiota can negatively affect brain functions related to memory and learning.           </p> </li> <li> Stress-VMPC Connections <p>               During pregnancy, mother's body influences the fetus where increased stress in a pregnant mother leads to an decreased efficacy of the vagus nerve in the infant. If this infant does not experience sufficient opportunities to further develop this system, they may experience difficulties regulating using their parasympathetic nervous system, which                may lead to an overall increase in cortisol and CRF. This can potentially lead to delayed development of the Ventromedial Prefrontal Cortex (VMPFC), which is a part of the cortex that is the key for learning and emotional regulation as it poses projection directly on teh Amygdala for inhibition. With less developed VMPFC comes with less emotion stability and decision making.           </p> </li> <li> Stress-Allostasis Connections <p>               Knowing the effects decribed above, the same can be said for teenager anxiety, the more anxious they are, the more CRH there would be from the HPA axis trying to regulate Homeostasis. However, the brain is not just about homeostasis, it chanegs over time and have both mechanism for feeding backward (via Locus Correculus (LC) projection onto Hypothalamus) and feeding forward (via LC projecting onto                Bed Nucleus of Stria Terminails (BNST) of the Extended Amygdala through Norepinephrine (NE)). It is about Allostasis. Then there will be more CRH released from the feed forward system to \"keep you going\" and causes the VMPFC to be less developed, making emotion stability and learning even harder (not mentioning other down stream effect of PFC being less activated and Amygdala being more activated and support \"habitual mode\").                Thus, making them having more anxiety.           </p> </li> </ul>"},{"location":"articles/neuroscience/parralel_reinforcing/","title":"Searching & Parralel Processing","text":"Kaiwen Bian 5 min read \u00b7 Jun 14, 2024 <p>     Biological system has always been used as inspirations for artificial networks that boosted learning. Upon all of them, I find Basal Ganglia to be the most facinating as it seems to resemble some \"nature\" of      information processing and finding the true structure that stems \"intelligence\" in nature.   </p>"},{"location":"articles/neuroscience/parralel_reinforcing/#basal-ganglia-structure","title":"Basal Ganglia Structure","text":"<p>     Basal Ganglia can be deemed as a \u201creinforcing unit\" that is highly connected to many of the other functional circuit in the brain and it is strongly modulated by the expectation of reward.     As a theme in neuroscience, \"generally speaking, we learn changes in the world\", the errors \\(\\delta\\) from expectation is what governs the neuromodulation system (dopamine ) in the Ventral Tagmental Area (VTA), which      have dense projections on the Striatum that encodes sensory motor information (via direct pathway D1 and indirect pathway D2).   </p> <p>     When reward is expected, the D1 direct pathway would be mroe active with D2 indirect pathway less active (reward association is captured: one facilitate reward prediction and one facilitate non-reward prediction). To be more      specific, the direct pathway would have inhibition from the Striatum to the GPi that inhibits the Thalamus. Thus, such inhibition of the inhibition would cause the Thalamus to be more active, creating positive reinforcing. On the      other hand, the indirect pathwat excite the GPe that inhibits the STN that excites the Thalamus. Thus, such excitation of the inhibition on the inhibition would cause the Thalamus to be less active, creating negative reinforcing.    </p> <p>Basal Ganglia (Kim, Hyoung F., and Okihide Hikosaka (2015))</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#search-manipulate-valuable-objects","title":"Search &amp; Manipulate Valuable Objects","text":"<p>The goal is always to maximize the reward, which can be separate into two problems, one being finding valueable objects in the environments with sensory input (exploration) and the other being manipulating such valuabel object to retain reward (exploitation). To reach      the reward, it is always a brain-wise effort because all attention, motivation, context, uncertainty measurement, risk assessment, ... need to be performed, in here we just particularly examine Basal Ganglia.   </p> <p>     Dopamine signals is what biases sensorimotor inputs, but how should it balance between the exploration adn exploitation? In algorithm research, that is always a question that puzzles researchers, but seems like the brain have its own answer. The blue (CDh-rvmSNr) circuit is inherently recieving projection that makes      quick learning + short memory retention while the red (CDt-cdlSNr) circuit is slow learning + long memory retention. In another words, the blue circuit is very sensitive to immediate reward (boost learning) while the red circuit is more proned to using stable-value-objects.   </p> <p>     Not talking about algorithm but just as a discussion about life, this is a neuroscience argument of why you should persue the things you are interested in the moment you find them interested   </p> <p>Exploration/Exploitation (Kim, Hyoung F., and Okihide Hikosaka (2015))</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#parallel-processing","title":"Parallel Processing","text":"<p>     From a more macro perspective, Basal Ganglia recieves multiple input from multiple areas at once. Both limbic/higher control (association cortical areas) and sensory information (somatosensory cortex) reaches the Striatum in the same time. The      true amazing aspect of Basal Ganglia is that no convergence is really needed, the topographic input is preserved in the same format when outputted. It can work independently on several streams of information from different orders of levels.   </p> <p>Projection to Basal Ganglia (Kim, Hyoung F., and Okihide Hikosaka (2015))</p> <p>     In a familier environment with familier objects (stable-value-objects), the objects should have well-predicetd valeus as they have been seen many times (mental representation is well defined): red (CDt-cdlSNr) circuit is active with blue (CDh-rvmSNr) circuit providing less bias. This setup can be seen      as an exploitation stage if putting under the framework of algorithm, it is maximizing the reward from previously known values. On teh other hand, if the envioronment is unfamilier (flexiable-objects), we can \"pick some object adn check their values\", refering to      the exploration stage. Overtime as more are explored, these known \"well-known value objects\" becomes stable-value-objects (I kind of interpret it as frozen neurons).   </p> <p>     That was describing one object, but in the real environment it is always multiple objects happening at thes ame time, some flexiable-object and some stable-value-objects, parralel processing comes in and the circuit is all processing in the same time  ith very different input, recieving projection and      projecting to many other functional areas + circuits. Techiqually speaking, it is many autonomic mechnisms and one voluntary mechanism operating in parralel to aim to achieve higher level common goals that requires multiple motor and mental processes, between mental processing and taking actions      (maybe this solves search and optimization problem?).   </p> <p>Learning is very expensive, long term learning is needed to learn a stable representation of coordination of such mental and motor functions (i.e. learning a sequential movement, from thinking of the movement to doing the movement). Initially, the motor action may be concious with many    prediction errors, but later performance should increase and the motor action to mental representation association is learned and motor actions can be facilitated by learned cognitive processes. Reward's association of flexiable-object is transfered to a stable-value-object.   </p> <p>Parallel Processing (Kim, Hyoung F., and Okihide Hikosaka (2015))</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#continual-learning-algorithms","title":"Continual Learning &amp; Algorithms?","text":""},{"location":"articles/neuroscience/parralel_reinforcing/#balancing-search","title":"Balancing Search","text":"<p>     To me the previously descrribed system that balances the search problem seems to be very much like AlphaGo, but I am not sure if either the discovery of them draws inspiration from each other, which is the exact reason of why this is so facinating: maybe some truth, some correct representation/structure of stemming intelligence is being captured.   </p>"},{"location":"articles/neuroscience/parralel_reinforcing/#different-perspective-on-learning","title":"Different Perspective on Learning","text":"<p>     Drawing inspirations on how we learn, When the system is initially learning, the representation of the weight should not be \"locked\" fully, but when learning continually, some key representation should be learned and then these stable-value objects should be locked for later needed task as a cognitive processes. The question of CL should be deem as a sequential learning that is not      epsisode over an episode tasks trying to preserve the previously learned information, but as a whole thing of continuously learning trying to find/search for the actual correct/useful representation over a sequential unit of time and training (learning some representation, then some more).   </p> <p>       From Prof.Gao: \"We may know the principal, but Computers may travel to places that is far beyond us.\"     </p> <p>With in mind, we should let the system itself to search for the right representation to lock, we should let them serach for needles in the haystacks.</p>"},{"location":"articles/neuroscience/parralel_reinforcing/#parralel-processing","title":"Parralel Processing","text":"<p>There is one stream of higher level guidance (voluntary voice): learned stable-value-objects or previously learned weights. Moreover, such stream of guidance processes simultaneously in parralel with the otehr sensory input: flexiable-objects or new data induced weightes.</p>"},{"location":"articles/neuroscience/positive_psycology/","title":"What We Think Deternmines What We Can Think","text":"Kaiwen Bian 5 min read \u00b7 Aug 20, 2024 <p>     Neurosciecne and psycology have usually been discussed together. After all, I think that one can not go to all the abstract realm of the mind without grounding      in a biological system. They have the mutual relationship where thoughts develops the neurological foundations within our brain and later such system we      developed effects how we think and feel (i.e. the formation of habits is a great demostration of Hebian plasticity and such system formation).   </p> <p>     I have always been facinated by the connections of imperial psycology findings and the corresponding biological mapping delivered by neuroscience, particularly about how      our ideas and thoughts may be shaped by the biological circuit behind it, discussing abstract ideas in psycology, but grounded in biology. I believe that with      understanding of psycology and neuroscience, we can make better decisions and reach closer to where we want to be at. To some degree, just like practcing muscles, we want      to develop a neuronal system that is strong and suitable for what we want. I want to discuss it through the lens of Positive Psycology and the      ideas discussed here are drawn mainly from professor Kaiping Peng's class in hundun-academy.   </p>"},{"location":"articles/neuroscience/positive_psycology/#positive-psycology","title":"Positive Psycology","text":"<p>     Evolution doesn't pick the strongest or the smartest species, but rather those that adapt to changes and cooperate most effectively. It is crucial for us to have the ability to manage our internal state and adapt to fit what      the environment need. This is the same as \"state flexability\" in the context of Allostasis (discussed in the other article). In positive psycology research, people have found these 3 characteristics to be key of adaptation.   </p> <ul> <li>Ability to constantly rise again - Resilience: The capacity to recover quickly.</li> <li>Ability to be tenacious - Grit: The ability to endure and persist.</li> <li>Post-traumatic growth and development - PTGD: The ability to not only survive but thrive after experiencing hardships and challenges.</li> </ul> <p>     I think that there are 3 (2 emphasizing on the abstract level and 1 on the biological detailed level) core aspects (Self Efficacy, Growth Mindset, and Building Neuronal Circuits) that can      give an good example of how what we think effects what we do and what we can think later. We don't want to not fail negativity, but rather how to think of it and react towards it.   </p>"},{"location":"articles/neuroscience/positive_psycology/#self-efficacy","title":"Self Efficacy","text":"<p>     Self-efficacy is the faith that you give to youyrself, which is crucial to the ability to withstand pressure. It is about believeing that you can and you have the      ability to succeed. With high self-efficacy, stress and challenges are opportunities to prove yourself. Though self-efficacy is a higher-level abstraction idea, we can      use other abstractions to trigger neruonal adaptations, namely:   </p> <ul> <li> Fake it until make it: When thoughts becomes action repeatidly, it becomes habits, then habits becomes more actions, repeated wise.     </li> <li> Vicarious success: When being accepted by people you want to be, it becomes easier to be these people.     </li> <li> Unexpected social support: Evolution favors cooperators (the strong effect of weak ties). Opportunities are usually brought by people that you wouldn't expect.     </li> <li> Simulated practice: One of the greatest power of the human brain is our predictive ability. Our brain can envision the future or jump around dimensions neglecting the effect of time.        When the brain is under the default mode/circuit (a mode when you are not exactly thinking about anything and just being \"board\", i.e., during shower), it constantly makes predictions,        processes emotions, conduct introspects, at a level beneath cognition. With such ability, we can escape the realm of \"reality\" to travel in much more abstract space.     </li> <ul> <li>         We can conduct \"visualization\", to mentally imagine success. The more you envision, the clearer the intention (mirror cell effect as well).       </li> <li>         Instead of focusing on details, see the bigger picture, the global properties.       </li> </ul> </ul>"},{"location":"articles/neuroscience/positive_psycology/#growth-mindset","title":"Growth Mindset","text":"<p>     A growth mindset, as contast to a stationary mindset, is a belief believing that only attitude and effort determine what can be learned, where      risks and challenges are enjoyed and self-feedback/suggestions determines outcomes. Th greatest hinders to a grwoth mindset is The burden of excellence where people believe in      that they \"should be something\" instead of they \"could be something\". This can can lead to negative feelings (below the standard line), while \"earned through effort\" provides positive feelings.      A growth mindset believes that there is no \u201cshould,\u201d no \u201cshould be good at learning\u201d or \u201cshould be smart,\u201d risks and challenges bring new outcomes.   </p> <ul> <li> Process-oriented: Self-assessment standards should be process-oriented, not outcome-oriented, R&amp;D is about challenges and risks (Success should be measured by changes, just as dopamine does).        Instead of saying that an event is \"impossible to achieve\", frame it as \"not yet\", or \"maybe try more later\".     </li> <li> Telling of the story makes the story: The stories we weave causes physical and mental reactions that we felt     </li> <ul> <li>         Activating event -&gt; Belief -&gt; Consequences: A cannot be changed, but B can, hence C changes. Use the prefrontal cortex to refute B's thoughts.       </li> <li>         Remanber that the failing to acknowledge negative aspects means missing out on positive experiences; the two are interconnected. One cannot achieve positivity without ubnderstadning negativity.       </li> </ul> </ul>"},{"location":"articles/neuroscience/positive_psycology/#thoughts-to-biological-circuits-and-vice-versa","title":"Thoughts to Biological Circuits and Vice Versa","text":"<p>     It's not the event itself that matters, but rather the perspective and the interpretation of it, attitude determines actions and reality. A positive cognition would deliver a physical and mental state that is      full of vitality and makes it easier to see the goodness in humanity. However, it is impossible to not feel negative emotions. Hinders of feeling negativity causes unhealthy connections between Amygdala and PFC,      causing potential development of being a psycopath. With that being said, we can use our understanding of neuroscience to navigate what we can do when negative feeling strikes, the key is always to adapt      to changes that the environment poses and regulate our internal feeings/mental state, what we think effects the biolgical mechanism of teh brain and such system effects how we think.   </p> <ul> <li> Physical cooling: The amygdala is located behind the nose and its activation represents congestion with temperature rises. When inhaling cool air, we physically cool down the amygdala and lowers its activations.     </li> <li> Celebrate success: Prolong the time of happiness, after four minutes, memory conslolidates and the physiological pleasure neural network is formed (remember, later watching is equivalent to doing in the eyes of neuronal mechanisms).     </li> <li> Neurologically \"proactive\": Use positive neurotransmitters to form positive neuro-emotional networks. Psychological activity isn't a single point, but an interconnected area.     </li> <ul> <li>         Dopamine: Released when helping others, discovering self-strengths, experiencing self-worth, doing what others cannot.       </li> <li>         Serotonin: Released when protecting self-esteem or basking in the sun.       </li> <li>         Endorphins: Happiness from deeper thinking or delayed gratifications.       </li> <li>         Oxytocin: Giving feel of love. released by warm hugs, receiving and giving compliments, or engaging in empathetic, understanding, appreciative, and supportive conversations.       </li> </ul> <li> Habitual behaviors: Humans easily fall into habitual behavior, to act automatically without thinking. When challenges arise, habits become instincts, and instincts serve as an important protective mechanism.     </li> <li> Build default immersive circuits: Find your own Flow and immerse in it to experience deep engagement (as what we say in chinese: \u6c89\u6d78\u5176\u4e2d\uff0c\u7269\u6211\u4e24\u5fd8\uff0c\u9163\u7545\u51cc\u5389\uff0c\u5982\u75f4\u5982\u9189). With such an system build and reinforced by Basal Ganglia, doing        more of it is much easier.     </li> </ul>"},{"location":"articles/neuroscience/positive_psycology/#evolution-of-a-desirable-circuits-habitization","title":"Evolution of a Desirable Circuits: Habitization","text":"<p>     Building a circuit against stress is a process similar to evolution, we are keep trying to construct better and better neuronal systems. We need to use all possible methods at hand to build a desirable neuronal cuirtuit during moment of      stress to react to it at the moment and also build foundations for later reactions. With more of such response, habitual circuits would be formed and reaction to stress or challenges later would be as simple as instinct.   </p> <p>     Matching on the thoughts levels than reality will follow. Think about the person that you want to be, imagine what this person would do and do it, act like him/her and align your thinking on an abstract level first. Every single little small thing that you do like this person would      prove a bit mroe that you are this person. As the bricks lays and as time goes on, you become this person.   </p>"},{"location":"articles/neuroscience/systematic_deviation/","title":"Neural Adaptation With Cost: Systematic Balance Distortion","text":"Kaiwen Bian 5 min read \u00b7 Jun 17, 2024 <p>       I find addiction's neural circuit as one that is really interesting to look at because it demonstrate a systematic method of adaptation to deviation (sometimes this is quite problematic), an well rounded circular circuit that feeds into each        other to achieve the adaptation that has an cost. More importantly, I think it may serve as a model that can help us to understand how adaptation works on a neuronal level, which may effect how we think, feel, and do on the cognitive level, how        higher level cognition may stem from basic biological units that continuously reevaluates the needs in the environment and readjust the body's parameter to fit such needs. After reading this, you may have a better intuition of how good cognitive functions really rely on        the balance between different circuits that communicates with each others.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#allostatic-processes-nueral-adapttaion-with-cost","title":"Allostatic Processes: Nueral Adapttaion With Cost","text":"<p>       I want to discuss the idea of addiction through the perspective of allostasis, which I will intriduce first. Homeostasis, the perspective of deeming how biological system works as a restoration of balance, of an setpoint, may be one that is too ideal to        impose to our body sometimes. It assumes a perfect senerio, an egineering system that is imposed onto biology. Allostasis, on the other hand, involves a feed-forward mechanism and describes \"stability through chanegs\".        In the Allostasis hypothesis, the feed-forward allows the matching and adjustment to the environment through continuous reevaluation of needs and continuous readjustment of all parameters toward the new set points.        However, when the body is under states that \"we don't exactly want to adapt to\", the ability to quickly mobilize resources and use feed-forward mechanisms to adapt to the environment can lead to some issues, the cost of neural adaptation.   </p> <p>An allostatic state can be defined as a state of chronic deviation of the regulatory system from its normal (homeostatic) operating level</p> <p>Koob and Le Moal (2001)</p> <p>       Sometimes the same circuit that help us adapt to an new environent quickly and adjust ourselves may be the same one that leads to allostatic load, allostatic state, or ultimately pathology under extreme conditions. Allostatic mechanims have long being hypothesized to be        regulating and maintaining motivational systems that have relevance to the pathology of additction. Using an simile to take home the points:   </p> <p>           An acute elevation of blood pressure is \u201cappropriate\u201d in an allostasis model to meet the environmental demand of acute arousal, but chronic blood pressure elevations under conditions of chronic stress may address the chronic environmental            demand but is certainly not healthy.       </p> <p>Sterling and Eyer (1988)</p> <p>The circuit of addiction spans across multiple area and doing multiple things all at once, aligning with the whole theme of \"all the area, all the functions, all the state, all at once\". Using the conceptual framework layed out in Koob and Schulkin's paper,      addiction is mainly separtated to 3 stages: Intoxination, Withdraw Affect, and Preoccupation and each stage have circuits that is specifically for them but also feeding in into other stages. For example:     <ul> <li> <p>Basal Ganglia (blue structure) is relevant to the Intoxination stage with the activation of various neurotransmitters such as dopamine and opiod peptide, engaging asscoiative mechanism with nucleus accumbens shell, and making an stimulus to response            habits engaging the dorsal striatum.</p> </li> <li> <p>In the Withdraw Affect, the extended amygdala (red structures) is highly activated and deviates the balance far from the original set point.</p> </li> <li> <p>Then during the Preoccupation period (green structure) would be relevant to hippocampus and executive function areas like prefrontal cortex + active seacrhing processing areas like orbital and anterior cingulate cortices and temporal lobe.</p> </li> </ul> </p> <p>Conceptual framework of how addiction happens (Koob and Volkow (2016)))</p>"},{"location":"articles/neuroscience/systematic_deviation/#too-much-reward-may-be-chronic-stress","title":"Too Much Reward May Be Chronic Stress","text":"<p>Motivation is a construct that can be defined as \u201ca state that varies with arousal and guides behavior in relationship to changes in the environment. The environment can be external (incentives) or internal (central motive states or drives), and such motivation or motivational states are not constants and vary over time\u201d.</p> <p>Koob et al. (2010)</p> <p>     Motivation talks about a state with arousal that guides behaviors in the changing environmengt, which have been shown to be correlated with how stress we are. Some levels of stress can be rewarding and benificial for learning as we know that Cortical Releasing Factor (CRF) (think it as stress hormone) may help us to spring to actions, lower the sensory threshold, and      have involvement in attentional responses to both external and internal events that increases incentive salience. However, excessive stress would take our body into the vicious cycle and may leads to pathology. The interesting things is that the same circuit that responses to stress have been indicated to have similar effect towards reward, lending the idea that too uch reward may be problematic.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#hpa-axis-glucocorticoid","title":"HPA Axis &amp; Glucocorticoid","text":"<p>     When we are facing stress, the HPA axis woud be activated and the CRF expressing neurons in the paraventricular nucleus of the hypothalamus would release CRF into the portal system, activates the release of ACTH from the pituitary, which in turn activates the release of glucocorticoids from the adrenal cortex.      This is one of the brain's attempt to restore balances as glucocorticoid activation may decrease CRF gene expression, which leads to less CRF. However, this is already a neuroadaptations that initiate the allostatic process, one that deviates from the original point (notice that deviation is perfectly normal as      we need to adapt to changes, but a systematic deviation that causes too great of a magnitude shift may be causing some issues).   </p> <p>     Back to addiction, drug (reward) would initiates similar effect that stimulate the HPA Axis with ACTH release and establishing the whole down stream effect that can be caused with mild stress described above. In some sense, it seems like stress and reward does both trigger similar activations that causes the reduction in CRH, which      makes us feel good. However, the difference being the amount of neurotransmitter that is released. In the drug's case, it has ahuge magnitute (in some sense it is signaling the brain that the person is under chronic stress whne they are actually under euphoria caused by drugs), which links to issues that will be expanded more below.   </p> <p>CRF Circuit (Koob and Schulkin (2019))</p>"},{"location":"articles/neuroscience/systematic_deviation/#withdraw-affect-residual-hysteresis","title":"Withdraw Affect: Residual Hysteresis","text":"<p>     The massive release of reward neurotransmitters (causing the downstream effect mentioned previously) comes with drug-opposite responses as the drug wears off. Because of the magnitute of neurotransmitter release, such opponent processes have been hypothesized to occur even with a single injection of a drug.      If the intake is persistant, the brain attempts to keep adapting to the new state and the bar with feeling rewarding would be elevated and may never returns to baseline levels hypothesized by residual hysteresis. progressively, it creats greater elevation of the baseline reward thresholds and leading to more compulsize drug seeking behavior (addiction).      Remanber that CRH may boost our learning because it have involvement in attentional responses to both external and internal events that increases incentive salience? Well, the same phenomenon would occur for drugs, the attentional salience map for this object (drug) wouuld be elevated with the \"value\" of such object over-estimated, hence, amking it even harder to withdraw from it.   </p> <p>Systematic decitation from setpoint (Koob and Schulkin (2019))</p> <p>     The Withdraw stage's effect may feed back into the Intoxination stage since such massive release of neurotransmitter from drug's effect are associated with adaptations such as decreases in dopaminergic transmission and dopaminergic neuron firing in the ventral striatum, nucleus accumbens, and ventral tagemental area, causing the lack of      dopamine signal when needed. As well known, dopamine is an key neural transmitter in encoding reward signal, so if there exist a adaptation of systematic deviation, issues would occur as this adaptation is somewhat also raising the bar of feeling reward becuase the body thinks that not as much dopamine is really needed because of the new given condition.      Similraly, molecular adaptation with G-protein functioning and protein kinase A activity in the nucleus accumbens also occurs to consolidate these changes that occurs between-systems.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#preoccupation-pfc-being-offline","title":"Preoccupation: PFC Being Offline","text":"<p>     The Preprontal cortex is usually deemed as a key for executive function and coognitive decision making (i.e. smart choices of not doing drugs). The activation of the HPA axis and the CRF system would negatively impacts the prefrontal cortex to impair such top-down connectivity and boost allostatic changes in the extended amygdala.      Now there is a decrease in the inhibitory function of the prefrontal cortex (ventromedial prefrontal cortex, orbitofrontal cortex, and cingulate cortex) and a boosting in the extended amygdala, the habitual mode mentioned in the other article would start to occur, leading to further preoccupation of drug stimuli.   </p>"},{"location":"articles/neuroscience/systematic_deviation/#distortion-of-balance","title":"Distortion of Balance","text":"<p>     Now you may begin to have some intuition of how these 3 stages work independently on its own but also feeding in to each other to create such trap of addiction and also how systematic deviatoon from the balance point may introduce really serious issues. On the extreme side of these distortion of balance would be psychopaths where it have been hypothesized that      one reaosn that may be behind the lack of emotion of psychopath is because of childhood traumatic events that caused them to seggragate their PFC and extended amygdala for too long, leading to the adaptation of distotred balance and the lack of feeling emotions.   </p>"},{"location":"literature/literatures/","title":"Literature","text":"<p>Stuff I find to be an interesting read... discussing novel ideas in mathematics, neuroscience, machine learning, or their intersections. My notes on literatures</p>"},{"location":"literature/literatures/#research-paradigm","title":"Research Paradigm","text":"<ul> <li> <p>The Standard Model of Machine Learning Standardlizing all machine learning approahces, forming a standard model for ML.</p> </li> <li> <p>The Neuroconnectionist Research Programme Lakatosian research program setting computational understanding of the brain.</p> </li> <li> <p>Building Machines that Learn and Think like People Foundation review discussing potential research directions for human-like AI.</p> </li> </ul>"},{"location":"literature/literatures/#reinforcement-learning","title":"Reinforcement Learning","text":""},{"location":"literature/literatures/#theoretical-reinforcement-learning","title":"Theoretical Reinforcement Learning","text":"<ul> <li> <p>Maximum a Posteriori Policy Optimisation Probabilistic flavor-infused cutting-edge actor-critic.</p> </li> <li> <p>Proximal Policy Optimization Algorithms Actor-critic algorithm on steroids.</p> </li> <li> <p>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures Distributed RL framework for off-policy learning.</p> </li> <li> <p>Addiction as a Computational Process Gone Awry Using RL methods to model the addiction process.</p> </li> </ul>"},{"location":"literature/literatures/#goal-directed-deep-reinforcement-learning","title":"Goal-Directed Deep Reinforcement Learning","text":"<ul> <li> <p>Divergent Representations of Ethological Visual Inputs Emerge from Supervised, Unsupervised, and Reinforcement Learning Proposes using ANNs to model the brain.</p> </li> <li> <p>Deep Neuroethology of a Virtual Rodent Aligning deep RL with biological counterparts.</p> </li> <li> <p>Whole-body Simulation of Realistic Fruit Fly Locomotion with Deep Reinforcement Learning Distributed trained MPO policy for goal-directed RL.</p> </li> </ul>"},{"location":"literature/literatures/#inverse-kinematics-imitation-learning","title":"Inverse Kinematics Imitation Learning","text":"<ul> <li> <p>A Virtual Rodent Predicts the Structure of Neural Activity Across Behaviors Imitation learning mimicking rodent behaviors, showing similar neuronal activations.</p> </li> <li> <p>CoMic: Complementary Task Learning &amp; Mimicry for Reusable Skills Encoder/decoder architecture transferring motor skills across tasks.</p> </li> </ul>"},{"location":"literature/literatures/#representation-building","title":"Representation Building","text":"<ul> <li>Inductive Biases of Neural Network Modularity in Spatial Navigation Building beliefs in artificial agents through MOPDP conditions.</li> </ul>"},{"location":"literature/literatures/#world-model-agent-model","title":"World Model &amp; Agent Model","text":"<ul> <li> <p>Language Models Meet World Models: Embodied Experiences Enhance Language Models L-policy: building embodied world model into language model agent through embodied experiences and finetunning.</p> </li> <li> <p>Building Cooperative Embodied Agents Modularly with Large Language Models Building collaborative agent in an partially observable embodied environment.</p> </li> </ul>"},{"location":"literature/notes/","title":"Research Literatures' Notes","text":"<p>To share ideas and hopefully inspire some new development... Constantly updating with new ideas.</p> <ul> <li> <p>Standard Model of Machine Learning Written by Dec 3, 2024</p> </li> <li> <p>Building Cooperative Modular Embodied Agents With LLM Written by Oct 18, 2024</p> </li> <li> <p>Language Models Meet World Models Written by Oct 16, 2024</p> </li> <li> <p>The Neuroconnectionist Research Programme Written by Jun 30, 2024</p> </li> <li> <p>Addiction as a Computational Process Gone Awry Written by Jul 18, 2024</p> </li> <li> <p>Building Machines that Learn and Think like People Written by Jul 16, 2024</p> </li> </ul>"}]}